[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brandon Scott",
    "section": "",
    "text": "I love learning and teaching so this blog is my motivation to learn new topics and learn how to best convey the messages I received. I hope to demonstrate in this blog an analytical way to approach problems. I like using a tool-box method in my posts, approaching a problem and demonstrating how various tools can be used to solve it. I hope you enjoy this blog as much as I enjoy writing it!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Brandon Scott",
    "section": "Education",
    "text": "Education\nTexas A&M University M.S. in Statistics | January 2023 - May 2025\nUtah Valley University M.S. in Computer Science | September 2021 - April 2022 (transferred)\nBrigham Young University B.S. in Actuarial Science | September 2017 - April 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Brandon Scott",
    "section": "Experience",
    "text": "Experience\nUber | Data Analyst | October 2022 - Present AdvancedMD | Senior Business Analyst | July 2021 - October 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Bayesian Bandit",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\nwelcome\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of relationship between GPA and Study Hours\n\n\n\nanalysis\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nDec 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html",
    "href": "posts/gpa-analysis/gpa_analysis.html",
    "title": "Analysis of relationship between GPA and Study Hours",
    "section": "",
    "text": "The hope of this analysis is to demonstrate that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to:\n\n\nGather inference to determine how GPA changes based on increase study time\n\n\nPredict what a student’s GPA would be based on study time\n\n\nData for this analysis can be found here"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#introduction",
    "href": "posts/gpa-analysis/gpa_analysis.html#introduction",
    "title": "Analysis of relationship between GPA and Study Hours",
    "section": "",
    "text": "The hope of this analysis is to demonstrate that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to:\n\n\nGather inference to determine how GPA changes based on increase study time\n\n\nPredict what a student’s GPA would be based on study time\n\n\nData for this analysis can be found here"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#exploratory-data-analysis-eda",
    "href": "posts/gpa-analysis/gpa_analysis.html#exploratory-data-analysis-eda",
    "title": "Analysis of relationship between GPA and Study Hours",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nImport data and libraries\nWe begin our analysis by importing the following libraries to create data visualizations as well as properly format and filter our data.\n\n#Import libraries\nimport opendatasets as od\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWe read in our data using pandas and will use the head function to check out the data.\n\n#Check dataframe\ndf.head()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n0\n4.00\n10.0\n\n\n1\n3.80\n25.0\n\n\n2\n3.93\n45.0\n\n\n3\n3.40\n10.0\n\n\n4\n3.20\n4.0\n\n\n\n\n\n\n\nThe head result indicates that the data has been read in correctly. In the next section, we will begin to clean the data.\n\n\nData Cleaning\nWe will begin our data cleaning by getting some basic summary statistics from the dataframe.\n\n#Describe dataset\ndf.describe()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\ncount\n193.000000\n193.000000\n\n\nmean\n3.586166\n17.476684\n\n\nstd\n0.285482\n11.408980\n\n\nmin\n2.600000\n2.000000\n\n\n25%\n3.400000\n10.000000\n\n\n50%\n3.620000\n15.000000\n\n\n75%\n3.800000\n20.000000\n\n\nmax\n4.300000\n69.000000\n\n\n\n\n\n\n\nThe resulting dataframe shows that we have 193 rows of data where on average the gpa is 3.58 with a standard deviation of .28. Average study hours show 17.47 and standard deviation of 11.4, meaning we have a large spread in the distribution of study hours but not too much in gpa. From the summary statistics, we can easily see that the max value is invalid as the scale we are using for this analysis is 0-4. We will eliminate any rows that fall outside of this range.\n\n#Check datatypes\ndf.dtypes\n\ngpa            float64\nstudy_hours    float64\ndtype: object\n\n\nChecking the datatypes of each of the columns, we see that each is of type float. This confirms that all rows in each column are of the same datatype and, in theory, we do not need to clean up any corrupted data/poorly entered data.\n\n#Check for null values\ndf.isnull().sum()\n\ngpa            0\nstudy_hours    0\ndtype: int64\n\n\nIn addition, checking for nulls, we see there are none so we do not need to do anything with those. In the next code chunk, we will eliminate erroneous gpa entries.\n\n#Convert outlier GPA (erroneous entries) to 4.0\ndf = df.assign(gpa = lambda x: np.where(x.gpa &gt; 4, 4, x.gpa))\n\nIn the above chunk, we use the assign function to override the gpa column and change values that are above 4 to just 4 while leaving all others the same.\n\n\nData Viz\nThe first visualization we will look at is comparing the distributions of gpa and study hours. We want to get a side by side look so we will use the subplots functionality from matplotlib. As well, we will plot the median of the two respective distributions to know where the 50% percentile lies. The following code produces Figure 1.\n\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\");\n\n#fig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\nFigure 1: Distributions of GPA and Study Hours\n\n\n\n\nFrom Figure 1, we see that GPA appears to be slightly left skewed with a couple of outliers to the left of the distribution. Study hours appears to be more right skewed with more outliers towards the right of the distribution. Median for GPA is 3.62 and 15 for study hours.\nSince we are working with two quantitative variables, we can use a scatterplot to view how linear their relationship (ie correlation) as well as view the where each gpa falls for a given amount of study hour. The below code produces Figure 2.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 2: Scatterplot of GPA by Study Hours\n\n\n\n\nFigure 2 shows that the linear relationship between study hours and GPA is not very strong. An exact score of .14 for the correlation indicates a weak linear relationship. Further more, like we already saw from the histograms, the data appears to be more grouped towards the beginning of the x axis (0-20ish hours).\nWe can attempt to adjust the distribution and correlation of the dataset by removing outliers. However, because the outlying dots appear to follow the trend of the data (more or less…) and we always like keeping as much data as possible, this is merely an optional step to fulfill our curiosity.\nWe will define an outlier as a point that falls outside of a box-plot range. Below is the function we will use to define and identify outliers.\n\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\nThe function calculates the IQR of the dataset and then returns a filtered dataframe based on whether a point passes the upper or lower bound of the box-plot. For example, below we see how the function identifies outliers using the GPA attribute.\n\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\nSimilarily, we can view the outliers for study hours below.\n\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n2\n3.930\n45.0\n\n\n7\n3.400\n40.0\n\n\n13\n3.830\n60.0\n\n\n51\n3.868\n40.0\n\n\n57\n3.125\n36.0\n\n\n77\n3.566\n40.0\n\n\n83\n3.850\n69.0\n\n\n89\n3.700\n45.0\n\n\n122\n3.750\n40.0\n\n\n125\n3.500\n49.0\n\n\n130\n3.825\n60.0\n\n\n135\n3.600\n40.0\n\n\n169\n3.830\n60.0\n\n\n\n\n\n\n\nFigure 3 below shows which points are labeled as outliers from our scatterplot.\n\n#Plot original scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\")\n\n#Show outliers by study hours (color red)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"study_hours\"), color='red')\n\n#Show outliers by gpa (color yellow)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"gpa\"), color='yellow');\n\n\n\n\nFigure 3: Scatterplot of GPA and Study Hours (highlighted outliers)\n\n\n\n\nAs already indicated, the red dots are outliers according to the “study hours” metric while yellow indicates outlier for “gpa”. While the red dots are technically “outliers”, they do sort of follow the general trend and direction of the data whereas the yellow dot does not. Let’s try removing the yellow dot to see how it changes the correlation of the data.\n\n#Remove outliers from dataset\ndf_filtered = df.query(\"gpa &gt; 2.6\")\n\nAfter removing the gpa outlier, the scatterplot and corresponding correlation are as follows.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df_filtered)\ncorr_coef = np.corrcoef(df_filtered.study_hours, df_filtered.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 4: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nWe see that removing the gpa outlier had a minimal effect on the correlation (actually slightly decreased) thus “weakening” the linear relationship of the variables. Perhaps we can try a non-linear transformation to aid our non-linear data.\nTo do this, we will perform a log transformation on both gpa and study hours (on our original dataset) and a polynomial transformation on study hours. Below is the result of our transformations.\n\n#Plot log transformed data\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.regplot(x='study_hours', y='gpa', data=np.log(df), ax=ax1)\nax1.set_xlabel(\"log(study_hours)\")\nax1.set_ylabel(\"log(gpa)\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax1.text(np.log(30), np.log(2.8), f\"Correlation: {corr_coef:.2f}\")\n\n#Plot polynomial transformed data\ndf_poly = df.assign(study_hours = lambda x: x.study_hours**2)\nsns.regplot(x='study_hours', y='gpa', data=df_poly, ax=ax2)\nax2.set_xlabel(\"study_hours^2\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax2.text(50**2, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 5: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nIn Figure 5, we see that our transformations really had not effect on the linear relationship. For the sake of getting on with the modeling, we will use our base dataset and interpret the model accordingly (barring that the other model assumptions hold)."
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#normal-frequentist-modeling",
    "href": "posts/gpa-analysis/gpa_analysis.html#normal-frequentist-modeling",
    "title": "Analysis of relationship between GPA and Study Hours",
    "section": "Normal (Frequentist) Modeling",
    "text": "Normal (Frequentist) Modeling\nI hesitate to use the word normal here as that term usually denotes a different meaning in the world of statistics, but I want to separate the modeling section into two parts.\n\n\nFrequentist approaches\n\n\nBayesian approaches\n\n\nAfter all, this is thebayesianbandit, but I like providing frequentist approaches as well to show the differences and similarities between normal statistical modeling (frequentist) and bayesian modeling.\n\nLinear Regression Framework\nSince we are dealing with “linear” data, it would make sense to attempt to attempt to fit a linear model to the data. Therefore, we will be utilizing a simple linear regression (SLR) model to model the relationship between GPA and study hours (per week). Below is the equation we will use.\n\\[\\begin{equation}\ny = X\\beta + \\epsilon\n\\end{equation}\\]\n\\[\\begin{equation}\n\\epsilon \\sim N(\\mu, \\sigma^2)\n\\end{equation}\\]\nThe above formula models our response variable is equal to our matrix of explanatory variables (in this case, just one) multiplied (scaled) by a matrix of betas, plus errors (residuals) that are distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "posts/welcome/welcome_to_blog.html",
    "href": "posts/welcome/welcome_to_blog.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "TL;DR\nThis blog focuses on the use of statistics, particularly bayesian statistics, in the application of business problems. In this post, we review the necessary math background in order to understand future blog posts. As well, we establish expectations for future blog posts and the order in which they are shared.\n\n\nWelcome To My Blog!\nWelcome to my blog! If you are reading this, I hope that means you are as excited about learning statistics as I am about teaching it (and continuing to learn it!). This blog, honestly, is mostly a tool for me to improve my own knowledge in statistics by learning how I would convey it to others. I plan on doing this from a data analysis perspective. Given a certain situation/set of data points, how do we construct the problems we want to solve and implement proper solutions. Particularly, I hope to demonstrate that Bayesian methods can be viable methods to solve problems in business, healthcare, etc. Anyways, I hope you find this blog interesting and useful in your data career. Whether you are just entering data or are an experienced data professional, this blog intends to be a resource in demonstrating good data analysis with sound applied mathematical theory.\n\n\nPrerequisites\nWhile I did mention above that this blog is intended to be useful for beginners and experts alike, I will admit that I will be covering some higher level math, such as:\n\n\nMultivariate Calculus\n\n\nProbability Theory\n\n\nLinear Algebra\n\n\nFor example, suppose we are studying a dataset with a variable that is modeled as an exponential random variable, we would use the below notation to show the probability density function (PDF)\n\\[\\begin{equation}\n\\int_{0}^{\\infty} \\lambda e^{\\lambda x} \\,dx\n\\end{equation}\\]\n\\[\\begin{equation}\nX \\sim Exponential(\\lambda)\n\\end{equation}\\]\nOr, the important Bayes Theorem found below.\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\end{equation}\\]\nDon’t be discourage if the above formulas don’t make sense right now. In each post that these appear, I will breakdown what they mean and their applicability in solving our problem.\n\n\nExpectations\nThe hope of this blog is to present statistics in the form of a data analysis case. For example, let’s say you are a host on AirBnB and want to maximize the number of nights you rent out. How do you properly price your rental given certain parameters (size, geographic setting, etc). We would then present the data points and walk through an analysis of the data by doing the following.\n\n\nExploring the data (EDA) to get to know our dataset\n\n\nConstructing a mathematical framework that can fit our data\n\n\nFit our data to said framework\n\n\nGather inference from our model\n\n\nReview answers explained by our model\n\n\nExplain possible enhancements to our model for future analyses\n\n\nThis will be our attempted framework to blog posts. I hope that by tackling problems in this way, our analytical toolbox will grow and our ability to construct measurable problems from our data will improve. This is the ultimate goal of this blog. I really hope that people will recognize this blog as an opportunity to learn how to think analytically.\n\n\nLet’s do this\nI just want to reiterate that I am excited to learn how to be a better analyst with you by getting to know the math that drives our analytics. I am very passionate about how data can be used to properly drive decision-making in organizations and I hope that this blog motivates you to do the same."
  }
]