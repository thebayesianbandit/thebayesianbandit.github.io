[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brandon Scott",
    "section": "",
    "text": "I love learning and teaching so this blog is my motivation to learn new topics and learn how to best convey the messages I received. I hope to demonstrate in this blog an analytical way to approach problems. I like using a tool-box method in my posts, approaching a problem and demonstrating how various tools can be used to solve it. I hope you enjoy this blog as much as I enjoy writing it!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Brandon Scott",
    "section": "Education",
    "text": "Education\nTexas A&M University M.S. in Statistics | January 2023 - May 2025\nUtah Valley University M.S. in Computer Science | September 2021 - April 2022 (transferred)\nBrigham Young University B.S. in Actuarial Science | September 2017 - April 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Brandon Scott",
    "section": "Experience",
    "text": "Experience\nUber | Data Analyst | October 2022 - Present AdvancedMD | Senior Business Analyst | July 2021 - October 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Bayesian Bandit",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\nwelcome\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGPA Analysis\n\n\n\nanalysis\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nDec 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html",
    "href": "posts/gpa-analysis/gpa_analysis.html",
    "title": "GPA Analysis",
    "section": "",
    "text": "The hope of this analysis is to demonstratet that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to: 1. Gather inference to determine how GPA changes based on increase study time 2. Predict what a student’s GPA would be based on study time\n\n\n\nWe begin our analysis by importing the following libraries to create data visualizations as well as properly format and filter our data.\n\n\nCode\n#Import libraries\nimport opendatasets as od\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\n### ONLY RUN ONCE ###\n#Download dataset\n#od.download(\"https://www.kaggle.com/datasets/joebeachcapital/gpa-study-hours\")\n\n\nDownloading gpa-study-hours.zip to ./gpa-study-hours\n\n\n\n100%|█████████████████████████████████| 658/658 [00:00&lt;00:00, 21.0kB/s]\n\n\n\n\nCode\n#Create dataframe from file\ndf = pd.read_csv(\"kaggle_datasets//gpa-study-hours/gpa_study_hours.csv\")\n\n\n\n\nCode\n#Check dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n0\n4.00\n10.0\n\n\n1\n3.80\n25.0\n\n\n2\n3.93\n45.0\n\n\n3\n3.40\n10.0\n\n\n4\n3.20\n4.0\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Describe dataset\ndf.describe()\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\ncount\n193.000000\n193.000000\n\n\nmean\n3.586166\n17.476684\n\n\nstd\n0.285482\n11.408980\n\n\nmin\n2.600000\n2.000000\n\n\n25%\n3.400000\n10.000000\n\n\n50%\n3.620000\n15.000000\n\n\n75%\n3.800000\n20.000000\n\n\nmax\n4.300000\n69.000000\n\n\n\n\n\n\n\n\n\nCode\n#Check datatypes\ndf.dtypes\n\n\ngpa            float64\nstudy_hours    float64\ndtype: object\n\n\n\n\nCode\n#Check for null values\ndf.isnull().sum()\n\n\ngpa            0\nstudy_hours    0\ndtype: int64\n\n\n\n\nCode\n#Convert outlier GPA (erroneous entries) to 4.0\ndf = df.assign(gpa = lambda x: np.where(x.gpa &gt; 4, 4, x.gpa))\n\n\n\n\n\n\n\nCode\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\")\n\nfig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\n\n\n\nCode\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\n\n\n\nCode\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\n\n\n\nCode\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\n\n\nCode\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")[\"study_hours\"].min()\n\n\n36.0\n\n\n\n\nCode\n#Remove outliers from dataset\ndf = df.query(\"gpa &gt; 2.6 and study_hours &lt; 36\")\n\n\n\n\nCode\n#Revisualize distributions without outliers\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\")\n\nfig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\n\n\n\nCode\n#Revisit scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(30, 3.1, f\"Correlation: {corr_coef:.2f}\");"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#abstract",
    "href": "posts/gpa-analysis/gpa_analysis.html#abstract",
    "title": "GPA Analysis",
    "section": "",
    "text": "The hope of this analysis is to demonstratet that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to: 1. Gather inference to determine how GPA changes based on increase study time 2. Predict what a student’s GPA would be based on study time"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#setup-analysis",
    "href": "posts/gpa-analysis/gpa_analysis.html#setup-analysis",
    "title": "GPA Analysis",
    "section": "",
    "text": "We begin our analysis by importing the following libraries to create data visualizations as well as properly format and filter our data.\n\n\nCode\n#Import libraries\nimport opendatasets as od\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nCode\n### ONLY RUN ONCE ###\n#Download dataset\n#od.download(\"https://www.kaggle.com/datasets/joebeachcapital/gpa-study-hours\")\n\n\nDownloading gpa-study-hours.zip to ./gpa-study-hours\n\n\n\n100%|█████████████████████████████████| 658/658 [00:00&lt;00:00, 21.0kB/s]\n\n\n\n\nCode\n#Create dataframe from file\ndf = pd.read_csv(\"kaggle_datasets//gpa-study-hours/gpa_study_hours.csv\")\n\n\n\n\nCode\n#Check dataframe\ndf.head()\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n0\n4.00\n10.0\n\n\n1\n3.80\n25.0\n\n\n2\n3.93\n45.0\n\n\n3\n3.40\n10.0\n\n\n4\n3.20\n4.0"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#data-cleaning",
    "href": "posts/gpa-analysis/gpa_analysis.html#data-cleaning",
    "title": "GPA Analysis",
    "section": "",
    "text": "Code\n#Describe dataset\ndf.describe()\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\ncount\n193.000000\n193.000000\n\n\nmean\n3.586166\n17.476684\n\n\nstd\n0.285482\n11.408980\n\n\nmin\n2.600000\n2.000000\n\n\n25%\n3.400000\n10.000000\n\n\n50%\n3.620000\n15.000000\n\n\n75%\n3.800000\n20.000000\n\n\nmax\n4.300000\n69.000000\n\n\n\n\n\n\n\n\n\nCode\n#Check datatypes\ndf.dtypes\n\n\ngpa            float64\nstudy_hours    float64\ndtype: object\n\n\n\n\nCode\n#Check for null values\ndf.isnull().sum()\n\n\ngpa            0\nstudy_hours    0\ndtype: int64\n\n\n\n\nCode\n#Convert outlier GPA (erroneous entries) to 4.0\ndf = df.assign(gpa = lambda x: np.where(x.gpa &gt; 4, 4, x.gpa))"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#eda",
    "href": "posts/gpa-analysis/gpa_analysis.html#eda",
    "title": "GPA Analysis",
    "section": "",
    "text": "Code\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\")\n\nfig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\n\n\n\nCode\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\n\n\n\nCode\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\n\n\n\nCode\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\n\n\nCode\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")[\"study_hours\"].min()\n\n\n36.0\n\n\n\n\nCode\n#Remove outliers from dataset\ndf = df.query(\"gpa &gt; 2.6 and study_hours &lt; 36\")\n\n\n\n\nCode\n#Revisualize distributions without outliers\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\")\n\nfig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\n\n\n\nCode\n#Revisit scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(30, 3.1, f\"Correlation: {corr_coef:.2f}\");"
  },
  {
    "objectID": "posts/welcome/welcome_to_blog.html",
    "href": "posts/welcome/welcome_to_blog.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "TL;DR\nThis blog focuses on the use of statistics, particularly bayesian statistics, in the application of business problems. In this post, we review the necessary math background in order to understand future blog posts. As well, we establish expectations for future blog posts and the order in which they are shared.\n\n\nWelcome To My Blog!\nWelcome to my blog! If you are reading this, I hope that means you are as excited about learning statistics as I am about teaching it (and continuing to learn it!). This blog, honestly, is mostly a tool for me to improve my own knowledge in statistics by learning how I would convey it to others. I plan on doing this from a data analysis perspective. Given a certain situation/set of data points, how do we construct the problems we want to solve and implement proper solutions. Particularly, I hope to demonstrate that Bayesian methods can be viable methods to solve problems in business, healthcare, etc. Anyways, I hope you find this blog interesting and useful in your data career. Whether you are just entering data or are an experienced data professional, this blog intends to be a resource in demonstrating good data analysis with sound applied mathematical theory.\n\n\nPrerequisites\nWhile I did mention above that this blog is intended to be useful for beginners and experts alike, I will admit that I will be covering some higher level math, such as:\n\n\nMultivariate Calculus\n\n\nProbability Theory\n\n\nLinear Algebra\n\n\nFor example, suppose we are studying a dataset with a variable that is modeled as an exponential random variable, we would use the below notation to show the probability density function (PDF)\n\\[\\begin{equation}\n\\int_{0}^{\\infty} \\lambda e^{\\lambda x} \\,dx\n\\end{equation}\\]\n\\[\\begin{equation}\nX \\sim Exponential(\\lambda)\n\\end{equation}\\]\nOr, the important Bayes Theorem found below.\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\end{equation}\\]\nDon’t be discourage if the above formulas don’t make sense right now. In each post that these appear, I will breakdown what they mean and their applicability in solving our problem.\n\n\nExpectations\nThe hope of this blog is to present statistics in the form of a data analysis case. For example, let’s say you are a host on AirBnB and want to maximize the number of nights you rent out. How do you properly price your rental given certain parameters (size, geographic setting, etc). We would then present the data points and walk through an analysis of the data by doing the following.\n\n\nExploring the data (EDA) to get to know our dataset\n\n\nConstructing a mathematical framework that can fit our data\n\n\nFit our data to said framework\n\n\nGather inference from our model\n\n\nReview answers explained by our model\n\n\nExplain possible enhancements to our model for future analyses\n\n\nThis will be our attempted framework to blog posts. I hope that by tackling problems in this way, our analytical toolbox will grow and our ability to construct measurable problems from our data will improve. This is the ultimate goal of this blog. I really hope that people will recognize this blog as an opportunity to learn how to think analytically.\n\n\nLet’s do this\nI just want to reiterate that I am excited to learn how to be a better analyst with you by getting to know the math that drives our analytics. I am very passionate about how data can be used to properly drive decision-making in organizations and I hope that this blog motivates you to do the same."
  }
]