[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brandon Scott",
    "section": "",
    "text": "I love learning and teaching so this blog is my motivation to learn new topics and learn how to best convey the messages I received. I hope to demonstrate in this blog an analytical way to approach problems. I like using a tool-box method in my posts, approaching a problem and demonstrating how various tools can be used to solve it. I hope you enjoy this blog as much as I enjoy writing it!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Brandon Scott",
    "section": "Education",
    "text": "Education\nTexas A&M University M.S. in Statistics | January 2023 - May 2025\nUtah Valley University M.S. in Computer Science | September 2021 - April 2022 (transferred)\nBrigham Young University B.S. in Actuarial Science | September 2017 - April 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Brandon Scott",
    "section": "Experience",
    "text": "Experience\nUber | Data Analyst | October 2022 - Present AdvancedMD | Senior Business Analyst | July 2021 - October 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Bayesian Bandit",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\nwelcome\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\nanalysis\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nDec 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html",
    "href": "posts/gpa-analysis/gpa_analysis.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "The hope of this analysis is to demonstrate that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to:\n\n\nGather inference to determine how GPA changes based on increase study time\n\n\nPredict what a student’s GPA would be based on study time\n\n\nData for this analysis can be found here"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#introduction",
    "href": "posts/gpa-analysis/gpa_analysis.html#introduction",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "The hope of this analysis is to demonstrate that there is a quantifiable relationship between GPA and study hours. We recognize that the collected data was self reported and therefore can be dishonest. Nonetheless, our goal is to still find a way to quantify a relationship between GPA and study hours to:\n\n\nGather inference to determine how GPA changes based on increase study time\n\n\nPredict what a student’s GPA would be based on study time\n\n\nData for this analysis can be found here"
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#exploratory-data-analysis-eda",
    "href": "posts/gpa-analysis/gpa_analysis.html#exploratory-data-analysis-eda",
    "title": "Simple Linear Regression",
    "section": "0.2 Exploratory Data Analysis (EDA)",
    "text": "0.2 Exploratory Data Analysis (EDA)\n\n0.2.1 Import data and libraries\nWe begin our analysis by importing the following libraries to create data visualizations as well as properly format and filter our data.\n\n#Import libraries\nimport opendatasets as od\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nWe read in our data using pandas and will use the head function to check out the data.\n\n#Check dataframe\ndf.head()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n0\n4.00\n10.0\n\n\n1\n3.80\n25.0\n\n\n2\n3.93\n45.0\n\n\n3\n3.40\n10.0\n\n\n4\n3.20\n4.0\n\n\n\n\n\n\n\nThe head result indicates that the data has been read in correctly. In the next section, we will begin to clean the data.\n\n\n0.2.2 Data Cleaning\nWe will begin our data cleaning by getting some basic summary statistics from the dataframe.\n\n#Describe dataset\ndf.describe()\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\ncount\n193.000000\n193.000000\n\n\nmean\n3.586166\n17.476684\n\n\nstd\n0.285482\n11.408980\n\n\nmin\n2.600000\n2.000000\n\n\n25%\n3.400000\n10.000000\n\n\n50%\n3.620000\n15.000000\n\n\n75%\n3.800000\n20.000000\n\n\nmax\n4.300000\n69.000000\n\n\n\n\n\n\n\nThe resulting dataframe shows that we have 193 rows of data where on average the gpa is 3.58 with a standard deviation of .28. Average study hours show 17.47 and standard deviation of 11.4, meaning we have a large spread in the distribution of study hours but not too much in gpa. From the summary statistics, we can easily see that the max value is invalid as the scale we are using for this analysis is 0-4. We will eliminate any rows that fall outside of this range.\n\n#Check datatypes\ndf.dtypes\n\ngpa            float64\nstudy_hours    float64\ndtype: object\n\n\nChecking the datatypes of each of the columns, we see that each is of type float. This confirms that all rows in each column are of the same datatype and, in theory, we do not need to clean up any corrupted data/poorly entered data.\n\n#Check for null values\ndf.isnull().sum()\n\ngpa            0\nstudy_hours    0\ndtype: int64\n\n\nIn addition, checking for nulls, we see there are none so we do not need to do anything with those. In the next code chunk, we will eliminate erroneous gpa entries.\n\n#Convert outlier GPA (erroneous entries) to 4.0\ndf = df.assign(gpa = lambda x: np.where(x.gpa &gt; 4, 4, x.gpa))\n\nIn the above chunk, we use the assign function to override the gpa column and change values that are above 4 to just 4 while leaving all others the same."
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#data-viz",
    "href": "posts/gpa-analysis/gpa_analysis.html#data-viz",
    "title": "Simple Linear Regression",
    "section": "0.3 Data Viz",
    "text": "0.3 Data Viz\nThe first visualization we will look at is comparing the distributions of gpa and study hours. We want to get a side by side look so we will use the subplots functionality from matplotlib. As well, we will plot the median of the two respective distributions to know where the 50% percentile lies. The following code produces Figure 1.\n\n# Check distributios of gpa and study hours\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.histplot(x=\"gpa\", data=df, ax=ax1, bins=15)\nax1.axvline(df.gpa.median(), color=\"black\", linestyle=\"dashed\")\nax1.text(df.gpa.median() - 0.3, ax1.get_ylim()[1] * 0.9, f\"Median: {df.gpa.median():.2f}\")\n\nsns.histplot(x=\"study_hours\", data=df, ax=ax2, color=\"red\", bins=15)\nax2.axvline(df.study_hours.median(), color=\"black\", linestyle=\"dashed\")\nax2.text(df.study_hours.median() + 2, ax2.get_ylim()[1] * 0.9, f\"Median: {df.study_hours.median():.2f}\");\n\n#fig.suptitle(\"Distributions of GPA and Study Hours\");\n\n\n\n\nFigure 1: Distributions of GPA and Study Hours\n\n\n\n\nFrom Figure 1, we see that GPA appears to be slightly left skewed with a couple of outliers to the left of the distribution. Study hours appears to be more right skewed with more outliers towards the right of the distribution. Median for GPA is 3.62 and 15 for study hours.\nSince we are working with two quantitative variables, we can use a scatterplot to view how linear their relationship (ie correlation) as well as view the where each gpa falls for a given amount of study hour. The below code produces Figure 2.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 2: Scatterplot of GPA by Study Hours\n\n\n\n\nFigure 2 shows that the linear relationship between study hours and GPA is not very strong. An exact score of .14 for the correlation indicates a weak linear relationship. Further more, like we already saw from the histograms, the data appears to be more grouped towards the beginning of the x axis (0-20ish hours).\nWe can attempt to adjust the distribution and correlation of the dataset by removing outliers. However, because the outlying dots appear to follow the trend of the data (more or less…) and we always like keeping as much data as possible, this is merely an optional step to fulfill our curiosity.\nWe will define an outlier as a point that falls outside of a box-plot range. Below is the function we will use to define and identify outliers.\n\n#Identify outliers\ndef find_outliers(x, column_name):\n    q1 = x[column_name].quantile(.25)\n    q3 = x[column_name].quantile(.75)\n    iqr = q3 - q1\n    lower = q1 - (1.5 * iqr)\n    upper = q3 + (1.5 * iqr)\n    outliers = x[(x[column_name] &lt; lower) | (x[column_name] &gt; upper)]\n    \n    return outliers\n    \n\nThe function calculates the IQR of the dataset and then returns a filtered dataframe based on whether a point passes the upper or lower bound of the box-plot. For example, below we see how the function identifies outliers using the GPA attribute.\n\n#Find outliers for gpa\nfind_outliers(df, \"gpa\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n108\n2.6\n7.0\n\n\n\n\n\n\n\nSimilarily, we can view the outliers for study hours below.\n\n#Find outliers for study hours\nfind_outliers(df, \"study_hours\")\n\n\n\n\n\n\n\n\ngpa\nstudy_hours\n\n\n\n\n2\n3.930\n45.0\n\n\n7\n3.400\n40.0\n\n\n13\n3.830\n60.0\n\n\n51\n3.868\n40.0\n\n\n57\n3.125\n36.0\n\n\n77\n3.566\n40.0\n\n\n83\n3.850\n69.0\n\n\n89\n3.700\n45.0\n\n\n122\n3.750\n40.0\n\n\n125\n3.500\n49.0\n\n\n130\n3.825\n60.0\n\n\n135\n3.600\n40.0\n\n\n169\n3.830\n60.0\n\n\n\n\n\n\n\nFigure 3 below shows which points are labeled as outliers from our scatterplot.\n\n#Plot original scatterplot\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df)\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\")\n\n#Show outliers by study hours (color red)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"study_hours\"), color='red')\n\n#Show outliers by gpa (color yellow)\nsns.scatterplot(x='study_hours', y='gpa', data=find_outliers(df, \"gpa\"), color='yellow');\n\n\n\n\nFigure 3: Scatterplot of GPA and Study Hours (highlighted outliers)\n\n\n\n\nAs already indicated, the red dots are outliers according to the “study hours” metric while yellow indicates outlier for “gpa”. While the red dots are technically “outliers”, they do sort of follow the general trend and direction of the data whereas the yellow dot does not. Let’s try removing the yellow dot to see how it changes the correlation of the data.\n\n#Remove outliers from dataset\ndf_filtered = df.query(\"gpa &gt; 2.6\")\n\nAfter removing the gpa outlier, the scatterplot and corresponding correlation are as follows.\n\n#Scatterplot of gpa and study hours\nplt.figure(figsize=(15,5))\nsns.regplot(x='study_hours', y='gpa', data=df_filtered)\ncorr_coef = np.corrcoef(df_filtered.study_hours, df_filtered.gpa)[0][1]\nplt.text(50, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 4: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nWe see that removing the gpa outlier had a minimal effect on the correlation (actually slightly decreased) thus “weakening” the linear relationship of the variables. Perhaps we can try a non-linear transformation to aid our non-linear data.\nTo do this, we will perform a log transformation on both gpa and study hours (on our original dataset) and a polynomial transformation on study hours. Below is the result of our transformations.\n\n#Plot log transformed data\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (18, 5))\nsns.regplot(x='study_hours', y='gpa', data=np.log(df), ax=ax1)\nax1.set_xlabel(\"log(study_hours)\")\nax1.set_ylabel(\"log(gpa)\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax1.text(np.log(30), np.log(2.8), f\"Correlation: {corr_coef:.2f}\")\n\n#Plot polynomial transformed data\ndf_poly = df.assign(study_hours = lambda x: x.study_hours**2)\nsns.regplot(x='study_hours', y='gpa', data=df_poly, ax=ax2)\nax2.set_xlabel(\"study_hours^2\")\ncorr_coef = np.corrcoef(df.study_hours, df.gpa)[0][1]\nax2.text(50**2, 2.8, f\"Correlation: {corr_coef:.2f}\");\n\n\n\n\nFigure 5: Scatterplot of GPA by Study Hours (filtered)\n\n\n\n\nIn Figure 5, we see that our transformations really had not effect on the linear relationship. For the sake of getting on with the modeling, we will use our base dataset and interpret the model accordingly (barring that the other model assumptions hold)."
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#classical-modeling",
    "href": "posts/gpa-analysis/gpa_analysis.html#classical-modeling",
    "title": "Simple Linear Regression",
    "section": "0.4 Classical Modeling",
    "text": "0.4 Classical Modeling\nThis section will be our first attempt at modeling the data, using classical (frequentist) style approaches. Even though this is thebayesianbandit, I like providing frequentist approaches as well to show the differences and similarities between normal statistical modeling (frequentist) and bayesian modeling.\n\n0.4.1 Linear Regression Framework\nSince we are dealing with “linear” data, it would make sense to attempt to attempt to fit a linear model to the data. Therefore, we will be utilizing a simple linear regression (SLR) model to model the relationship between GPA and study hours (per week). Below is the equation we will use.\n\\[\ny = X\\beta + \\epsilon\n\\tag{1}\\] \\[\\epsilon \\sim N(\\mu, \\sigma^2)\\]\nEquation 1 models our response variable is equal to our matrix of explanatory variables (in this case, just one) multiplied (scaled) by a matrix of betas, plus errors (residuals) that are distributed normally with mean \\(\\mu\\) and variance \\(\\sigma^2\\)\nIn linear algebra/matrix notation, it would look something like this.\n\\[\n\\begin{bmatrix}\ny_1 \\\\\n... \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1 \\\\\n... & ... \\\\\n1 & x_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n... \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\tag{2}\\]\nWe utilize ordinary least squares (OLS) to estimate our coefficients (\\(\\beta\\) values) by minimizing the residual sum of squares (RSS). Below are the equations we will use.\n\\[\nRSS = \\epsilon^2_1 + ... + \\epsilon^2_n\n\\tag{3}\\]\n\\[\n\\epsilon = y_i - \\hat{y_i}\n\\tag{4}\\]\n\\[\n\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\tag{5}\\]\n\\[\n\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x}\n\\tag{6}\\]\nBy estimating our coefficients like this, we have a mathematical framework of estimating the line of best fit (the average line between all data points). This yields are final equation below.\n\\[\n\\hat{y} = X\\hat{\\beta}\n\\tag{7}\\]\nWhere \\(\\hat{y}\\) is our “predicted” value based off our inputs in the matrix of \\(X\\), scaled/multiplied by \\(\\hat{\\beta}\\), our estimated coefficients.\n\n\n0.4.2 Calculate beta values and interpret results\nNow that we have our framework by which we will fit our data, we can use Python libraries to perform the above calculations.\n\n#Import statsmod and fit SLR model\nimport statsmodels.api as sm\ny = df['gpa']\nX = pd.DataFrame({'intercept':np.ones(df.shape[0]), 'study_hours':df['study_hours']})\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n\n#Print summary from SLR model\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngpa\nR-squared:\n0.019\n\n\nModel:\nOLS\nAdj. R-squared:\n0.014\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.714\n\n\nDate:\nWed, 06 Sep 2023\nProb (F-statistic):\n0.0555\n\n\nTime:\n20:33:28\nLog-Likelihood:\n-27.443\n\n\nNo. Observations:\n193\nAIC:\n58.89\n\n\nDf Residuals:\n191\nBIC:\n65.41\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n3.5249\n0.037\n95.292\n0.000\n3.452\n3.598\n\n\nstudy_hours\n0.0034\n0.002\n1.927\n0.055\n-8.05e-05\n0.007\n\n\n\n\n\n\nOmnibus:\n10.671\nDurbin-Watson:\n2.385\n\n\nProb(Omnibus):\n0.005\nJarque-Bera (JB):\n11.481\n\n\nSkew:\n-0.595\nProb(JB):\n0.00321\n\n\nKurtosis:\n2.897\nCond. No.\n38.3\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nFrom the summary, we have \\(\\beta_0\\) = 3.5249 and \\(\\beta_1\\) = .0034, meaning on average someone who studies 0 hours a week would see a GPA value of about 3.5. In addition, on average, for an increase of 1 hour in study time, we would see an increase of about .0034 in GPA. This isn’t too surprising considering the weak linear relationship we observed earlier in Figure 2. Essentially, if I were a student in this class, in order to change a whole letter grade (i.e. go from B to B+) and assuming I was right at B level (3.0), I would need to study 88 extra hours. Essentially, this model says that you’re stuck where you’re at.\nNevertheless, we can gain confidence in knowing a couple of things.\n\n\nThe data has a fairly weak linear relationship, so using a linear model to fit the data may not tell the most accurate story on how GPA is influenced by study hours.\n\n\nThe data could be inaccurate due to self-reporting\n\n\n\n\n0.4.3 Is study hours a statistically significant variable in determining GPA?\nTo determine the answer to the above question, we pose the following hypotheses.\n\\[\nH_0: \\beta_1 = 0\n\\] \\[\nH_a: \\beta_1 \\neq 0\n\\]\nThe aim of these hypotheses is to determine whether or not \\(\\beta_1\\) on average always has some kind of impact on GPA (whether it is an increase or decrease, no one knows in these things). We set our \\(\\alpha\\) = .05 to run our hypotheses.\nThe p-value we obtain from the summary print out indicates that p-value &gt; \\(\\alpha\\), so we fail to reject \\(H_0\\) and conclude that there isn’t sufficient evidence to reject the null. Essentially, we don’t have enough statistical evidence from the data to prove that study hours, on average, will never have a 0 impact on GPA.\nTo confirm this notion, we can view the corresponding 95% confidence interval for \\(\\beta_1\\) provided in the summary tab. The values of (-.000085, .007) indicate that 0 is included in the interval and therefore, the average change on GPA based on study hours can be 0.\n\n\n0.4.4 Check assumptions of linear regression model\nEven though the \\(\\beta_1\\) value proved to not be statistically significant (though very close), we should still verify that the assumptions of our linear model hold with this data set. The assumptions for a linear regression model are listed below.\n\n\nLinearity\n\n\nIndependence\n\n\nNormality\n\n\nEqual Variance\n\n\nWe already verified that there is a weak linear relationship, but still a linear relationship since correlation was not 0. Independence is verified by the fact that each observation in the study was independent of one another, thus making them i.i.d random variables.\nFor the normality assumption, we can plot a histogram of the residuals to check that the residuals are approximately normally distributed. Below is the graph.\n\n#Plot histogram of residuals\nplt.figure(figsize=(15,5))\nsns.histplot(results.resid);\n\n\n\n\nFigure 6: Histogram of resiudals\n\n\n\n\nAs we can see in Figure 6, the distribution looks fairly left skewed with a long left tail. For ease of this analysis/exercise, we will say this is “approximately” normal, but this would more than likely fail in the real world.\nTo verify equal variance, we will view a scatterplot of the fitted vs residual values to make sure that there appears to be equal variance (good randomness, no trends) in the graph.\n\n#Plot scatterplot of fitted vs residuals\nplt.figure(figsize=(15,5))\nsns.scatterplot(x=results.fittedvalues,y=results.resid);\n\n\n\n\nFigure 7: Scatterplot of fitted vs residuals\n\n\n\n\nIn Figure 7, there appears to be no significant trends in the data (though it does tend to begin to taper as we increase across the x-axis). However, since there is nothing obvious or glaring, we can verify this assumption.\n\n\n0.4.5 Do students spend on average 9 hours a week studying?\nFrom my past university experience, many professors have told me that for every 1 hour of lecture, should be 3 hours of study/homework time. Assuming this class was a 3 credit hour class, most students should be spending about 9 hours studying outside of class. To test this theory, we will use a t-test for the following hypotheses.\n\\[H_0: \\mu = 9\\] \\[H_a: \\mu \\neq 9\\]\n\n#Perform t-test to see if students on average spend 9 hours studying\nfrom scipy.stats import ttest_1samp\nttest_1samp(a=df['study_hours'], popmean=9)\n\nTtestResult(statistic=10.32185697364204, pvalue=3.767865100544389e-20, df=192)\n\n\nThe above result with a super small p-value (less than \\(\\alpha\\) = .05) indicates that we can reject the null hypothesis and conclude that students do not study an average of 9 hours for this class. To determine what the range of possible average values are, we will calculate a 95% confidence interval below.\n\n#Calculate 95% confidence interval\nttest_1samp(a=df['study_hours'], popmean=9).confidence_interval()\n\nConfidenceInterval(low=15.856880282374465, high=19.096487593273206)\n\n\nWe are 95% confident that the true average study time spent is between 15.85 to 19.09 hours. So, on average, students are spending a lot more time studying for this class than the hoped benchmark set by this university (assuming this university has set the same benchmark as my other ones)."
  },
  {
    "objectID": "posts/gpa-analysis/gpa_analysis.html#bayesian-modeling",
    "href": "posts/gpa-analysis/gpa_analysis.html#bayesian-modeling",
    "title": "Simple Linear Regression",
    "section": "0.5 Bayesian Modeling",
    "text": "0.5 Bayesian Modeling\nFinally, getting into the bayesian part of the analysis! As with our classical approach, we will layout the framework we will use for this section. We will be using the bayesian model (Bayes theorem) shown below.\n\\[P(H|\\theta) = \\frac{P(\\theta|H) P(H)}{P(\\theta)} \\tag{8}\\]\n\\[H = \\text{Our Hypothesis (prior)}\\] \\[\\theta = \\text{Our Data}\\]"
  },
  {
    "objectID": "posts/welcome/welcome_to_blog.html",
    "href": "posts/welcome/welcome_to_blog.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "TL;DR\nThis blog focuses on the use of statistics, particularly bayesian statistics, in the application of business problems. In this post, we review the necessary math background in order to understand future blog posts. As well, we establish expectations for future blog posts and the order in which they are shared.\n\n\nWelcome To My Blog!\nWelcome to my blog! If you are reading this, I hope that means you are as excited about learning statistics as I am about teaching it (and continuing to learn it!). This blog, honestly, is mostly a tool for me to improve my own knowledge in statistics by learning how I would convey it to others. I plan on doing this from a data analysis perspective. Given a certain situation/set of data points, how do we construct the problems we want to solve and implement proper solutions. Particularly, I hope to demonstrate that Bayesian methods can be viable methods to solve problems in business, healthcare, etc. Anyways, I hope you find this blog interesting and useful in your data career. Whether you are just entering data or are an experienced data professional, this blog intends to be a resource in demonstrating good data analysis with sound applied mathematical theory.\n\n\nPrerequisites\nWhile I did mention above that this blog is intended to be useful for beginners and experts alike, I will admit that I will be covering some higher level math, such as:\n\n\nMultivariate Calculus\n\n\nProbability Theory\n\n\nLinear Algebra\n\n\nFor example, suppose we are studying a dataset with a variable that is modeled as an exponential random variable, we would use the below notation to show the probability density function (PDF)\n\\[\\begin{equation}\n\\int_{0}^{\\infty} \\lambda e^{\\lambda x} \\,dx\n\\end{equation}\\]\n\\[\\begin{equation}\nX \\sim Exponential(\\lambda)\n\\end{equation}\\]\nOr, the important Bayes Theorem found below.\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\end{equation}\\]\nDon’t be discourage if the above formulas don’t make sense right now. In each post that these appear, I will breakdown what they mean and their applicability in solving our problem.\n\n\nExpectations\nThe hope of this blog is to present statistics in the form of a data analysis case. For example, let’s say you are a host on AirBnB and want to maximize the number of nights you rent out. How do you properly price your rental given certain parameters (size, geographic setting, etc). We would then present the data points and walk through an analysis of the data by doing the following.\n\n\nExploring the data (EDA) to get to know our dataset\n\n\nConstructing a mathematical framework that can fit our data\n\n\nFit our data to said framework\n\n\nGather inference from our model\n\n\nReview answers explained by our model\n\n\nExplain possible enhancements to our model for future analyses\n\n\nThis will be our attempted framework to blog posts. I hope that by tackling problems in this way, our analytical toolbox will grow and our ability to construct measurable problems from our data will improve. This is the ultimate goal of this blog. I really hope that people will recognize this blog as an opportunity to learn how to think analytically.\n\n\nLet’s do this\nI just want to reiterate that I am excited to learn how to be a better analyst with you by getting to know the math that drives our analytics. I am very passionate about how data can be used to properly drive decision-making in organizations and I hope that this blog motivates you to do the same."
  }
]