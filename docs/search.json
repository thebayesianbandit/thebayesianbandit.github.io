[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brandon Scott",
    "section": "",
    "text": "I love learning and teaching so this blog is my motivation to learn new topics and learn how to best convey the messages I received. I hope to demonstrate in this blog an analytical way to approach problems. I like using a tool-box method in my posts, approaching a problem and demonstrating how various tools can be used to solve it. I hope you enjoy this blog as much as I enjoy writing it!"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Brandon Scott",
    "section": "Education",
    "text": "Education\nBrigham Young University MBA (Marketing) | September 2024 - April 2026\nTexas A&M University M.S. in Statistics | January 2023 - May 2024\nUtah Valley University M.S. in Computer Science | September 2021 - April 2022 (transferred)\nBrigham Young University B.S. in Actuarial Science | September 2017 - April 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Brandon Scott",
    "section": "Experience",
    "text": "Experience\nUber | Data Analyst | October 2022 - July 2023 AdvancedMD | Senior Business Analyst | July 2021 - October 2022"
  },
  {
    "objectID": "code_books/proj_code.html",
    "href": "code_books/proj_code.html",
    "title": "Media Mix Modeling (MMM)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport random as rd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom causalml.inference.meta import BaseTRegressor\nfrom causalml.metrics import auuc_score, qini_score\n\nimport xgboost as xgb\n\nimport jax.numpy as jnp\nimport jax.random as random\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n/home/thebscotte/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n#Define function for channel activity\ndef spend_data(n, cut=.9):\n    spend = np.random.uniform(0, 1, n)\n    return np.where(spend &gt; cut, spend, spend / 2)\n\n#Define function for saturation link \ndef saturation_link(x, a=1, b=.1, n_slope=1):\n    return (a * x**n_slope) / (b**n_slope + x**n_slope)\n\n#Define function for carryover link\ndef carryover_link(x, k=.3, l=7):\n    w = np.array([np.power(k, i) for i in range(l)])\n    xx = np.vstack([np.append(np.zeros(i), x[:len(x)-i]) for i in range(l)])\n    y = np.dot(w / np.sum(w), xx)\n    \n    return y\n\n#Define function for saturation and carryover \ndef sat_w_carry_link(x, a, b, k, l):\n    sat = saturation_link(x, a, b)\n    \n    return carryover_link(sat, k, l)\n\n#Define function for generating mmm data\ndef gen_mmm_data(num_channels, l):\n    min_date = pd.to_datetime('2024-01-01')\n    max_date = pd.to_datetime('2024-12-31')\n    beta_true = np.array([3, 6, 9])\n    a_true = np.array([1, 2, 3])\n    b_true = np.array([.25, .45, .65])\n    k_true = np.array([.2, .3, .4])\n    baseline = 8\n    \n    df = pd.DataFrame(index=pd.date_range(min_date, max_date, freq='W'))\n    \n    \n    raw_activity = [spend_data(df.shape[0], ) for i, j in zip(range(num_channels), [.9,.8, .7])]\n    link_activity = [sat_w_carry_link(raw_activity[i], a_true[i], b_true[i], k_true[i], l) for i in range(num_channels)]\n    \n    for i in range(num_channels):\n        df[f'channel_{i+1}'] = raw_activity[i]\n        df[f'channel_{i+1}_link'] = link_activity[i]\n        \n        \n    df['noise'] = .2 * np.random.uniform(-1, 1, df.shape[0])\n    df['ticket_size'] = baseline + np.dot(np.array(link_activity).T, beta_true) + df['noise'].values\n    \n    return df\n\n\n#Plot spend\nt_s = spend_data(52)\nplt.plot(t_s)\n\n\n\n\n\n#Plot saturation\nplt.plot(np.linspace(0,1,100), saturation_link(np.linspace(0,1,100),a=3))\n\n\n\n\n\n#Plot carryover\nx_t = np.zeros(100)\nx_t[0] = 10\nplt.plot(x_t, carryover_link(x_t, k=.8, l=12))\n\n\n\n\n\n#Generate synthetic data\ntemp_df = gen_mmm_data(3, 2)\n\n\ntemp_df\n\n\n\n\n\n\n\n\nchannel_1\nchannel_1_link\nchannel_2\nchannel_2_link\nchannel_3\nchannel_3_link\nnoise\nticket_size\n\n\n\n\n2024-01-07\n0.313123\n0.463373\n0.182564\n0.444015\n0.986273\n1.291620\n0.093320\n23.772107\n\n\n2024-01-14\n0.130625\n0.378663\n0.156092\n0.529417\n0.181175\n0.983735\n-0.097775\n21.068333\n\n\n2024-01-21\n0.137517\n0.352920\n0.057482\n0.293123\n0.380882\n0.978560\n-0.122185\n19.502355\n\n\n2024-01-28\n0.179916\n0.407887\n0.346990\n0.722087\n0.376330\n1.102422\n-0.180355\n23.297628\n\n\n2024-02-04\n0.313146\n0.533136\n0.289682\n0.803451\n0.160485\n0.738601\n0.116493\n21.184021\n\n\n2024-02-11\n0.276677\n0.530449\n0.213372\n0.675596\n0.151015\n0.573715\n-0.025053\n18.783303\n\n\n2024-02-18\n0.221230\n0.478783\n0.945779\n1.190913\n0.266757\n0.785124\n0.164530\n23.812473\n\n\n2024-02-25\n0.062295\n0.244474\n0.928521\n1.348989\n0.298862\n0.924344\n0.107690\n25.254140\n\n\n2024-03-03\n0.190831\n0.393987\n0.324362\n0.955301\n0.411435\n1.100590\n0.004299\n24.823375\n\n\n2024-03-10\n0.405585\n0.587699\n0.100335\n0.473814\n0.988796\n1.625177\n0.023074\n27.255650\n\n\n2024-03-17\n0.042100\n0.223217\n0.436780\n0.841909\n0.261717\n1.132299\n-0.174686\n23.737112\n\n\n2024-03-24\n0.405672\n0.539614\n0.104152\n0.516481\n0.249779\n0.840908\n-0.075140\n20.210766\n\n\n2024-03-31\n0.134571\n0.394722\n0.075143\n0.306886\n0.253175\n0.838622\n-0.056606\n18.516474\n\n\n2024-04-07\n0.134912\n0.350405\n0.181729\n0.508611\n0.260196\n0.852845\n-0.107811\n19.670680\n\n\n2024-04-14\n0.935103\n0.715957\n0.390497\n0.847544\n0.306761\n0.932082\n0.044482\n23.666360\n\n\n2024-04-21\n0.040351\n0.247320\n0.292567\n0.820577\n0.237694\n0.848604\n-0.029252\n21.273610\n\n\n2024-04-28\n0.003011\n0.033081\n0.165295\n0.595142\n0.139231\n0.607542\n0.143709\n17.281686\n\n\n2024-05-05\n0.369864\n0.499222\n0.165742\n0.538103\n0.395469\n0.961789\n0.001786\n21.384177\n\n\n2024-05-12\n0.054175\n0.247869\n0.430851\n0.876742\n0.079834\n0.558631\n-0.182165\n18.849570\n\n\n2024-05-19\n0.258685\n0.453465\n0.154351\n0.618675\n0.125724\n0.441060\n0.068887\n17.110870\n\n\n2024-05-26\n0.000667\n0.086972\n0.207732\n0.603771\n0.297871\n0.812318\n-0.061751\n19.132657\n\n\n2024-06-02\n0.003438\n0.011746\n0.320335\n0.785519\n0.945074\n1.538993\n-0.091898\n26.507390\n\n\n2024-06-09\n0.344242\n0.485007\n0.431587\n0.945089\n0.052980\n0.669350\n0.143734\n21.293445\n\n\n2024-06-16\n0.146939\n0.405033\n0.040204\n0.352125\n0.074348\n0.284545\n0.169715\n14.058470\n\n\n2024-06-23\n0.226208\n0.457546\n0.914834\n1.069067\n0.263844\n0.706662\n-0.078362\n22.068638\n\n\n2024-06-30\n0.220672\n0.469874\n0.939133\n1.349452\n0.064147\n0.439951\n0.195140\n21.661030\n\n\n2024-07-07\n0.069399\n0.259207\n0.304445\n0.932849\n0.378571\n0.865681\n-0.023522\n22.142319\n\n\n2024-07-14\n0.196291\n0.402736\n0.934124\n1.224531\n0.946639\n1.585965\n-0.117486\n30.711594\n\n\n2024-07-21\n0.286770\n0.518514\n0.925966\n1.346804\n0.225118\n1.059431\n-0.058587\n27.112658\n\n\n2024-07-28\n0.150527\n0.402228\n0.116997\n0.628049\n0.070631\n0.430522\n0.126611\n16.976292\n\n\n2024-08-04\n0.041720\n0.181815\n0.024316\n0.174106\n0.326249\n0.800125\n-0.011937\n16.779267\n\n\n2024-08-11\n0.351548\n0.510840\n0.356780\n0.704010\n0.054257\n0.451533\n-0.148981\n17.671400\n\n\n2024-08-18\n0.213627\n0.481379\n0.143971\n0.577009\n0.342912\n0.806092\n-0.127661\n20.033353\n\n\n2024-08-25\n0.407676\n0.593357\n0.144163\n0.485151\n0.209092\n0.817567\n-0.052739\n19.996338\n\n\n2024-09-01\n0.261414\n0.529279\n0.272632\n0.692409\n0.378658\n0.997422\n-0.161233\n22.557852\n\n\n2024-09-08\n0.123621\n0.360921\n0.971257\n1.225480\n0.303595\n0.997740\n0.079068\n25.494368\n\n\n2024-09-15\n0.420758\n0.577885\n0.418180\n1.056443\n0.155656\n0.686897\n-0.060060\n22.194322\n\n\n2024-09-22\n0.305875\n0.563097\n0.308612\n0.848175\n0.199131\n0.668129\n0.199051\n20.990555\n\n\n2024-09-29\n0.414946\n0.611735\n0.098326\n0.463637\n0.935950\n1.465619\n-0.009188\n25.798411\n\n\n2024-10-06\n0.367966\n0.600211\n0.246722\n0.627560\n0.357380\n1.266047\n-0.182042\n24.778380\n\n\n2024-10-13\n0.332042\n0.574639\n0.902262\n1.189938\n0.426729\n1.153339\n-0.163653\n27.079942\n\n\n2024-10-20\n0.384761\n0.600206\n0.038491\n0.429175\n0.962087\n1.618551\n-0.024229\n26.918400\n\n\n2024-10-27\n0.387557\n0.607590\n0.987459\n1.093210\n0.140849\n0.893179\n0.073218\n24.493863\n\n\n2024-11-03\n0.111724\n0.358701\n0.311100\n0.945900\n0.104531\n0.449522\n0.105973\n18.903176\n\n\n2024-11-10\n0.169724\n0.388452\n0.280461\n0.779347\n0.256157\n0.724501\n0.004641\n20.366593\n\n\n2024-11-17\n0.403007\n0.581692\n0.432403\n0.931099\n0.345756\n0.986366\n-0.118564\n24.090395\n\n\n2024-11-24\n0.306734\n0.561986\n0.365158\n0.915336\n0.246130\n0.886181\n0.090747\n23.244347\n\n\n2024-12-01\n0.999109\n0.758373\n0.414032\n0.943960\n0.426017\n1.083822\n-0.064920\n25.628362\n\n\n2024-12-08\n0.334666\n0.610314\n0.031606\n0.322126\n0.150571\n0.742389\n-0.160226\n18.284969\n\n\n2024-12-15\n0.096870\n0.328125\n0.362569\n0.716751\n0.007241\n0.184818\n-0.161674\n14.786573\n\n\n2024-12-22\n0.164077\n0.376751\n0.142551\n0.576048\n0.209629\n0.532000\n0.002177\n17.376719\n\n\n2024-12-29\n0.172926\n0.406775\n0.050690\n0.266786\n0.246227\n0.797745\n0.154448\n18.155198\n\n\n\n\n\n\n\n\n#Keep necessary columns for modeling\ntemp_df = temp_df.drop(['channel_1_link', 'channel_2_link', 'channel_3_link', 'noise'], axis=1)\n\n\n#Define functions for bayesian mmm links\ndef jax_saturation_link(x, a=1, b=.1, n_slope=1):\n    return (a * x**n_slope) / (b**n_slope + x**n_slope)\n\n#Define function for carryover link\ndef jax_carryover_link(x, k=.3, l=2):\n    w = jnp.array([jnp.power(k, i) for i in range(l)])\n    xx = jnp.vstack([jnp.append(jnp.zeros(i), x[:len(x)-i]) for i in range(l)])\n    y = jnp.dot(w / jnp.sum(w), xx)\n    \n    return y\n\n\n#Define function for bayesian mmm\ndef bayes_mmm_orders(X_vars, y_vars=None):\n    baseline = numpyro.sample('baseline', dist.Normal(5, 5))\n    beta = numpyro.sample('beta', dist.Normal(5,3).expand([X_vars.shape[1]]))\n    alpha = numpyro.sample('alpha', dist.Gamma(3, 2).expand([X_vars.shape[1]]))\n    b = numpyro.sample('b', dist.Beta(2, 2).expand([X_vars.shape[1]]))\n    gamma = numpyro.sample('gamma', dist.Beta(2, 2).expand([X_vars.shape[1]]))\n    sigma = numpyro.sample('sigma', dist.HalfNormal(1))\n    \n    X_vars_sat = jax_saturation_link(X_vars, alpha, b)\n    X_vars_link = jnp.array([jax_carryover_link(X_vars_sat[:,i], gamma[i]) for i in range(X_vars.shape[1])]).T\n    \n    mu = baseline + jnp.dot(X_vars_link, beta)\n    \n    with numpyro.plate(\"data\", X_vars.shape[0]):\n        numpyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y_vars)\n\n\n#Define and run MCMC\nrng_key = random.PRNGKey(0)\nkernel = NUTS(bayes_mmm_orders)\nmcmc = MCMC(kernel, num_warmup=1000, num_samples=2000)\nmcmc.run(rng_key, temp_df.drop(['ticket_size'], axis=1).values, temp_df['ticket_size'].values)\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\nsample: 100%|██████████████████████████| 3000/3000 [00:19&lt;00:00, 156.91it/s, 511 steps of size 4.73e-03. acc. prob=0.91]\n\n\n\n#Plot trace plots of betas\nfig, ax = plt.subplots(1,3, figsize=(15,3))\n\nfor i, a in enumerate(ax.flatten()):\n    a.plot(mcmc.get_samples()['beta'][:,i])\n    a.set_title(f\"beta[{i}]\")\n    \nfig.tight_layout();\n\n\n\n\n\n#View summary\nprint(mcmc.print_summary())\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n  alpha[0]      1.02      0.59      0.87      0.27      1.75   1340.54      1.00\n  alpha[1]      1.96      0.61      1.85      1.03      2.85   1767.24      1.00\n  alpha[2]      3.16      0.75      3.06      1.92      4.26   1554.36      1.00\n      b[0]      0.29      0.04      0.28      0.22      0.35   1693.80      1.00\n      b[1]      0.48      0.02      0.48      0.45      0.52   1684.91      1.00\n      b[2]      0.66      0.02      0.66      0.63      0.69   2184.57      1.00\n  baseline      8.20      0.12      8.20      8.02      8.42   1493.97      1.00\n   beta[0]      3.95      2.03      3.51      1.01      7.07   1215.27      1.00\n   beta[1]      6.96      2.04      6.75      3.56     10.04   1685.56      1.00\n   beta[2]      8.95      2.07      8.75      5.51     12.11   1329.96      1.00\n  gamma[0]      0.16      0.03      0.16      0.11      0.22   2448.76      1.00\n  gamma[1]      0.32      0.01      0.32      0.31      0.34   2594.03      1.00\n  gamma[2]      0.39      0.01      0.39      0.39      0.41   2405.02      1.00\n     sigma      0.11      0.01      0.11      0.09      0.13   1936.12      1.00\n\nNumber of divergences: 3\nNone\n\n\n\n#View posterior densities\nfig, ax = plt.subplots(1,3, figsize=(15,3))\n\nfor i, a in enumerate(ax.flatten()):\n    sns.kdeplot(mcmc.get_samples()['beta'][:,i], ax=a)\n    a.set_title(f\"beta[{i}]\")\n    \nfig.tight_layout();\n\n\n\n\n\nUplift Causal Modeling\n\n#Define function for uplift synthetic data\ndef generate_synthetic_uplift(n_samples=1000, random_seed=42):\n    np.random.seed(random_seed)\n\n    X = pd.DataFrame({\n        'age': 18 + np.random.binomial(30, .3, n_samples),\n        'last_order_days': np.random.randint(0, 30, n_samples),\n        'loyalty_tier': np.random.choice(['bronze', 'silver', 'gold'], n_samples),\n        'gender': np.random.choice(['Male', 'Female'], n_samples)\n    })\n\n    X = pd.get_dummies(X, columns=['loyalty_tier', 'gender'], drop_first=True)\n\n    treatment_prob = 1 / (1 + np.exp(-(0.01 * X['age'] + 0.01 * X['last_order_days'] - 0.4 * X['loyalty_tier_silver'])))\n    T = np.random.binomial(1, treatment_prob)\n\n    U = 0.01 * X['age'] + 0.15 * X['last_order_days']  + 1.4 * X['loyalty_tier_silver'] + .1 * X['loyalty_tier_gold']\n\n    baseline_outcome = 0.1 * X['age'] - 0.1 * X['last_order_days'] + 0.1 * X['loyalty_tier_silver'] + 0.5 * X['loyalty_tier_gold']\n    Y = baseline_outcome + T * U + np.random.normal(0, 0.5, n_samples)\n\n    data = pd.DataFrame({\n        'treatment': T,\n        'outcome': Y,\n        'true_uplift': U\n    })\n    data = pd.concat([data, X], axis=1)\n\n    return data\n\n\n# Generate the dataset\nsynthetic_uplift = generate_synthetic_uplift()\n\n# Display the first few rows\nsynthetic_uplift.head()\n\n\n\n\n\n\n\n\ntreatment\noutcome\ntrue_uplift\nage\nlast_order_days\nloyalty_tier_gold\nloyalty_tier_silver\ngender_Male\n\n\n\n\n0\n1\n3.129651\n2.46\n26\n14\nTrue\nFalse\nFalse\n\n\n1\n0\n2.389504\n3.36\n31\n11\nFalse\nTrue\nFalse\n\n\n2\n0\n0.794982\n6.04\n29\n29\nFalse\nTrue\nTrue\n\n\n3\n0\n1.248120\n3.93\n28\n15\nFalse\nTrue\nTrue\n\n\n4\n0\n-0.152519\n5.09\n24\n23\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\n\n#View average true uplift\nsynthetic_uplift['true_uplift'].mean()\n\n2.98171\n\n\n\n#Train test split\nX = synthetic_uplift.drop(['outcome', 'true_uplift'], axis=1)\ny = synthetic_uplift['outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n#Instantiate model\nbase_t_xgb = BaseTRegressor(xgb.XGBRegressor())\n\n\n#Fit model\nbase_t_xgb.fit(X_train.drop('treatment', axis=1), X_train['treatment'], y_train)\n\n\n#Get ATE\nprint(f\"ATE: {base_t_xgb.estimate_ate(X_test.drop('treatment', axis=1), X_test['treatment'], y_test)}\")\n\nATE: (array([3.14011101]), array([2.9729837]), array([3.30723833]))\n\n\n\n#Get uplift predictions\nbase_t_xgb_cate = base_t_xgb.predict(X_test.drop('treatment', axis=1))\n\n\n#Show CATE distribution\nplt.figure(figsize=(15,5))\nplt.hist(base_t_xgb_cate);\n\n\n\n\n\n#View complete test set\nX_complete = (X_test.assign(uplift_scores = base_t_xgb_cate)\n              .join(synthetic_uplift['true_uplift']))\n\n\nX_complete\n\n\n\n\n\n\n\n\ntreatment\nage\nlast_order_days\nloyalty_tier_gold\nloyalty_tier_silver\ngender_Male\nuplift_scores\ntrue_uplift\n\n\n\n\n521\n1\n26\n11\nFalse\nFalse\nTrue\n1.369953\n1.91\n\n\n737\n1\n29\n13\nTrue\nFalse\nFalse\n2.693130\n2.34\n\n\n740\n1\n27\n28\nFalse\nFalse\nTrue\n4.320621\n4.47\n\n\n660\n0\n26\n3\nTrue\nFalse\nTrue\n0.531828\n0.81\n\n\n411\n1\n31\n21\nTrue\nFalse\nTrue\n3.134103\n3.56\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n506\n0\n30\n5\nFalse\nFalse\nTrue\n0.981644\n1.05\n\n\n342\n1\n25\n1\nFalse\nFalse\nTrue\n0.715153\n0.40\n\n\n485\n1\n30\n5\nTrue\nFalse\nFalse\n0.839881\n1.15\n\n\n711\n1\n28\n24\nFalse\nTrue\nTrue\n5.205279\n5.28\n\n\n133\n0\n26\n3\nFalse\nTrue\nTrue\n1.720232\n2.11\n\n\n\n\n330 rows × 8 columns\n\n\n\n\n\nNext Best Action Model (RL)\n\n#Plot purchase distribution and effect of distance from store\nfig, ax = plt.subplots(2,3, figsize=(15,5))\n\nax[0,0].hist(np.random.lognormal(.75,.3, 100))\nax[0,0].set_title(\"none base distribution\")\n\nax[0,1].hist(np.random.lognormal(1,.2, 100))\nax[0,1].set_title(\"bronze base distribution\")\n\nax[0,2].hist(np.random.lognormal(1.2,.15, 100))\nax[0,2].set_title(\"silver base distribution\")\n\nax[1,0].hist(np.random.lognormal(2,.05, 100))\nax[1,0].set_title(\"gold base distribution\")\n\nax[1,2].hist(-np.exp(-1.5 * np.random.lognormal(1, .5, 100)))\nax[1,2].set_title(\"effect of distance on purchase probability\")\n\nfig.tight_layout();\n\n\n\n\n\nprint(np.__version__)\n\n1.24.2\n\n\n\n#Define function for incorporating uplift into RL\ndef calc_uplift(mark_action, tier, channel):\n    comp_uplift = {\n        'none': {'push': {'ad': .2, 'small discount': .3, 'large discount': 1}, \n                 'text': {'ad': 0, 'small discount': 0, 'large discount': 0}, \n                 'email': {'ad': .1, 'small discount': .2, 'large discount': .75}\n                },\n        'bronze': {'push': {'ad': .4, 'small discount': .7, 'large discount': 1}, \n                 'text': {'ad': .05, 'small discount': .5, 'large discount': 1}, \n                 'email': {'ad': .2, 'small discount': .6, 'large discount': 1}\n                },\n        'silver': {'push': {'ad': .4, 'small discount': 1, 'large discount': 2}, \n                 'text': {'ad': .05, 'small discount': .5, 'large discount': 1}, \n                 'email': {'ad': .3, 'small discount': .8, 'large discount': 1.5}\n                },\n        'gold': {'push': {'ad': -1.5, 'small discount': .2, 'large discount': .2}, \n                 'text': {'ad': -3, 'small discount': 0, 'large discount': 0}, \n                 'email': {'ad': -1.3, 'small discount': .2, 'large discount': .2}\n                }\n    }\n    \n    comp_base = {\n        'none': np.random.lognormal(.75, .3),\n        'bronze': np.random.lognormal(1, .3),\n        'silver': np.random.lognormal(1.2, .15),\n        'gold': np.random.lognormal(2, .05)\n    }\n    \n    if mark_action == 'none':\n        return np.round(comp_base[tier], 2)\n    else:\n        uplift = comp_uplift[tier][channel][mark_action]\n        return np.round(comp_base[tier], 2) + uplift\n\n#Define function for updating loyalty tier (state)\ndef update_tier(current_tier, purchase):\n    tier_transition_purchase = {\n        'none': [.7, .3], #prob of staying, prob of going up\n        'bronze': [.5, .5], #prob of staying, prob of going up\n        'silver': [.5, .5], # prob of staying, prob of going up\n        'gold': [1, 0] # prob of staying\n    }\n    tier_transition_no_purchase = {\n        'none': [1, 0], #prob of staying\n        'bronze': [.6, .4], #prob of staying, prob of going down\n        'silver': [.5, .5], # prob of staying, prob of going down\n        'gold': [.8, .2] # prob of staying, prob of doing down\n    }\n    choices = [0, 1]\n    cond_list = [current_tier == 'none', current_tier == 'bronze', current_tier == 'silver', current_tier == 'gold']\n    choice_list_up = ['bronze', 'silver', 'gold', 'gold']\n    choice_list_down = ['none', 'none', 'bronze', 'silver']\n    \n    if purchase:\n        tier_choice = np.random.choice(choices, p=tier_transition_purchase[current_tier])\n        return np.where(tier_choice == 0, current_tier, np.select(cond_list, choice_list_up))\n    else:\n        tier_choice = np.random.choice(choices, p=tier_transition_no_purchase[current_tier])\n        return np.where(tier_choice == 0, current_tier, np.select(cond_list, choice_list_down))\n            \n    \n\n#Define function for updating probabilities\ndef update_purchase_probs(dict_probs, p):\n    for key, val in dict_probs.items():\n        val[-1] += p\n        total = sum(val)\n        dict_probs[key] = [v / total for v in val]\n        \n    return dict_probs\n\n#Define function for customer profiles\ndef generate_rl_cust(n_users=100, n_t=100, random_seed=42):\n    np.random.seed(42)\n    \n    marketing_channels = [\n        'push', \n        'text', \n        'email'\n    ]\n    \n    events_cust = [\n        0, #None\n        1, #Visit\n        2  #Purchase\n    ]\n    \n    event_probs_dict = {\n        'none': [.7, .25, .05],\n        'bronze': [.6, .1, .3],\n        'silver': [.4, .1, .5],\n        'gold': [.05, .05, .9]\n    }\n    \n    marketing_probs_dict = {\n        'none': {'ad': .05, 'small discount': .2, 'large discount': .5},\n        'bronze': {'ad': .05, 'small discount': .3, 'large discount': .5},\n        'silver': {'ad': .1, 'small discount': .4, 'large discount': .6},\n        'gold': {'ad': -.5, 'small discount': 0, 'large discount': 0}\n    }\n    \n    marketing_costs = {\n        'none': 0,\n        'ad': .75,\n        'small discount': 1.5,\n        'large discount': 2.5\n    }\n    \n    data_dict = {\n        'user_id': [],\n        'time_step': [],\n        'age': [],\n        'gender': [],\n        'loyalty_tier': [],\n        'event': [],\n        'ticket_size': [],\n        'distance_store': [],\n        'marketing_action': [],\n        'marketing_channel':[],\n        'marketing_cost': []\n    }\n    \n    for i in range(1, n_users+1):\n        age = 18 + np.random.binomial(30, .3)\n        tier = np.random.choice(['none', 'bronze', 'silver', 'gold'], p=[.4, .3, .2, .1])\n        gender = np.random.choice(['Male', 'Female'])\n        \n        for t in range(n_t):\n            distance_store = np.round(np.random.lognormal(1, .5), 2)\n            channel = np.random.choice(marketing_channels)\n            action = np.random.choice([k for k in marketing_costs.keys()], p=[.4, .3, .2, .1])\n            cost = marketing_costs[action]\n            distance_eff = np.where(-np.exp(-1.5 * distance_store) &gt; .1, -.1, -np.exp(-1.5 * distance_store)).item()\n            \n            if action == 'none':\n                data_dict['marketing_channel'].append('none')\n                temp_probs_dict = event_probs_dict.copy()\n                temp_probs_dict = update_purchase_probs(temp_probs_dict, distance_eff)\n                event = rd.choices(events_cust, temp_probs_dict[tier], k=1)[0]\n            else:\n                data_dict['marketing_channel'].append(channel)\n                temp_probs_dict = event_probs_dict.copy()\n                change_probs_purchase = marketing_probs_dict[tier][action]\n                temp_probs_dict = update_purchase_probs(temp_probs_dict, change_probs_purchase - distance_eff)\n                \n                event = rd.choices(events_cust, temp_probs_dict[tier], k=1)[0]\n            \n            ticket_size = np.where(event == 2, calc_uplift(action, tier, channel), 0).item()\n            \n            data_dict['user_id'].append(i)\n            data_dict['time_step'].append(t)\n            data_dict['age'].append(age)\n            data_dict['gender'].append(gender)\n            data_dict['loyalty_tier'].append(tier)\n            data_dict['event'].append(event)\n            data_dict['ticket_size'].append(ticket_size)\n            data_dict['distance_store'].append(distance_store)\n            data_dict['marketing_action'].append(action)\n            data_dict['marketing_cost'].append(cost)\n            \n            tier = np.where(event == 2, update_tier(tier, True), update_tier(tier, False)).item()\n            \n            \n                \n    \n    X = pd.DataFrame(data_dict)\n    \n    return X\n\n\n#Generate data\ntemp = generate_rl_cust(500, 50)\n\n\n#View data\ntemp.shape\n\n(25000, 11)\n\n\n\n#Get dummies\ntemp = pd.get_dummies(temp, columns=['loyalty_tier'], dtype=float)\n\n\ntemp.head()\n\n\n\n\n\n\n\n\nuser_id\ntime_step\nage\ngender\nevent\nticket_size\ndistance_store\nmarketing_action\nmarketing_channel\nmarketing_cost\nloyalty_tier_bronze\nloyalty_tier_gold\nloyalty_tier_none\nloyalty_tier_silver\n\n\n\n\n0\n1\n0\n26\nMale\n2\n7.22\n3.45\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n1\n1\n1\n26\nMale\n2\n4.51\n3.57\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n2\n1\n2\n26\nMale\n1\n0.00\n5.66\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1\n3\n26\nMale\n1\n0.00\n4.12\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n4\n1\n4\n26\nMale\n2\n6.90\n2.08\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n#Define Deep Q Network\nclass DQN(nn.Module):\n    def __init__(self, state_size, action_size):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_size, 24)\n        self.fc2 = nn.Linear(24, 24)\n        self.fc3 = nn.Linear(24, action_size)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        \n        return self.fc3(x)\n\n\n#Intialize parameters for DQN\nm_actions = ['none', 'ad', 'small discount', 'large discount']\nm_channels = ['push', 'text', 'email']\ncomb_act_channels = ['none'] + [f\"{a}-{c}\" for a in m_actions[1:] for c in m_channels]\n\nstate_size = 8\naction_size = len(comb_act_channels)\nmod = DQN(state_size, action_size)\noptimizer = optim.Adam(mod.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n\ncomb_act_channels\n\n['none',\n 'ad-push',\n 'ad-text',\n 'ad-email',\n 'small discount-push',\n 'small discount-text',\n 'small discount-email',\n 'large discount-push',\n 'large discount-text',\n 'large discount-email']\n\n\n\n#Instantiate DQN Agent\nclass DQNAgent:\n    def __init__(self, state_size, action_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = []\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n        self.model = mod\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        if rd.random() &lt;= self.epsilon:\n            return rd.randrange(self.action_size)\n        with torch.no_grad():\n            act_values = self.model(state.unsqueeze(0))\n        return torch.argmax(act_values[0]).item()\n\n    def replay(self, batch_size):\n        if len(self.memory) &lt; batch_size:\n            return\n\n        minibatch = rd.sample(self.memory, batch_size)\n        states, actions, rewards, next_states, dones = zip(*minibatch)\n\n        states = torch.stack(states)\n        actions = torch.tensor(actions, dtype=torch.long)\n        rewards = torch.tensor(rewards, dtype=torch.float32)\n        next_states = torch.stack(next_states)\n        dones = torch.tensor(dones, dtype=torch.float32)\n\n        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n        next_q_values = self.model(next_states).max(1)[0]\n        targets = rewards + (1 - dones) * self.gamma * next_q_values\n\n        loss = self.criterion(q_values, targets)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.epsilon &gt; self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n        \n        return loss.item()\n\n\n#Set parameters for training loop\nagent = DQNAgent(state_size, action_size)\nbatch_size = 50\nepisodes = 10\n\n\n#Define state function\ndef get_state(state):\n    s_dict = {\n        'age': state['age'],\n        'gender': np.where(state['gender'] == 'Male', 1, 0).item(),\n        'distance_store': state['distance_store'],\n        'time_step': state['time_step'],\n        'loyalty_tier_none': state['loyalty_tier_none'],\n        'loyalty_tier_bronze': state['loyalty_tier_bronze'],\n        'loyalty_tier_silver': state['loyalty_tier_silver'],\n        'loyalty_tier_gold': state['loyalty_tier_gold']\n    }\n    \n    return torch.tensor(list(s_dict.values()), dtype=torch.float32)\n\n\n#Define reward funciton\ndef get_reward(user, i):\n    comp_uplift = {\n        'none': {'push': {'ad': .2, 'small discount': .3, 'large discount': 1}, \n                 'text': {'ad': 0, 'small discount': 0, 'large discount': 0}, \n                 'email': {'ad': .1, 'small discount': .2, 'large discount': .75}\n                },\n        'bronze': {'push': {'ad': .4, 'small discount': .7, 'large discount': 1}, \n                 'text': {'ad': .05, 'small discount': .5, 'large discount': 1}, \n                 'email': {'ad': .2, 'small discount': .6, 'large discount': 1}\n                },\n        'silver': {'push': {'ad': .4, 'small discount': 1, 'large discount': 2}, \n                 'text': {'ad': .05, 'small discount': .5, 'large discount': 1}, \n                 'email': {'ad': .3, 'small discount': .8, 'large discount': 1.5}\n                },\n        'gold': {'push': {'ad': -1.5, 'small discount': 0, 'large discount': .2}, \n                 'text': {'ad': -3, 'small discount': 0, 'large discount': 0}, \n                 'email': {'ad': -1.3, 'small discount': 0, 'large discount': .2}\n                }\n    }\n    \n    eff_reward = 0\n    none_bool = False\n    \n    if user.iloc[i]['marketing_action'] == 'none':\n        eff_reward = user.iloc[i]['ticket_size']\n        none_bool = True\n    else:\n        uplift_effect = comp_uplift['none'][user.iloc[i]['marketing_channel']][user.iloc[i]['marketing_action']] * user.iloc[i]['loyalty_tier_none'] \n        + comp_uplift['bronze'][user.iloc[i]['marketing_channel']][user.iloc[i]['marketing_action']] * user.iloc[i]['loyalty_tier_bronze'] \n        + comp_uplift['silver'][user.iloc[i]['marketing_channel']][user.iloc[i]['marketing_action']] * user.iloc[i]['loyalty_tier_silver'] \n        + comp_uplift['gold'][user.iloc[i]['marketing_channel']][user.iloc[i]['marketing_action']] * user.iloc[i]['loyalty_tier_gold']\n        \n        eff_reward = user.iloc[i]['ticket_size'] + uplift_effect - user.iloc[i]['marketing_cost']\n    \n    if i + 1 &lt; len(user):\n        current_tier = np.where(user.iloc[i]['loyalty_tier_none'] == 1, 1, \n                               np.where(user.iloc[i]['loyalty_tier_bronze'] == 1, 2, \n                                       np.where(user.iloc[i]['loyalty_tier_silver'] == 1, 3, 4))).item()\n        next_tier = np.where(user.iloc[i+1]['loyalty_tier_none'] == 1, 1, \n                               np.where(user.iloc[i+1]['loyalty_tier_bronze'] == 1, 2, \n                                       np.where(user.iloc[i+1]['loyalty_tier_silver'] == 1, 3, 4))).item()\n        \n        if current_tier == 4 and next_tier == 4:\n            if none_bool == True:\n                eff_reward += 5\n            elif user.iloc[i]['ticket_size'] &gt; 0:\n                eff_reward += 2\n            else:\n                eff_reward -= 5\n        elif next_tier &gt; current_tier:\n            eff_reward += 3\n        elif next_tier &lt; current_tier:\n            eff_reward -= 5\n        \n    return eff_reward\n        \n\n\n#Perform training\nactions_per_user = {}\ncumulative_reward = []\nloss_func = []\n\nfor e in range(episodes):\n    e_reward = 0\n    step_count = 0\n    \n    for user_id in temp['user_id'].unique():\n        user_data = temp[temp['user_id'] == user_id].sort_values(by='time_step')\n        state = get_state(user_data.iloc[0])\n        a = []\n\n        for i in range(len(user_data) - 1):\n            action = agent.act(state)\n        \n            reward = get_reward(user_data, i)\n            #print(f\"user_{user_id} reward: {reward}\")\n            e_reward += reward\n\n            next_state = get_state(user_data.iloc[i + 1])\n            done = (i == len(user_data) - 2)\n\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            a.append(comb_act_channels[action])\n            \n            step_count += 1\n\n        if len(agent.memory) &gt; batch_size and step_count % 50 == 0:\n            loss = agent.replay(batch_size)\n            loss_func.append(loss)\n        \n        if user_id == 1:\n            actions_per_user[user_id] = a\n        #print(f\"cumulative_reward: {e_reward}\")\n    \n    cumulative_reward.append(e_reward)\n\n    print(f\"Episode: {e + 1}/{episodes}, Epsilon: {agent.epsilon}\")\n\nEpisode: 1/10, Epsilon: 0.9511101304657719\nEpisode: 2/10, Epsilon: 0.9046104802746175\nEpisode: 3/10, Epsilon: 0.8603841919146962\nEpisode: 4/10, Epsilon: 0.8183201210226743\nEpisode: 5/10, Epsilon: 0.778312557068642\nEpisode: 6/10, Epsilon: 0.7402609576967045\nEpisode: 7/10, Epsilon: 0.7040696960536299\nEpisode: 8/10, Epsilon: 0.6696478204705644\nEpisode: 9/10, Epsilon: 0.6369088258938781\nEpisode: 10/10, Epsilon: 0.6057704364907278\n\n\n\n#View actions by agent for users\nactions_per_user[1]\n\n['ad-email',\n 'small discount-push',\n 'none',\n 'small discount-push',\n 'ad-text',\n 'large discount-push',\n 'none',\n 'small discount-push',\n 'small discount-email',\n 'small discount-text',\n 'large discount-text',\n 'ad-push',\n 'small discount-push',\n 'ad-push',\n 'large discount-text',\n 'ad-text',\n 'large discount-email',\n 'large discount-push',\n 'large discount-push',\n 'ad-push',\n 'small discount-text',\n 'ad-push',\n 'small discount-push',\n 'ad-push',\n 'ad-push',\n 'small discount-text',\n 'large discount-email',\n 'large discount-text',\n 'small discount-push',\n 'large discount-text',\n 'ad-text',\n 'ad-email',\n 'large discount-text',\n 'ad-text',\n 'large discount-email',\n 'none',\n 'ad-text',\n 'ad-text',\n 'small discount-text',\n 'none',\n 'small discount-push',\n 'small discount-push',\n 'ad-push',\n 'small discount-email',\n 'large discount-email',\n 'small discount-text',\n 'small discount-text',\n 'large discount-text',\n 'ad-email']\n\n\n\ntemp.query(\"user_id == 1\")\n\n\n\n\n\n\n\n\nuser_id\ntime_step\nage\ngender\nevent\nticket_size\ndistance_store\nmarketing_action\nmarketing_channel\nmarketing_cost\nloyalty_tier_bronze\nloyalty_tier_gold\nloyalty_tier_none\nloyalty_tier_silver\n\n\n\n\n0\n1\n0\n26\nMale\n2\n7.22\n3.45\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n1\n1\n1\n26\nMale\n2\n4.51\n3.57\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n2\n1\n2\n26\nMale\n1\n0.00\n5.66\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n3\n1\n3\n26\nMale\n1\n0.00\n4.12\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n4\n1\n4\n26\nMale\n2\n6.90\n2.08\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n5\n1\n5\n26\nMale\n2\n7.28\n2.28\nsmall discount\ntext\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n6\n1\n6\n26\nMale\n2\n7.73\n2.16\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n7\n1\n7\n26\nMale\n2\n7.31\n3.96\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n8\n1\n8\n26\nMale\n1\n0.00\n1.24\nad\nemail\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n9\n1\n9\n26\nMale\n1\n0.00\n5.87\nsmall discount\npush\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n10\n1\n10\n26\nMale\n2\n6.01\n1.84\nad\nemail\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n11\n1\n11\n26\nMale\n2\n6.89\n2.20\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n12\n1\n12\n26\nMale\n2\n7.50\n2.31\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n13\n1\n13\n26\nMale\n2\n5.70\n3.10\nad\npush\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n14\n1\n14\n26\nMale\n2\n7.00\n1.95\nlarge discount\ntext\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n15\n1\n15\n26\nMale\n2\n4.79\n1.65\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n16\n1\n16\n26\nMale\n2\n8.13\n8.00\nsmall discount\nemail\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n17\n1\n17\n26\nMale\n2\n7.71\n1.35\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n18\n1\n18\n26\nMale\n2\n7.11\n2.08\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n19\n1\n19\n26\nMale\n2\n7.45\n2.31\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n20\n1\n20\n26\nMale\n2\n7.50\n4.09\nsmall discount\ntext\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n21\n1\n21\n26\nMale\n2\n4.38\n1.90\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n22\n1\n22\n26\nMale\n2\n7.36\n1.68\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n23\n1\n23\n26\nMale\n2\n7.58\n3.22\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n24\n1\n24\n26\nMale\n2\n7.22\n2.70\nlarge discount\ntext\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n25\n1\n25\n26\nMale\n2\n8.07\n4.01\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n26\n1\n26\n26\nMale\n2\n6.95\n4.61\nsmall discount\npush\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n27\n1\n27\n26\nMale\n2\n8.17\n3.43\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n28\n1\n28\n26\nMale\n2\n7.41\n2.93\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n29\n1\n29\n26\nMale\n2\n5.62\n1.59\nad\npush\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n30\n1\n30\n26\nMale\n2\n5.97\n2.58\nad\nemail\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n31\n1\n31\n26\nMale\n2\n6.60\n2.32\nad\npush\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n32\n1\n32\n26\nMale\n2\n8.21\n2.41\nsmall discount\ntext\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n33\n1\n33\n26\nMale\n2\n7.98\n2.12\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n34\n1\n34\n26\nMale\n2\n4.56\n3.23\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n35\n1\n35\n26\nMale\n2\n7.22\n4.80\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n36\n1\n36\n26\nMale\n2\n6.61\n1.99\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n37\n1\n37\n26\nMale\n2\n6.01\n5.45\nad\nemail\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n38\n1\n38\n26\nMale\n2\n6.84\n4.11\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n39\n1\n39\n26\nMale\n2\n7.99\n1.78\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n40\n1\n40\n26\nMale\n2\n6.86\n4.97\nsmall discount\npush\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n41\n1\n41\n26\nMale\n2\n7.41\n6.59\nlarge discount\ntext\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n42\n1\n42\n26\nMale\n2\n7.13\n4.23\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n43\n1\n43\n26\nMale\n2\n6.86\n2.44\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n44\n1\n44\n26\nMale\n2\n7.07\n3.76\nsmall discount\nemail\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n45\n1\n45\n26\nMale\n2\n7.35\n1.78\nsmall discount\ntext\n1.50\n0.0\n1.0\n0.0\n0.0\n\n\n46\n1\n46\n26\nMale\n2\n6.73\n4.14\nad\npush\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n47\n1\n47\n26\nMale\n2\n7.41\n1.85\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n48\n1\n48\n26\nMale\n2\n7.80\n1.86\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n49\n1\n49\n26\nMale\n2\n7.98\n4.42\nlarge discount\npush\n2.50\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n#Plot cumulative rewards\nplt.figure(figsize=(12,5))\nplt.plot(cumulative_reward);\n\n\n\n\n\n#Plot loss\nplt.figure(figsize=(12,5))\nplt.plot(loss_func);\n\n\n\n\n\n#Define test data\ntest_data = generate_rl_cust(n_users=500, n_t=1)\n\n\n#Get dummies\ntest_data = pd.get_dummies(test_data, columns=['loyalty_tier'], dtype=float)\n\n\n#View data\ntest_data.head()\n\n\n\n\n\n\n\n\nuser_id\ntime_step\nage\ngender\nevent\nticket_size\ndistance_store\nmarketing_action\nmarketing_channel\nmarketing_cost\nloyalty_tier_bronze\nloyalty_tier_gold\nloyalty_tier_none\nloyalty_tier_silver\n\n\n\n\n0\n1\n0\n26\nMale\n2\n7.22\n3.45\nnone\nnone\n0.00\n0.0\n1.0\n0.0\n0.0\n\n\n1\n2\n0\n26\nFemale\n0\n0.00\n3.57\nnone\nnone\n0.00\n1.0\n0.0\n0.0\n0.0\n\n\n2\n3\n0\n25\nFemale\n1\n0.00\n4.79\nnone\nnone\n0.00\n1.0\n0.0\n0.0\n0.0\n\n\n3\n4\n0\n23\nFemale\n2\n4.46\n6.86\nad\ntext\n0.75\n0.0\n1.0\n0.0\n0.0\n\n\n4\n5\n0\n31\nFemale\n0\n0.00\n1.73\nnone\nnone\n0.00\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\n#Define test function\ndef test_agent(df, n_t=50, random_seed=42):\n    data_dict = {\n        'user_id': [],\n        'time_step': [],\n        'age': [],\n        'gender': [],\n        'event': [],\n        'ticket_size': [],\n        'distance_store': [],\n        'marketing_action': [],\n        'marketing_channel':[],\n        'marketing_cost': [],\n        'loyalty_tier_none': [],\n        'loyalty_tier_bronze': [],\n        'loyalty_tier_silver': [],\n        'loyalty_tier_gold': []\n    }\n    \n    marketing_costs = {\n        'none': 0,\n        'ad': .75,\n        'small discount': 1.5,\n        'large discount': 2.5\n    }\n    \n    marketing_probs_dict = {\n        'none': {'ad': .05, 'small discount': .2, 'large discount': .5},\n        'bronze': {'ad': .05, 'small discount': .3, 'large discount': .5},\n        'silver': {'ad': .1, 'small discount': .4, 'large discount': .6},\n        'gold': {'ad': -.1, 'small discount': 0, 'large discount': 0}\n    }\n    \n    events_cust = [\n        0, #None\n        1, #Visit\n        2  #Purchase\n    ]\n    \n    event_probs_dict = {\n        'none': [.7, .25, .05],\n        'bronze': [.6, .1, .3],\n        'silver': [.4, .1, .5],\n        'gold': [.05, .05, .9]\n    }\n    \n    for i in range(len(df)):\n        tier = np.where(df.iloc[i]['loyalty_tier_none'] == 1, 'none', \n                            np.where(df.iloc[i]['loyalty_tier_bronze'] == 1, 'bronze', \n                                    np.where(df.iloc[i]['loyalty_tier_silver'] == 1, 'silver', 'gold'))).item()\n        for t in range(1, n_t+1):\n            distance_store = np.round(np.random.lognormal(1, .5), 2)\n            \n            state = {\n                'age': df.iloc[i]['age'],\n                'gender': np.where(df.iloc[i]['gender'] == 'Male', 1, 0).item(),\n                'distance_store': distance_store,\n                'time_step': t,\n                'loyalty_tier_none': 1 if tier == 'none' else 0,\n                'loyalty_tier_bronze': 1 if tier == 'bronze' else 0,\n                'loyalty_tier_silver': 1 if tier == 'silver' else 0,\n                'loyalty_tier_gold': 1 if tier == 'gold' else 0\n            }\n            \n            current_state = get_state(state)\n            \n            with torch.no_grad():\n                agent.model.eval()\n                action_test = agent.model(current_state).argmax().item()\n                agent.model.train()\n\n            \n            distance_eff = np.where(-np.exp(-1.5 * distance_store) &gt; .1, -.1, -np.exp(-1.5 * distance_store)).item()\n            action_name = comb_act_channels[action_test]\n\n            if action_name == 'none':\n                action_channel = 'none'\n                cost = marketing_costs[action_name]\n                data_dict['marketing_channel'].append(action_channel)\n                temp_probs_dict = event_probs_dict.copy()\n                temp_probs_dict = update_purchase_probs(temp_probs_dict, distance_eff)\n                event = rd.choices(events_cust, temp_probs_dict[tier], k=1)[0]\n            else:\n                action_name, action_channel = comb_act_channels[action_test].split(\"-\")\n                cost = marketing_costs[action_name]\n                data_dict['marketing_channel'].append(action_channel)\n                temp_probs_dict = event_probs_dict.copy()\n                change_probs_purchase = marketing_probs_dict[tier][action_name]\n                temp_probs_dict = update_purchase_probs(temp_probs_dict, change_probs_purchase - distance_eff)\n                event = rd.choices(events_cust, temp_probs_dict[tier], k=1)[0]\n\n            ticket_size = np.where(event == 2, calc_uplift(action_name, tier, action_channel), 0).item()\n\n            data_dict['user_id'].append(i+1)\n            data_dict['time_step'].append(t)\n            data_dict['age'].append(df.iloc[i]['age'])\n            data_dict['gender'].append(df.iloc[i]['gender'])\n            data_dict['event'].append(event)\n            data_dict['ticket_size'].append(ticket_size)\n            data_dict['distance_store'].append(distance_store)\n            data_dict['marketing_action'].append(action_name)\n            data_dict['marketing_cost'].append(cost)\n            data_dict['loyalty_tier_none'].append(1 if tier == 'none' else 0)\n            data_dict['loyalty_tier_bronze'].append(1 if tier == 'bronze' else 0)\n            data_dict['loyalty_tier_silver'].append(1 if tier == 'silver' else 0)\n            data_dict['loyalty_tier_gold'].append(1 if tier == 'gold' else 0)\n            \n            tier = np.where(event == 2, update_tier(tier, True), update_tier(tier, False)).item()\n        \n    return pd.DataFrame(data_dict)\n\n\n#Get results\nres_test = test_agent(test_data)\n\n\n#Show actions for user 1\nres_test.query(\"user_id == 2\")\n\n\n\n\n\n\n\n\nuser_id\ntime_step\nage\ngender\nevent\nticket_size\ndistance_store\nmarketing_action\nmarketing_channel\nmarketing_cost\nloyalty_tier_none\nloyalty_tier_bronze\nloyalty_tier_silver\nloyalty_tier_gold\n\n\n\n\n50\n2\n1\n26\nFemale\n2\n2.93\n1.85\nsmall discount\npush\n1.50\n0\n1\n0\n0\n\n\n51\n2\n2\n26\nFemale\n2\n4.50\n1.76\nsmall discount\npush\n1.50\n0\n0\n1\n0\n\n\n52\n2\n3\n26\nFemale\n2\n7.82\n3.52\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n53\n2\n4\n26\nFemale\n2\n7.91\n0.96\nnone\nnone\n0.00\n0\n0\n0\n1\n\n\n54\n2\n5\n26\nFemale\n2\n7.34\n4.76\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n55\n2\n6\n26\nFemale\n2\n7.24\n5.45\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n56\n2\n7\n26\nFemale\n2\n7.76\n1.80\nnone\nnone\n0.00\n0\n0\n0\n1\n\n\n57\n2\n8\n26\nFemale\n2\n7.15\n1.87\nnone\nnone\n0.00\n0\n0\n0\n1\n\n\n58\n2\n9\n26\nFemale\n2\n7.83\n2.48\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n59\n2\n10\n26\nFemale\n2\n8.02\n2.85\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n60\n2\n11\n26\nFemale\n2\n4.86\n1.22\nad\ntext\n0.75\n0\n0\n0\n1\n\n\n61\n2\n12\n26\nFemale\n2\n7.48\n2.48\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n62\n2\n13\n26\nFemale\n2\n7.51\n3.38\nlarge discount\ntext\n2.50\n0\n0\n0\n1\n\n\n63\n2\n14\n26\nFemale\n2\n7.38\n4.30\nlarge discount\ntext\n2.50\n0\n0\n0\n1\n\n\n64\n2\n15\n26\nFemale\n2\n7.54\n3.71\nlarge discount\ntext\n2.50\n0\n0\n0\n1\n\n\n65\n2\n16\n26\nFemale\n2\n6.70\n3.25\nlarge discount\ntext\n2.50\n0\n0\n0\n1\n\n\n66\n2\n17\n26\nFemale\n2\n7.50\n1.99\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n67\n2\n18\n26\nFemale\n2\n7.29\n2.31\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n68\n2\n19\n26\nFemale\n2\n7.53\n2.57\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n69\n2\n20\n26\nFemale\n2\n7.20\n6.87\nlarge discount\nemail\n2.50\n0\n0\n0\n1\n\n\n70\n2\n21\n26\nFemale\n2\n5.70\n2.75\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n71\n2\n22\n26\nFemale\n2\n5.69\n3.28\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n72\n2\n23\n26\nFemale\n2\n5.86\n4.69\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n73\n2\n24\n26\nFemale\n2\n5.53\n3.00\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n74\n2\n25\n26\nFemale\n2\n6.43\n2.72\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n75\n2\n26\n26\nFemale\n2\n5.81\n2.22\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n76\n2\n27\n26\nFemale\n2\n5.04\n3.83\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n77\n2\n28\n26\nFemale\n2\n5.93\n4.01\nad\nemail\n0.75\n0\n0\n0\n1\n\n\n78\n2\n29\n26\nFemale\n2\n6.69\n1.41\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n79\n2\n30\n26\nFemale\n2\n7.18\n2.09\nlarge discount\npush\n2.50\n0\n0\n0\n1\n\n\n80\n2\n31\n26\nFemale\n2\n7.35\n3.82\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n81\n2\n32\n26\nFemale\n2\n7.28\n2.14\nsmall discount\nemail\n1.50\n0\n0\n0\n1\n\n\n82\n2\n33\n26\nFemale\n2\n7.95\n1.01\nsmall discount\nemail\n1.50\n0\n0\n0\n1\n\n\n83\n2\n34\n26\nFemale\n2\n7.73\n2.03\nsmall discount\nemail\n1.50\n0\n0\n0\n1\n\n\n84\n2\n35\n26\nFemale\n2\n8.04\n0.73\nsmall discount\nemail\n1.50\n0\n0\n0\n1\n\n\n85\n2\n36\n26\nFemale\n2\n7.96\n4.56\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n86\n2\n37\n26\nFemale\n2\n7.36\n5.49\nlarge discount\nemail\n2.50\n0\n0\n0\n1\n\n\n87\n2\n38\n26\nFemale\n2\n7.82\n2.92\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n88\n2\n39\n26\nFemale\n2\n7.36\n3.93\nsmall discount\npush\n1.50\n0\n0\n0\n1\n\n\n89\n2\n40\n26\nFemale\n2\n4.71\n1.13\nad\ntext\n0.75\n0\n0\n0\n1\n\n\n90\n2\n41\n26\nFemale\n2\n7.14\n2.20\nnone\nnone\n0.00\n0\n0\n0\n1\n\n\n91\n2\n42\n26\nFemale\n2\n7.71\n5.01\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n92\n2\n43\n26\nFemale\n2\n7.51\n2.24\nnone\nnone\n0.00\n0\n0\n0\n1\n\n\n93\n2\n44\n26\nFemale\n2\n7.39\n6.10\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n94\n2\n45\n26\nFemale\n2\n6.91\n3.39\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n95\n2\n46\n26\nFemale\n2\n7.63\n7.91\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n96\n2\n47\n26\nFemale\n2\n7.68\n5.62\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n97\n2\n48\n26\nFemale\n2\n7.30\n2.79\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n98\n2\n49\n26\nFemale\n2\n7.23\n7.46\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n99\n2\n50\n26\nFemale\n2\n7.00\n2.85\nsmall discount\ntext\n1.50\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n#SHow counts of tiers after \n(res_test.groupby('user_id')\n .tail(1)[['loyalty_tier_none', 'loyalty_tier_bronze', 'loyalty_tier_silver', 'loyalty_tier_gold']]\n .sum(axis=1).sum())\n\n500\n\n\n\n#Show counts\n(res_test.groupby('user_id')\n .tail(1)[['loyalty_tier_none', 'loyalty_tier_bronze', 'loyalty_tier_silver', 'loyalty_tier_gold']]\n .sum())\n\nloyalty_tier_none        0\nloyalty_tier_bronze      0\nloyalty_tier_silver      1\nloyalty_tier_gold      499\ndtype: int64\n\n\n\n#TODO: CHANGE REWARD STRUCTURE TO KEEP ACTIONS TO GOLD TIER AT A MINIMUM\n# HIERARCHICAL LOGIG TO UPDATING MARKETING PROBS BASED ON CHANNEL AS WELL"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Bayesian Bandit",
    "section": "",
    "text": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference\n\n\n\noperations\n\n\nmarketing\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nSep 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAir PSM: Measuring the effects of dynamic pricing on host revenue\n\n\n\noperations\n\n\nmarketing\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nSep 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDJ RD: Using regression discontinuity to measure music personalization\n\n\n\noperations\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nSep 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n(AI)rline Pricing: Estimating Willingness To Pay\n\n\n\noperations\n\n\noptimization\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nAug 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThere and Back Again: Optimizing Home-Swapping on Kindred\n\n\n\noperations\n\n\noptimization\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJul 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Impact of Sports on University Philanthropy: A Causal Analysis\n\n\n\nmarketing\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJul 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Doomscrolling Algorithm: Optimizing Content to Maximize Revenue\n\n\n\noperations\n\n\noptimization\n\n\nreinforcement learning\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy\n\n\n\noperations\n\n\noptimization\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJun 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDo Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM\n\n\n\noperations\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJun 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCan I get your password? An empirical approach to account sharing policies\n\n\n\noperations\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJun 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Treatments in Marketing Messaging with Non-Compliance\n\n\n\nmarketing\n\n\ncausal inference\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMay 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgentic Marketing: Optimizing Out-of-app Messaging using Causal Inference\n\n\n\nmarketing\n\n\ncausal inference\n\n\nreinforcement learning\n\n\n\n\n\n\n\nBrandon Scott\n\n\nApr 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn implementation of the ITCC algorithm in Python\n\n\n\npython\n\n\nclustering\n\n\n\n\n\n\n\nBrandon Scott\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn analysis of Uber fares\n\n\n\nanalysis\n\n\nbayesian\n\n\npython\n\n\n\n\n\n\n\nBrandon Scott\n\n\nJul 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Enrollment Size by Class at TAMU\n\n\n\nanalysis\n\n\nbayesian\n\n\npython\n\n\nforecasting\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMay 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAn Analysis of TA-MU Hotel Booking Data\n\n\n\nanalysis\n\n\nbayesian\n\n\npython\n\n\nclassification\n\n\n\n\n\n\n\nBrandon Scott\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog!\n\n\n\nwelcome\n\n\nbayesian\n\n\n\n\n\n\n\nBrandon Scott\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html",
    "href": "posts/airbnb_psm/airbnb_psm.html",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "",
    "text": "A core component of vacation planning is reserving accommodations for the duration of a stay. Many vacationers use platforms like Airbnb to immerse themselves in the local experience. Since all listings on Airbnb are owned by hosts, hosts are charged with setting prices for their respective listings. To help with this, Airbnb developed Smart Pricing to dynamically set prices for hosts. Though this tool is available to all hosts, some hosts for different reasons choose to continue to set their own prices. This scenario allows us to perform a “natural experiment” using causal inference techniques to measure the causal impact of the Smart Pricing tool on host revenue. Specifically, we use propensity score matching (PSM) to identify the causal effect of hosts using vs not using dynamic pricing."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#airbnb-as-a-platform",
    "href": "posts/airbnb_psm/airbnb_psm.html#airbnb-as-a-platform",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "Airbnb as a Platform",
    "text": "Airbnb as a Platform\nAirbnb can be described as a platform that connects two sides of a market: hosts and guests. Hosts provide the supply by listing their dwellings for rent, while guests create the demand with their preferences for locations, dates, and other criteria. The goal of Airbnb is to help both hosts and guests have “good” experiences on the platform by ensuring that hosts’ dwellings are rented and that guests’ preferences are met. This cycle is illustrated in Figure Figure 2.1.\n\n\n\nFigure 2.1: Airbnb Platform\n\n\nPlatform businesses like Airbnb face the “chicken or the egg” problem. Guests want to find the right dwelling for their trip, which requires an adequate supply to meet their needs. Likewise, hosts want their dwellings to be rented so they can earn money. Each side of the market is dependent on the other to fulfill its role. Airbnb acts as the “mediator” by attempting to balance this marketplace through various actions like incentives, policies, and technology tools."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#dynamic-pricing-for-hosts",
    "href": "posts/airbnb_psm/airbnb_psm.html#dynamic-pricing-for-hosts",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "Dynamic Pricing for Hosts",
    "text": "Dynamic Pricing for Hosts\nAccording to microeconomic theory, price is determined by the supply and demand of a marketplace. Someone trained in economic theory could develop models to understand the trend, seasonality, and other factors that affect supply and demand to best price their product. However, most hosts on Airbnb are not trained economists (and even if they were, they don’t have the time or data to model the market). Thus, knowing a good price point to maximize revenue while minimizing lost booking opportunities is a difficult problem.\nTo reduce this friction for hosts, Airbnb developed Smart Pricing to optimally set prices for hosts based on their developed models. This tool is available to all hosts on the platform. Even though it is freely available, not all hosts use the tool. For example, some hosts may be more aggresive in pricing and want to capture more revenue per booking.\nFor Airbnb, an important question is whether there is a true difference in host revenue between those who use Smart Pricing and those who do not. In most scenarios, we would want to run a randomized experiment to easily measure a causal effect. This is no longer feasible since the feature has been rolled out for some time. However, this scenario allows us to analyze a “natural experiment.” If we can adjust for the confounding effects of those who opted into the treatment (those who use Smart Pricing) and those who chose not to (those who set their own prices), we can potentially extract a quantified causal effect of dynamic pricing on host revenue."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#propensity-score-matching",
    "href": "posts/airbnb_psm/airbnb_psm.html#propensity-score-matching",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\nFinding causal effects in observational is difficult due to the presence of potential confounders. In randomized control trials like A/B tests, we control for these confounders by isolating the effects of a given treatment through random assignment. In observational studies, we can attempt to control these confounders by balancing the distribution of confounders between “treatment” and “control groups. To do this, we’ll use propensity score matching (PSM) [2].\nPropensity Score Matching (PSM) uses the theory that we can control for confounders by matching individuals from both the treated group and the untreated group who have a relatively similar probability of receiving the treatment. In our case, this would involve matching hosts who use Smart Pricing with those who do not, based on their similar probabilities of using Smart Pricing. The flow of causality in this model is shown in Figure 4.1.\n\n\n\n\n\nFigure 4.1: DAG for a simple PSM\n\n\n\n\n\\(e(x)\\) is the propensity score that we calculate for each individual. With it, we can say that certain features \\(X\\) are a cause of an individuals propensity score, which in turn determines if they use the treatment which ultimately determines the response variable \\(Y\\). The propensity score is the conditional probability of receiving the treatment given certain features \\(X\\). This formula is shown in Equation 4.1.\n\\[\ne(x) = Pr(T=1|X=x)\n\\tag{4.1}\\]\nOnce we generate propensity scores for individuals within our dataset, we match them based on an arbitraty distance criteria that finds the minimum distance between individuals based on propensity scores. In our analysis, we use the KNN algorithm. After creating the matches, we can then calculate our causal estimate, the average treatment effect (ATE) (assuming our PSM keeps essential causal model assumptions intact)."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#eda",
    "href": "posts/airbnb_psm/airbnb_psm.html#eda",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "EDA",
    "text": "EDA\nOur data consists of 500 hosts from a single geographic region with similar listing types (in this case, 2 bed, 1 bath listings with similar sq ft). A subset of our data is shown below.\n\n\n\n\n\n\n\n\n\nhost_id\nyrs_host\ngender\nuse_smart\navg_revenue\n\n\n\n\n0\n0\n2\nm\nFalse\n79.72\n\n\n1\n1\n1\nm\nTrue\n72.64\n\n\n2\n2\n3\nf\nTrue\n118.53\n\n\n3\n3\n1\nm\nTrue\n70.95\n\n\n4\n4\n2\nf\nTrue\n93.17\n\n\n\n\n\n\n\nThe dataset is a simple datset that consists of only 2 features (years as host and gender) the treatment binary variable if the host uses Smart Pricing, and the response variable average revenue which is the simple average of total revenue over number of bookings (Note: we assume these hosts are booked at a similar frequency).\nWe begin exploring the data by visualizing the count data of between those who use Smart Pricing in the dataset and those who do not. This is shown in Figure 4.2.\n\n\n\n\n\nFigure 4.2: Count of users of smart pricing vs non users\n\n\n\n\nThe dataset appears to be fairly evenly split between non users and users of Smart Pricing, with a slight edge to non users. This is a good sign for our causal modeling purposes since a key assumption is good overlap between the treated and untreated. We continue our investigation in Figure 4.3 by visualizing the average revenue distribution by treated and untreated.\n\n\n\n\n\nFigure 4.3: Histogram of treated vs untreated average revenue\n\n\n\n\nFigure 4.3 shows that the distribution of those who use Smart Pricing is a bit higher than those who do not use it. This is a good indication that Smart Pricing helps hosts have an increase in revenue. We can continue to investigate this relationship in Figure 4.4.\n\n\n\n\n\nFigure 4.4: Bar plot comparing average revenue by years as host and smart pricing users\n\n\n\n\nThere appears to be a relationship between the number of years as a host and the average revenue. While those who use Smart Pricing consistently outperform hosts without Smart Pricing, for each year increase as host, the average revenue increases. One small note is that there aren’t any hosts by year 4 who don’t use Smart Pricing. We may need to account for this in our model."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#calculating-propensity-scores",
    "href": "posts/airbnb_psm/airbnb_psm.html#calculating-propensity-scores",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "Calculating Propensity Scores",
    "text": "Calculating Propensity Scores\nAs mentioned previously, once we calculate the propensity scores for our observations, we don’t need to account for other attributes \\(X\\) since it adds no additional information that is already captured in the propensity score. To calculate the propensity score, we can use any arbitrary classification model to estimate Equation 4.1. In our case, we use logistic regression, as shown in Equation 4.2. Some of our resulting propensity scores are shown in the table below.\n\\[\nlog(\\frac{p}{1-p}) = \\beta^{T}X + \\epsilon\n\\tag{4.2}\\]\n\n\n\n\n\n\n\n\n\nyrs_host\nuse_smart\navg_revenue\ngender_f\nprop_score\n\n\n\n\n0\n2\nFalse\n79.72\nFalse\n0.522335\n\n\n1\n1\nTrue\n72.64\nFalse\n0.454340\n\n\n2\n3\nTrue\n118.53\nTrue\n0.569330\n\n\n3\n1\nTrue\n70.95\nFalse\n0.454340\n\n\n4\n2\nTrue\n93.17\nTrue\n0.501642\n\n\n\n\n\n\n\nThe example output shows some decent balance of propensity scores across the different demographics. We can visualize the scores via histogram overlays, similar to Figure 4.3. This propensity score histogram is shown below.\n\n\n\n\n\nFigure 4.5: Histogram of propensity scores between treated and untreated hosts\n\n\n\n\nThe histogram of propensity scores shows pretty good overlap between treated and untreated propensities. There are no scores for untreated hosts on the far right of the distribution (i.e. hosts of 4 years). However, because there is such a small number of individuals in this group, we can proceed.\n\nQuick Aside on Calculating Propensity Scores\nJust a quick note on the propensity scores. Some may believe that a model like logistic regression would be a simple approach to “predicting” treatment. However, the goal of estimating \\(\\hat{e(x)}\\) is not to accurately predict treatment. Rather, our \\(\\hat{e(x)}\\) estimate should attempt to control for confounding. This slight difference yields very big consequences in the long run of modeling the causal effect of treatment. The high-level explanation is that if we include variables that are only good at predicting the treatment but are not remotely related to the original response variable, then we would be increasing the variance of our propensity score estimates and subsequently breaking important assumptions like overlap (positivity) and biasing our causal effect estimates."
  },
  {
    "objectID": "posts/airbnb_psm/airbnb_psm.html#causal-effect-estimation-via-psm",
    "href": "posts/airbnb_psm/airbnb_psm.html#causal-effect-estimation-via-psm",
    "title": "Air PSM: Measuring the effects of dynamic pricing on host revenue",
    "section": "Causal Effect Estimation via PSM",
    "text": "Causal Effect Estimation via PSM\nSince our data appears to hold true on our core causal assumptions, we can proceed with building a model that estimates the causal effect of Smart Pricing on host revenue. In our analysis, we use the causalinference [3] package to fit our data to a Propensity Score Matching (PSM) model. The model takes care of our nearest-neighbor matching (using KNN) and estimates the ATE, ATC, and ATT. Equation 4.3 shows the math behind each of these estimates.\n\\[\nATE = E[Y_{1} - Y_{0}] = E[Y_{1}] - E[Y_{0}]\n\\] \\[\nATC = E[Y_{1} - Y_{0}|T=0] = E[Y_{1}|T=0] - E[Y_{0}|T=0]\n\\] \\[\nATT = E[Y_{1} - Y_{0}|T=1] = E[Y_{1}|T=1] - E[Y_{0}|T=1]\n\\tag{4.3}\\]\nThe ATE is the difference between the mean of the treated and the mean of the untreated. This holds due to our assumption of unconfoundess via \\(T \\perp (Y_{0}, Y_{1})|e(x)\\). This assumption continues to hold for ATC, which is the average treatment effect on the control, meaning the average effect of the treatment if it were to have been applied to the control group. Similarly, the average treatment effect on the treated shows the average effect of the treatment of those who were treated and their counterfactual of if they didn’t receive treatment.\nIn our analysis, we focus on the ATT since we are dealing with matching. We want to understand how the treatment helped the hosts compared to what could have happened if they didn’t use Smart Pricing. The counterfactuals are estimated using the matches in our model. The results for each of these calculations from Equation 4.3 are shown below.\n\n\n\nTreatment Effect Estimates: Matching\n\n                     Est.       S.e.          z      P&gt;|z|      [95% Conf. int.]\n--------------------------------------------------------------------------------\n           ATE     12.074      0.194     62.102      0.000     11.693     12.455\n           ATC     12.036      0.196     61.447      0.000     11.652     12.419\n           ATT     12.114      0.195     62.024      0.000     11.731     12.497\n\n\n\nOur PSM model shows that each of our estimates are statistically significant. The ATT from the model shows that on average, hosts that use Smart Pricing earn $12 more than those who do not use Smart Pricing. This is a good indication that our dynamic pricing tool helps hosts earn more money per booking. For more context on the scale of this effect, we can review the distribution of untreated hosts.\n\n\nMean: 75.3310588235294\nSD: 12.555868912587034\n\n\nLooking at the SD, our ATT is almost equal to it. Thus, our ATT says that on average, hosts who use Smart Pricing are essentially one standard deviation above those who don’t use Smart Pricing."
  },
  {
    "objectID": "posts/bayes-pricing/bayesian_pricing_uber.html",
    "href": "posts/bayes-pricing/bayesian_pricing_uber.html",
    "title": "An analysis of Uber fares",
    "section": "",
    "text": "Uber is the current leader in the rideshare market. With millions of people using the platform everyday, the company must prioritize resources to understanding their customer base and how to price their product effectively. This analysis is a simple illustration of the power of bayesian pricing methods for rideshare companies. Specifically, we demonstrate the effectiveness of a basic bayesian linear model at predicting the price of an Uber ride based on our supplied data. Data for this analysis can be found here."
  },
  {
    "objectID": "posts/bayes-pricing/bayesian_pricing_uber.html#investigating-relationships-with-fare-amount",
    "href": "posts/bayes-pricing/bayesian_pricing_uber.html#investigating-relationships-with-fare-amount",
    "title": "An analysis of Uber fares",
    "section": "Investigating Relationships with Fare Amount",
    "text": "Investigating Relationships with Fare Amount\nAs stated earlier, the purpose of this analysis is to analyze what, if anything, have effects on fare amount. We further this exploration in Figure 2.7 where we check the distribution of fare amount by the passenger count.\n\n\n\n\n\nFigure 2.7: Box-and-whisker plots showing the distribution of fare amount by passenger count.\n\n\n\n\nAs we can see, the distributions among passenger count appear to be approximately the same. The median fare amount for each passenger count appears to be constant and each quartile appears to be relatively equal to the others.\nAnother possible influence could be day of the week when a ride is taken. Figure 2.8 shows the respective distributions of fare amount by weekday and weekend.\n\n\n\n\n\nFigure 2.8: Box-and-whisker plot showing distribution of fare amount by weekday vs weekend.\n\n\n\n\nJust as we saw in Figure 2.7, Figure 2.8 shows that there is not much difference between weekday and weekend trips with regard to fare amount.\nAfter having cleaned up our dataset, we can view a scatterplot of trip in miles vs fare amount, colored by weekend or weekday, to see the relationhsip between these variables. This is shown in Figure 2.9.\n\n\n\n\n\nFigure 2.9: Scatterplot of trip in miles vs fare amount, with each dot colored by weekend (orange) and weekday (blue).\n\n\n\n\nEven after our data cleanup, we see in Figure 2.9 that there is a large amount of data to still use. There appears to be a slight positive linear relationship between trip in miles vs fare amount. We will hope to quantify this when we build our model. There also appears to be a lot more rides during the week vs the weekend (which makes sense since there are more weekdays than weekends)."
  },
  {
    "objectID": "posts/bayes-pricing/bayesian_pricing_uber.html#check-for-differences-in-number-of-rides-by-hour",
    "href": "posts/bayes-pricing/bayesian_pricing_uber.html#check-for-differences-in-number-of-rides-by-hour",
    "title": "An analysis of Uber fares",
    "section": "Check for Differences in Number of Rides by Hour",
    "text": "Check for Differences in Number of Rides by Hour\nAs we mentioned at the end of the previous section, there are more rides on weekdays vs weekends. We should continue to explore the difference in these different segments. First, we’ll explore the difference in the number of rides by hour, split by weekdays vs weekends. This is shown in Figure 2.10.\n\n\n\n\n\nFigure 2.10: Top panel shows number of rides by hour on weekdays. Bottom panel shows number of rides by hour on weekends.\n\n\n\n\nAs we can see, although the total number of rides might differ between weekdays and weekends, the trend appears more or less the same. Weekdays have much heavier peaks around the usual rush hour commuting times. Weekends appear to steadily increase from morning to evening.\nWhile weekdays vs weekends are one possible segmentation for number of rides, seasons are also a potential group we should explore for delineations in number of rides. Figure 2.11 shows the number of rides by hour for each season.\n\n\n\n\n\nFigure 2.11: 2x2 grid showing number of rides by hour according to the labeled season.\n\n\n\n\nThere does not appear to be any differences between seasons, except for spring having very low peaks in comparison to the other 3 seasons. Generally speaking, each season follows the same trend of number of rides by hour, with winter, summer, and fall each following very similar trends and numbers."
  },
  {
    "objectID": "posts/bayes-pricing/bayesian_pricing_uber.html#fare-amount-by-time-of-day-and-season",
    "href": "posts/bayes-pricing/bayesian_pricing_uber.html#fare-amount-by-time-of-day-and-season",
    "title": "An analysis of Uber fares",
    "section": "Fare Amount by Time of Day and Season",
    "text": "Fare Amount by Time of Day and Season\nWe return to investigating fare amount since that is the principle objective of this analysis. One relationship we have yet to explore is the fare amount in relation to time of day. To do this, we will bucket each hour into a specific time of day (ie morning) and calculate the distributio of fare prices at that respective time. Figure 2.12 shows this distribution.\n\n\n\n\n\nFigure 2.12: Box-and-whisker plot showing distribution of fare amount by bucketed time of day.\n\n\n\n\nFrom Figure 2.12, we see that there is no real difference in distributions. Dawn appears to have the highest variance of the 5 time buckets, but does not appear to be significant.\nTo combine some of the relationships we have explored thus far, we will look at the change in average fare, split by season and time of day. Perhaps a combination of these segments could show a possible difference in fare price. Figure 2.13 shows this relationship.\n\n\n\n\n\nFigure 2.13: Top panel shows the average fare price and its change over seasons, split by time of day. Bottom panel shows the average fare amount by season, split by weekday vs weekend.\n\n\n\n\nFigure 2.13 is very informative as it shows clear delineations between average fare prices. The top panel shows that dawn by far has the highest average fare price over the other 4 time of day buckets. The closest to each other are evening and afternoon across all seasons. Interestingly, afternoon has a sharp dip in average fare price in the winter. The average afternoon fare price in winter is the lowest, followed closely by morning and evening. While we do notice clear separation between these lines, we should recognize that these prices are within essentially a dollar a part.\nThe bottom panel of Figure 2.13 shows a clear (yet small in dollar value) difference between weekend and non-weekend average fare amounts across seasons. Weekends on average have a higher fare amount than weekdays across all seasons. Additionally, The difference appears constant between the average fare amount of weekends and weekdays across all seasons. Figure 2.13 shows the usefulness of these features in helping the model capture defined segments across the data."
  },
  {
    "objectID": "posts/bayes-pricing/bayesian_pricing_uber.html#simple-prediction-demonstration",
    "href": "posts/bayes-pricing/bayesian_pricing_uber.html#simple-prediction-demonstration",
    "title": "An analysis of Uber fares",
    "section": "Simple Prediction Demonstration",
    "text": "Simple Prediction Demonstration\nTo demonstrate how our model predicts, we will make a prediction for a single passenger, riding 5 miles, in the winter, in the morning, on a weekday. Using our estimated beta distributions above, we obtain the posterior predictive distribution in Figure 3.3.\n\n\n\n\n\nFigure 3.3: Posterior predictive density for a single passenger riding 5 miles in winder during the morning on a weekday.\n\n\n\n\nFrom Figure 3.3, it appears that the average fare amount for a ride like this would be around 20.15 dollars. Approximate 95% PI would be (20.10, 20.20). Our model makes very precise predictions as evident by the short width of the distribution. Nonetheless, this illustrates very well how our bayesian linear model makes a predictive distribution instead of just a single point estimate. We have a small amount of uncertainty around this estimate due to the large amount of training data."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html",
    "href": "posts/cfa_drive/drive_thru.html",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "",
    "text": "Drive-thrus are the American way of dining. Therefore, companies need this part of the restaurant to operate at peak efficiency. One company in particular that excels in this is Chick-fil-A. Recently, Chick-fil-A has begun testing mobile-thrus, a designated drive-thru lane for mobile orders made through the Chick-fil-A app. In this post, we briefly review the history of the drive-thru for fast food restaurants. We further zoom in on Chick-fil-A’s drive-thru strategy and operations. We then present an experimentation design for Chick-fil-A to validate this drive-thru pilot program using Synthetic Difference-in-Difference (SDiD). We conclude by discussing possible strategic directions from our causal findings."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#drive-thru-operations",
    "href": "posts/cfa_drive/drive_thru.html#drive-thru-operations",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Drive-Thru Operations",
    "text": "Drive-Thru Operations\nIn practice, drive-thru operations at each Chick-fil-A location are not identical; they vary based on factors like store layout and local demand. For this analysis, we will assume a standard operational structure and flow as depicted in Figure 1, which serves as a representative model for our discussion.\n\n\n\nFigure 3.1: Chick-fil-A Drive-Thru Diagram\n\n\nOur Chick-fil-A restaurant, as shown in Figure 1, features a double-lane drive-thru. The process is as follows:\n\nA car enters the drive-thru and chooses a lane.\nThe customer places their order with the designated order taker for that lane.\nIf paying with cash, the customer stops at a shared cashier station located between the two lanes.\nThe car proceeds to the delivery window, where an employee delivers the food.\n\nInside the restaurant, a team of baggers and beverage/milkshake station employees assemble the orders based on their contents."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#analyzing-the-drive-thru-via-queue-theory",
    "href": "posts/cfa_drive/drive_thru.html#analyzing-the-drive-thru-via-queue-theory",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Analyzing the Drive-Thru via Queue Theory",
    "text": "Analyzing the Drive-Thru via Queue Theory\nOur drive-thru operation can be analyzed using tools from queue theory [4]. Below are some key parameters.\n\nArrival Rate (\\(\\lambda\\)): The average rate of customers arriving at the drive-thru. In our scenario, \\(\\lambda\\) = 2 customers per minute.\nOrder Take Time (OTT): The average time it takes for customers to place their order. In our scneario, OTT = 30 seconds.\nService Rate (\\(\\mu\\)): The average rate at which orders are processed and fulfilled (time from order placement to order delivery). In our scenario, \\(\\mu\\) = 2.33 customers per minute.\n\nWith two order takers and a single service window, the system’s bottleneck is the service window, as it has the lowest service rate. Since the overall system’s throughput is limited by this single service point, we can model the entire drive-thru as a M/M/1 queue using Kendall notation [5], where the service window represents the single server. This simplifies our analysis by focusing on the primary constraint on the system’s capacity.\nFor our drive-thru to run efficiently, we want to ensure that certain key metrics are optimized. These metrics and the respective calculations are listed below.\n\nSystem Utilization (\\(\\rho\\)): \\(\\rho = \\frac{\\lambda}{\\mu} = 85.8\\%\\)\nAverage Number of Customers in Queue (\\(L_{q}\\)): \\(L_{q} = \\frac{\\lambda^{2}}{\\mu(\\mu - \\lambda)} = 5.2\\)\nAverage Time in Queue (\\(W_{q}\\)): \\(W_{q} = \\frac{L_{q}}{\\lambda} = 2.6 \\text{ mins}\\)\nAverage Number of Customers in System (\\(L\\)): \\(L = \\frac{\\lambda}{\\mu - \\lambda} = 6.1\\)\nAverage Time in System (\\(W\\)): \\(W = \\frac{1}{\\mu - \\lambda} = 3.0 \\text{ mins}\\)\n\nUnder the current system, the service window operates at around 86% capacity. On average, there are 5 cars waiting for their food, with each car spending an average of 2.5 minutes in the queue. The total average time a car spends in the system—from entering the drive-thru to receiving their food—is 3 minutes, with a total of 6 cars in the system on average. Overall, for peak hours, these are acceptable values that represent an efficient drive-thru."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#chick-fil-a-one",
    "href": "posts/cfa_drive/drive_thru.html#chick-fil-a-one",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Chick-fil-A One",
    "text": "Chick-fil-A One\nIn 2012, Chick-fil-A released their own mobile app. This app allowed customers to order items from their mobile device and pick up their order in the lobby or via the drive-thru. In 2016, Chick-fil-A added an incentive to get the app by developing their Chick-fil-A One rewards program. Any customer could now sign up and earn rewards for their purchases through the app.\n\n\n\nFigure 3.2: Chick-fil-A One Rewards Program Benefits\n\n\nAs shown in Figure 3.2, customers earn points based on their spending tier, with higher tiers offering greater benefits. This program is not only valuable to customers but also immensely beneficial to Chick-fil-A. In exchange for these rewards, the company collects crucial data on customer purchasing habits. This data can reveal insights into a customer’s typical order time, their distance to the store when placing an order, purchasing trends on weekdays versus weekends, etc. These data points allow Chick-fil-A to personalize the customer experience and optimize store operations."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#mobile-thru",
    "href": "posts/cfa_drive/drive_thru.html#mobile-thru",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Mobile Thru",
    "text": "Mobile Thru\nWhile members can place orders on the app for drive-thru pickup, these orders are currently not given priority. The system processes them like any other order in the queue. The only difference is the speed of order taking. A customer simply provides their name to an order taker, who then adds the pre-entered mobile order to the queue.\nSince drive-thrus are the most popular way for customers to order their food and Chick-fil-A wants to continue to encourage customers to use their app, Chick-fil-A came up with “mobile-thrus”. Here, customers who use the mobile app have a dedicated lane in the drive-thru instead of joining the normal queue mixed with mobile and non-mobile customers.\n\n\n\nFigure 3.3: Mobile Thru at Chick-fil-A\n\n\nWe can view how this would affect the operations outlined in Figure 3.1. Using the same notation as before, we assume that the average OTT remains at 30 seconds. In the new mobile-thru lane, average OTT is 10 seconds. While this is a great improvement in OTT, the bottleneck still occurs at the service window. Instead of dealing with a M/M/2 converging into a M/M/1 queue, we can break this into two parallel M/M/1 queues. The system specifications are enumerated below.\n\n\\(\\lambda_{normal}\\) = 1 customer per minute\n\\(\\lambda_{mobile}\\) = 1 customer per minute\n\\(OTT_{normal}\\) = 30 seconds\n\\(OTT_{mobile}\\) = 10 seconds\n\\(\\mu_{normal}\\) = 1.1 customers per minute\n\\(\\mu_{mobile}\\) = 1.3 customers per minute\n\nIn the scenario with the specifications above, we assume the same average arrival rate but split evenly between the two lanes. The key difference is the service rate between the two lanes. We assume that we put the “faster” bagger on the mobile lane to enhance that experience. With these rates, we calculate the following metrics in Table 3.1.\n\n\nTable 3.1: Drive-Thru Metrics w/ Mobile Thru\n\n\n\nNormal\nMobile\n\n\n\n\n\\(\\rho\\)\n90.9%\n76.9%\n\n\n\\(W_{q}\\)\n9.1\n2.6\n\n\n\\(L_{q}\\)\n~9\n~3\n\n\n\\(W\\)\n10\n3.3\n\n\n\\(L\\)\n~10\n~3\n\n\n\n\nAs evident in the table, the mobile lane runs at much higher efficiency than the normal lane. On average, the normal line has almost 9 cars in the queue, 3 times as many as the mobile lane. Subsequently, cars in the normal lane wait on average 9 minutes for their order to be completed, whereas the mobile lane only waits about 2.6 minutes."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#theory-behind-strategy",
    "href": "posts/cfa_drive/drive_thru.html#theory-behind-strategy",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Theory Behind Strategy",
    "text": "Theory Behind Strategy\nOur mobile-thru is an application of nudge theory [6], which states that people’s decisions can be influenced by small changes to their “choice architecture.” In this scenario, we alters our drive-thru by changing the lane configuration from two traditional lanes to one traditional lane and one exclusive “mobile-thru” lane. This change in choice architecture, combined with the greater efficiency and rewards associated with the mobile lane, provides a strong nudge for customers to choose the mobile-thru option.\nThis scenario also is a practical application of social exchange theory [7]. In exchange for exceptional service and loyalty points, we gain valuable customer data. This information includes a range of data points such as demographics, average ticket sizes, redeemed rewards, and the effectiveness of promotions."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#testing-our-theories",
    "href": "posts/cfa_drive/drive_thru.html#testing-our-theories",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Testing Our Theories",
    "text": "Testing Our Theories\nFrom our theories, we can develop research questions from which we will design our experiment. For our analysis, we focus on one primary question: - Does the mobile-thru increase the proportion of mobile orders?\nIn order to answer this questions, we need to design a robust experimentation framework that will allow us to gather data points to then perform valid causal inference. Ideally, we’d like to perform some kind of randomized controlled trial (RCT). However, this is infeasible since we can’t randomly assign customers to one of the lanes. Furthermore, a wide-scale rollout is too risky, as an unpopular mobile-thru could damage brand credibility and incur significant costs.\nTo accomdate these requirements, the proposed research design is as follows: - Randomly select \\(n\\) stores in each of our core geographic regions - Perform a staggered roll out of the mobile-thru to the \\(n\\) locations - For each roll restaurant, collect transcation data \\(t_{1}\\) number of days before roll out and \\(t_{2}\\) days after roll out - Compare the transaction data between “treated” stores and “non-treated” stores\nNote: We perform random selection of restaurants to mitigate the potential for confoundness. We also perform a staggered roll out for the same reason (perhaps economic conditions are better in one month vs another)."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#exploring-the-data",
    "href": "posts/cfa_drive/drive_thru.html#exploring-the-data",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nWe begin exploring the data by first seeing what data we have available. We call a simple head method call to preview the data.\n\n\n\n\n\n\n\n\n\nstore_id\nday_num\nhas_mobile\nperc_rev_member\n\n\n\n\n0\n0\n0\n0\n0.412567\n\n\n1\n0\n1\n0\n0.299284\n\n\n2\n0\n2\n0\n0.465392\n\n\n3\n0\n3\n0\n0.190485\n\n\n4\n0\n4\n0\n0.647600\n\n\n\n\n\n\n\nWe have four columns currently. Store id represents the unique identifier for each restaurant (0-7 for a total of eight restaurants). Day num represents the day number collected relative to the beginning of the experiment/rollout. Has mobile is a boolean representing if/when a restaurant receives the mobile thru. Percent revenue member represents the proportion of revenue at the restaurant for the day generated from members (i.e. via the app).\nTo help with visualizations, we create a new column called gets_treat to show which restaurants will receive treatment. In our study, this corresponds to restaurants 0, 2, 4, and 6. Each of these restaurants, as stated earlier in the post, has a similar restaurant within the same region that acts as the control (Note: in our analysis, we assume homogeneity across geographic regions). Thus, one interesting visualization would be plotting the 100-day experiment comparing the response variable between treated restaurants and controls. This is shown in Figure 4.1.\n\n\n\n\n\nFigure 4.1: Aggregated panel data plot showing average percent of revenue from mobile orders between treated and control units\n\n\n\n\nThe dotted black lines in Figure 4.1 show the rollout dates for each of the restaurants in the study. As we approach the 100-day mark, the distance between the treated and control restaurants grows larger, indicating a potential treatment effect. To verify if this is true across all restaurants or if one restaurant is driving this change, we plot each region separately in Figure 4.2.\n\n\n\n\n\nFigure 4.2: Proportion of revenue generated by mobile orders between treated and control in each region\n\n\n\n\nThe graph above verifies that each region tested shows a growing gap between treated and control restaurants. As such, we’d like to quantify this gap to help Chick-fil-A know on average what a full mobile-thru rollout would do for driving an increase in mobile-app order usage."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#sdid-model-and-results",
    "href": "posts/cfa_drive/drive_thru.html#sdid-model-and-results",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "SDiD Model and Results",
    "text": "SDiD Model and Results\nNotice: We’d like to recognize the work of [11] and the work of Matheus Facure [Facure:SynDiD]. We use Matheus’s implementation of SDiD in our post and highly recommend that people read his book, Causal Inference for The Brave and True.\nIn most causal inference models, we are dealing with estimating what is happening versus what could have happened using the potential outcomes framework [12]. To do this in our analysis, we begin by deriving the SDiD estimator by showing the optimization equations for DiD and SCM in Equation 5.1.\n\\[\n\\hat{\\tau}^{sc} = \\underset{\\beta, \\tau}{argmin}  \\bigg\\{ \\sum_{i=1}^N \\sum_{t=1}^T \\big(Y_{it} - \\beta_t - \\tau D_{it}\\big)^2 \\hat{w}^{sc}_i \\bigg\\}\n\\] \\[\n\\hat{\\tau}^{did} = \\underset{\\mu, \\alpha, \\beta, \\tau}{argmin} \\bigg\\{ \\sum_{i=1}^N \\sum_{t=1}^T \\big(Y_{it} - (\\mu + \\alpha_i + \\beta_t + \\tau D_{it}\\big)^2 \\bigg\\}\n\\tag{5.1}\\]\nThe top equation is for SCM. In SCM, we attempt to find the parameters \\(\\beta\\) and \\(\\tau\\) that minimize the difference between the observed result \\(Y_{it}\\). In DiD, we attempt to find parameters \\(\\mu\\), \\(\\alpha\\), \\(\\beta\\), and \\(\\tau\\) that minize the difference between the observed result \\(Y_{it}\\). The goal in SCM is to generate a counterfactual that mimics the pre-treatment period for the treated unit. Likewise, in DiD, we use a control unit that mimics the behavior of the treated unit (i.e. follows the parallel trends assumption).\nIn SDiD, we merge the two equations together to get Equation 5.2. We describe each part of this equation in the list below.\n\\[\n\\hat{\\tau}^{sdid} = \\underset{\\mu, \\alpha, \\beta, \\tau}{argmin}  \\bigg\\{ \\sum_{i=1}^N \\sum_{t=1}^T \\big(Y_{it} - (\\mu + \\alpha_i + \\beta_t + \\tau D_{it})^2 \\hat{w}^{sdid}_i \\hat{\\lambda}^{sdid}_t \\big) \\bigg\\}\n\\tag{5.2}\\]\n\\[\n\\begin{split}\n\\hat{\\lambda}^{sdid} = \\underset{\\lambda}{\\mathrm{argmin}} \\ ||\\bar{\\pmb{y}}_{post, co} - (\\pmb{\\lambda}_{pre} \\pmb{Y}_{pre, co} +  \\lambda_0)||^2_2 \\\\\n\\text{s.t } \\ \\sum \\lambda_t = 1 \\text{ and } \\ \\lambda_t &gt; 0 \\ \\forall \\ t\n\\end{split}\n\\tag{5.3}\\]\n\\[\n\\begin{split}\n\\hat{w}^{sdid} = \\underset{w}{\\mathrm{argmin}} \\ ||\\bar{\\pmb{y}}_{pre, tr} - (\\pmb{Y}_{pre, co} \\pmb{w}_{co} +  w_0)||^2_2 + \\zeta^2 T_{pre} ||\\pmb{w}_{co}||^2_2\\\\\n\\text{s.t } \\ \\sum w_i = 1 \\text{ and } \\ w_i &gt; 0 \\ \\forall \\ i\n\\end{split}\n\\tag{5.4}\\]\n\n\\(Y_{it}\\) is the actual outcome for restaurant \\(i\\) at time \\(t\\)\n\\(\\mu\\) is the baseline of the counterfactual\n\\(\\alpha_{i}\\) is the fixed effect for the \\(i\\)th restaurant\n\\(\\beta_{t}\\) is the time fixed effect for time \\(t\\)\n\\(D_{it}\\) is the treatment indicator (1 for treated, 0 for control)\n\\(\\tau\\) is the ATT\n\\(\\hat{w_{i}}^{sdid}\\) is the \\(ith\\) unit weight from SCM\n\\(\\hat{\\lambda_{t}}^{sdid}\\) is the \\(th\\) time weight\n\nWe won’t go into too much detail with Equation 5.3 nor Equation 5.4, as they are mostly there for reference as to where those weights come from. The critical equation is Equation 5.2, which shows that it is a balanced combination of SCM and DiD. This means that SDiD finds optimal parameters that both estimate the treated unit’s counterfactual trajectory. In short, the SDiD estimator becomes a doubly robust estimator for estimating ATT.\nIn particular, SDiD is a doubly robust estimator in staggered adoption designs. Because SDiD relies on block-matrix design (see [11] section 8 for more information), we adjust this block design by running SDiD four times across the four treated units while still comparing them across all the controls. After fitting our data to the SDiD estimator, we obtain the ATT shown below.\n\n\nATT: 0.3035668498514986\n\n\nOur ATT indicates that restaurants that have implemented the mobile-thru have, on average, about a 0.31 increase in the proportion of revenue generated from mobile orders. This is a major increase! Previously, drive-thrus had, on average, about 30% of revenue from mobile orders. Now, with mobile-thrus, this number is basically double.\nNote: In a more thorough analysis (and obviously with real data), we’d want to verify this ATT by performing other statistical tests and reporting relevant statistical measures. However, since this is synthetic data for a synthetic example, it suffices to show that the SDiD correctly uncovered the underlying causal mechanism, accounting for confounding effects and properly handling the staggered rollout."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#operational-efficiency",
    "href": "posts/cfa_drive/drive_thru.html#operational-efficiency",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Operational Efficiency",
    "text": "Operational Efficiency\nThe increase in mobile orders means increased insight into customer purchase patterns. We can now know the minute of the day, distance, and preference of food for each individual customer. Since we have a more accurate picture into what customers are looking to purchase, we can leverage predictive analytics to more accurately forecast our supply chain needs and workforce management.\nFurthermore, since we have continuous feedback from customers, these models can be continuously updated to match the current trends and preferences of customers. By planning our resources around this new reliable source of data, Chick-fil-A can enhance operational efficiency to drive profits."
  },
  {
    "objectID": "posts/cfa_drive/drive_thru.html#personalized-customer-experience",
    "href": "posts/cfa_drive/drive_thru.html#personalized-customer-experience",
    "title": "Eat More SDID? Testing Chick-fil-A Mobile Thru using Synthetic Difference-in-Difference",
    "section": "Personalized Customer Experience",
    "text": "Personalized Customer Experience\nThe app gives incredible insight into each customer’s tastes and preferences. As more users continue to go into the app, we can get better insights into different customer demographics and activities. Using those insights, we can better personalize deals and opportunities. We can understand the timing of sending promotions, different app layouts to maximize customer checkout throughput, etc. There are a plethora of ways Chick-fil-A can leverage this new digital touchpoint to better position themselves to provide maximum value to their customers."
  },
  {
    "objectID": "posts/co_clust_1/co_clustering.html",
    "href": "posts/co_clust_1/co_clustering.html",
    "title": "An implementation of the ITCC algorithm in Python",
    "section": "",
    "text": "Abstract\nClustering is a useful tool in data mining whereby data points are collected together based on some predefined similarity metric. This results in a more digestible data model that can provide solid inference into how related certain attributes of data are with each other. In this post, we discuss the information theoretic co-clustering algorithm (Dhillon, Mallela, and Modha 2003) and provide a python implementation of said algorithm. We discuss the usefulness of co-clustering and propose potential future projects.\nSee this link for the paper.\n\n\nIntroduction\nCo-clustering can be defined as a family of algorithms that simultaneously clusters rows and columns. Suppose we have a matrix A that contains n rows and m columns, as shown below.\n\\[A = \\begin{bmatrix}\n    a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{n1} & a_{n2} & \\cdots & a_{nm}\n\\end{bmatrix}\\]\nOur objective with \\(A\\) would be to identify distinct groups of data that exhibit great similarities to one another. One way to expose these groupings would be through a clustering algorithm like K-means. However, the down side to K-means is the single modality nature of the algorithm. That is, rows and columns would be clustered independetly of each other. In real-world scenarios, there are generally relationships between the rows and columns that we’d want to capture as part of our analysis. For example, suppose the rows of \\(A\\) were customers and the columns of \\(A\\) were products available. \\(a_{ij}\\) would then represent the quantities purchased by the \\(i\\)th customer for the \\(j\\)th product. We’d want to capture in our analysis the relationships between customers and the products. This is where co-clustering comes in.\nCo-clustering are families of algorithms that simultaneously cluster rows and columns. Contuining with the example above of rows representing customers and columns representing products, suppose we have the following data.\n\\[\\begin{pmatrix}\n    3 & 3 & 0 \\\\\n    4 & 4 & 0 \\\\\n    0 & 0 & 1\n\\end{pmatrix}\\]\n\\(a_{ij}\\) once again represents the purchases by the \\(i\\)th customer for the \\(j\\)th product. In normal clustering approaches (like in K means clustering), we would ignore the column structure of this matrix and instead focus on the row structure only. Thus, we would probably end up with two clusters, with the the first two rows pertaining to one cluster and the last row pertaining to another. This approach, while helpful in identifying data points that closely resemble each other, does not take into account the complete picture offered by the matrix.\nIn co-clustering, we would analyze both the row structure and the column structure. We would continue to use the row structure, as illustrated in the previous example, but we would also include column clusters, more than likely clustering the first two columns together and the last one by itself. Thinking back to what this data set represents, we can now show at a more granular level what customers have similar purchase histories, as well as what products are purchased together. Thus, we have a better understanding of a customer journey and their interactions with our products.\n\n\nInformation Theoretic Co-Clustering\nNow that we have a solid foundation on what co-clustering is and its potential uses, we explore the co-clustering algorithm “Information Theoretic Co-Clustering” (Dhillon, Mallela, and Modha 2003). For a more detailed explanation, please refer to the paper link above or the referenced paper in the works cited.\nThe ITCC algorithm poses the challenge of clustering as an optimization problem using relative entropy, as shown below in Equation 3.1.\n\\[\\begin{equation}\nD_{KL}(P||Q) = \\sum_{x} \\sum_{y} P(x, y) \\log \\left( \\frac{P(x, y)}{Q(x, y)} \\right) \\label{eq:kl_joint_discrete}\n\\end{equation} \\tag{3.1}\\]\nEssentially, we are trying to find a prototype or approximate joint distribution \\(Q(x,y)\\) to minimizes the distance from \\(P(x,y)\\). To do this, the ITCC algorithm attempts to calculate this minimized approximated joint disribution \\(Q(x,y)\\) by monotonically decreasing the objective function (the objective function being mutual information loss, thus minimizing the information lost between the true joint distribution and the approximate joint distribution).\nTo calculate this approximate joint distribution, a set number of row clusters and column clusters are assigned. Then, rows and columns are assigned to a specific cluster number, up to n row clusters and m column clusters. A joint cluster distribution is then calculated, which we will denote as \\(P(\\hat{x},\\hat{y})\\). The approximate joint distribution \\(Q(x,y)\\) is the calculated using \\(P(\\hat{x},\\hat{y}\\) and other conditional distributions. \\(Q(x,y)\\) is calculated by finding rows and columns that minimize Equation 3.1, relative to the conditional \\(P(X|\\hat{Y})\\) and \\(P(Y|\\hat{X})\\). A more structured run through of the algorithm is found below.\n\n\nInitialize co-cluster for rows (\\(C_X\\)) and co-cluster for columns (\\(C_Y\\))\n\n\nCalculate \\(q^{(0)}\\) \\((\\hat{X}, \\hat{Y})\\), \\(q^{(0)}\\) \\((X|\\hat{X})\\), \\(q^{(0)}\\) \\((Y|\\hat{Y})\\), and \\(q^{(0)}\\) \\((Y|\\hat{x})\\)\n\n\nCompute new column clusters for each row x where \\(C^{t+1}_{X}(x)=\\) \\(argmin_{\\hat{x}}\\) \\(D(p(Y|x) || q^{(t)}(Y|\\hat{x}))\\)\n\n\nCompute distributions \\(q^{(t+1)}\\) \\((\\hat{X}, \\hat{Y})\\), \\(q^{(t+1)}\\) \\((X|\\hat{X})\\), \\(q^{(t+1)}\\) \\((Y|\\hat{Y})\\), and \\(q^{(t+1)}\\) \\((X|\\hat{y})\\)\n\n\nCompute new column clusters for each column y where \\(C^{t+2}_{Y}(y)=\\) \\(argmin_{\\hat{y}}\\) \\(D(p(X|y) || q^{(t+1)}(X|\\hat{y}))\\)\n\n\nCompute distributions \\(q^{(t+2)}\\) \\((\\hat{X}, \\hat{Y})\\), \\(q^{(t+2)}\\) \\((X|\\hat{X})\\), \\(q^{(t+2)}\\) \\((Y|\\hat{Y})\\), and \\(q^{(t+2)}\\) \\((Y|\\hat{x})\\)\n\n\nStop and return current row and column clusters (\\(C_X,C_Y\\)) if the change in the objective function value is small. Else, repeat starting at step 2.\n\nEssentially, the algorithm forms row and column cluster prototypes, calculates the appropriate distributions to get to the approximate joint distribution \\(q(X,Y)\\), then measures the distance between \\(p(X,Y)\\) and \\(q(X,Y\\) using Equation 3.1. The algorithm is able to be monotonically decreasing due to the fact that we always select the cluster that minimizes Equation 3.1 (see steps 3 and 5 in the algorithm steps above).\n\n\nCode Walk-Through\nIn this section, we’ll walk through the various code chunks that make up our ITCC algorithm. Full disclosure, this is a very crude approach to implementing this algorithm. There are far better implementations of this algorithm along with other co-clustering algorithms (e.g. scikit learn). This is merely an educational exercise by me to practice implementing clustering algorithms.\nTo begin the code walk through, we will initialize our joint distribution \\(P(X,Y)\\), which is found below. This is the same joint distribution used in the ITCC paper.\n\n#Initialize test array\ntest_arr = np.array([.05, .05, .05, 0, 0, 0, \n          .05, .05, .05, 0, 0, 0, \n          0, 0, 0, .05, .05, .05, \n          0, 0, 0, .05, .05, .05, \n          .04, .04, 0, .04, .04, .04,\n          .04, .04, .04, 0, .04, .04]).reshape(6,6)\n\nOur \\(P(X,Y)\\) is a 6x6 matrix. We will attempt to find the optimal 3 row clusters and 2 columns clusters using our ITCC implementation, just as performed in the ITCC paper. To do this, we will implement various functions that calculate the necessary distributions for eventually comparing \\(P(X,Y)\\) and \\(Q(X,Y)\\).\nOur first function in the code walk through is calculating the joint distribution \\(Q(\\hat{X},\\hat{Y})\\), which is the joint distribution of the row and column clusters. The code to do this is found below.\n\n#Define function for calculating joint distribution\ndef calc_joint(x: dict, y: dict, a: np.array) -&gt; np.array:\n    joint_arr = np.zeros((len(x), len(y)))\n    #joint_arr = {}\n    for i, j in x.items():\n        for z, p in y.items():\n            ind = np.ix_(j,p)\n            joint_arr[i,z] = a[ind].sum()\n    \n    return joint_arr\n\nThe function accepts the row clusters, column clusters, and joint distribution \\(P(X,Y)\\). We chose to use dictionaries to illustrate the structure of co cluster prototypes with the co cluster number as the key and the value being a list of indices where the respective row(s) or column(s) lie. The function returns the mxn array, where m is the number of row clusters and n is the number of columns clusters.\nThe next function we define is \\(Q(X|\\hat{X})\\). This accepts the joint distribution \\(P(X,Y)\\) and the dictionary for the row cluster prototypes. The function then returns a mxn matrix where m is the number of rows in the original matrix (or the total number of rows from all the row clusters) and n is the number of row clusters.\n\n#Define function for calculating conditional of x given x_hat\ndef calc_x_cond(jd: np.array, x_ind: dict) -&gt; np.array:\n    x_mar = jd.sum(axis=1)\n    xhat_mar = calc_x_mar(jd, x_ind)\n    cond_dist = np.zeros((len(x_mar), len(x_ind)))\n    \n    for key, val in x_ind.items():\n        for idx, prb in enumerate(x_mar):\n            if idx in val:\n                cond_dist[idx,key] = prb / xhat_mar[key]\n            else:\n                cond_dist[idx,key] = 0\n    \n    return cond_dist\n\n\\(Q(\\hat{X}|\\hat{Y})\\) is the conditional distribution of the \\(\\hat{X}\\) given \\(\\hat{Y}\\). This just becomes the joint distribution \\(Q(\\hat{X},\\hat{Y})\\) divided by the marginal of \\(Q(\\hat{Y})\\). The function returns a mxn matrix where m is the number of row clusters and n is the number of column clusters.\n\n#Define function for calculation conditional of x_hat given y_hat\ndef calc_xhat_cond(jd: np.array) -&gt; np.array:\n    cond_dist = np.zeros((jd.shape[0], jd.shape[1]))\n    \n    for j in range(jd.shape[1]):\n        y_mar = jd[:,j].sum()\n        for i in range(jd.shape[0]):\n            if y_mar == 0:\n                cond_dist[i,j] = 0\n            else:\n                cond_dist[i,j] = jd[i,j] / y_mar\n    \n    return cond_dist\n\n\\(Q(X|\\hat{Y})\\) is calculated as the product of \\(Q(X|\\hat{X})Q(\\hat{X}|\\hat{Y})\\). This function accepts then the previously calculated \\(Q(X|\\hat{X})\\) distribution and the \\(Q(\\hat{X}|\\hat{Y})\\) distribution. This function returns a mxn matrix where m is the number of rows from the original joint distribution \\(P(X,Y)\\) and n is the the number of columns from the same said distribution.\n\n#Define function for calculating x given y_hat\ndef calc_x_cond_yhat(x_xhat: np.array, xhat_yhat: np.array, x_ind: dict, y_ind: dict) -&gt; np.array:\n    full_arr = []\n    \n    for y_key, y_val in y_ind.items():\n        for _ in range(len(y_val)):\n            row = []\n            for x_key, x_val in x_ind.items():\n                row.extend(xhat_yhat[x_key,y_key] * x_xhat[x_val, x_key])\n            full_arr.append(row)\n            \n    return np.array(full_arr)\n\nWe now arrive at defining the column distributions, first with \\(Q(Y|\\hat{Y})\\). This function accepts the primary joint distribution and the dictionary of \\(\\hat{Y}\\). The function returns a matrix of dimension mxn where m is the number of columns in the original joint distribution and n is the number of column clusters.\n\n#Define function for calculating conditional of y given y_hat\ndef calc_y_cond(jd: np.array, y_ind: dict) -&gt; np.array:\n    y_mar = jd.sum(axis=0)\n    yhat_mar = calc_y_mar(jd, y_ind)\n    cond_dist = np.zeros((len(y_mar), len(y_ind)))\n    \n    for key, val in y_ind.items():\n        for idx, prb in enumerate(y_mar):\n            if idx in val:\n                cond_dist[idx,key] = prb / yhat_mar[key]\n            else:\n                cond_dist[idx,key] = 0\n    \n    return cond_dist\n\n\\(Q(\\hat{Y}|\\hat{X})\\) is calculated very similariy to \\(Q(\\hat{X}|\\hat{Y})\\) and returns, like the previous function, the same dimension of the of the joint co cluster distribution.\n\n#Define function for calculating conditional of y_hat given x_hat\ndef calc_yhat_cond(jd: np.array) -&gt; np.array:\n    cond_dist = np.zeros((jd.shape[0], jd.shape[1]))\n    \n    for i in range(jd.shape[0]):\n        x_mar = jd[i,:].sum()\n        for j in range(jd.shape[1]):\n            if x_mar == 0:\n                cond_dist[i,j] = 0\n            else:\n                cond_dist[i,j] = jd[i,j] / x_mar\n    \n    return cond_dist\n\n\\(Q(Y|\\hat{X})\\) is equal to the product of \\(Q(Y|\\hat{Y}) Q(\\hat{Y}|\\hat{X})\\). This function accepts the \\(Q(Y|\\hat{Y})\\) distribution, \\(Q(\\hat{Y}|\\hat{X})\\) distribution, the column cluster dictionary, and the row cluster dictionary. The function returns a mxn matrix of the same dimension of the original joint distribution.\n\n#Define function for calculating y given x_hat\ndef calc_y_cond_xhat(y_yhat: np.array, yhat_xhat: np.array, y_ind: dict, x_ind: dict) -&gt; np.array:\n    full_arr = []\n    \n    for x_key, x_val in x_ind.items():\n        for _ in range(len(x_val)):\n            row = []\n            for y_key, y_val in y_ind.items():\n                row.extend(yhat_xhat[x_key,y_key] * y_yhat[y_val, y_key])\n            full_arr.append(row)\n            \n    return np.array(full_arr)\n\n\\(C_X\\) represents our current row clusters. The function below takes in the distributions \\(Q(Y|\\hat{X}\\) and \\(P(Y|X)\\), along with the mappings of each row to row to cluster using the x_ind dictionary. This function utilizes Equation 3.1 to find calculate the distance between these distributions. It then selects the cluster row prototype that minimizes the distance between a row of the true distribution and that of the approximate distribution. The function returns a dictionary with the new row cluster prototypes.\n\n#Define function for calculation next c_x\ndef next_cx(y_xhat: np.array, y_x: np.array, x_ind: dict) -&gt; dict:\n    new_x_hat = {key: [] for key in x_ind}\n\n    for i in range(y_x.shape[0]):\n        temp_idx = []\n    \n        for key, val in x_ind.items():\n            proto = np.mean(y_xhat[val,:], axis=0)\n            kl_res = np.nan_to_num(kl_div(y_x[i,:], proto), posinf=10, neginf=-10)\n            #kl_res = rel_entr(y_x[i,:], proto)\n            temp_idx.append(np.sum(kl_res))\n       \n        temp_val = np.argmin(np.array(temp_idx))\n        #temp_val = tie_breaker(temp_idx)\n        new_x_hat[temp_val].append(i)\n    \n    return new_x_hat\n\nSimilarily to the function above \\(C_Y\\) represents our current column clusters. The function utilizes \\(Q(X|\\hat{Y})\\) and \\(P(X|Y)\\) along with the mappings found in y_ind, which contain the mappings of columns to column clusters. It then uses Equation 3.1 to perform the same calculations used in \\(C_X\\). This function also returns a dictionary with the new mappings for column cluster prototypes.\n\n#Define function for calculation next c_y\ndef next_cy(x_yhat: np.array, x_y: np.array, y_ind: dict) -&gt; dict:\n    new_y_hat = {key: [] for key in y_ind}\n\n    for i in range(x_y.shape[1]):\n        temp_idx = []\n\n        for key, val in y_ind.items():\n            proto = np.mean(x_yhat[:,val], axis=1)\n            kl_res = np.nan_to_num(kl_div(x_y[:,i], proto), posinf=10, neginf=-10)\n            #kl_res = rel_entr(x_y[:,i], proto)\n            temp_idx.append(np.sum(kl_res))\n            \n        temp_val = np.argmin(np.array(temp_idx))\n        #temp_val = tie_breaker(temp_idx)\n        new_y_hat[temp_val].append(i)\n       \n    return new_y_hat\n\nThis function is where we formally define the co-clustering algorithm utilizing the functions defined above as well as the steps outline for the ITCC algorithm above. We refer the reader to the above section “Information Theoretic Co-Clustering”. The function accepts the target distribution we wish to perform co-clustering on, as well as the number of row clusters \\(k\\) and column clusters \\(l\\). The user can also specify the number of iterations. The function returns a tuple of form (dict, dict), where the first dictionary is the row cluster and the second is the column cluster.\n\n#Define co-clustering algorithm\ndef co_cluster(joint_dist: np.array, k: int, l: int, num_iter: int) -&gt; (dict, dict):\n    #Initialize x_hat\n    x_hat = get_x_hat(joint_dist, k, new=True)\n    #Initialize y_hat \n    y_hat = get_y_hat(joint_dist, l, new=True)\n    #Initialize min kl val\n    max_kl = .0001\n    print(f\"init_x_hat: {x_hat}, init_y_hat: {y_hat}\")\n    #Enter loop\n    for _ in range(num_iter):\n        \n        #Calculate q(x_hat, y_hat)\n        q_joint_hat = calc_joint(x_hat, y_hat, joint_dist)\n    \n        #Calculate q(x|x_hat)\n        q_x_cond_xhat = calc_x_cond(joint_dist, x_hat)\n        #Calculate q(y|y_hat)\n        q_y_cond_yhat = calc_y_cond(joint_dist, y_hat)\n        #Calculate p(y_hat|x_hat)\n        q_yhat_cond_xhat = calc_yhat_cond(q_joint_hat)\n        \n        #Calculate q(y|x_hat)\n        q_y_cond_xhat = calc_y_cond_xhat(q_y_cond_yhat, q_yhat_cond_xhat, y_hat, x_hat)\n        #Calculate p(y|x)\n        p_y_x = joint_dist / joint_dist.sum(axis=1).reshape(-1,1)\n        \n        #Get next cx\n        x_hat_2 = next_cx(q_y_cond_xhat, p_y_x, x_hat)\n        #Check if x_hat_2 is valid\n        x_hat_2 = valid_cluster(x_hat_2)\n        \n        \n        #Calculate qt+1(x_hat, y_hat)\n        q_joint_hat_2 = calc_joint(x_hat_2, y_hat, joint_dist)\n        \n        #Calculate qt+1(x|x_hat)\n        q_x_cond_xhat_2 = calc_x_cond(joint_dist, x_hat_2)\n        \n        #Calculate qt+1(x_hat|y_hat)\n        q_xhat_cond_yhat_2 = calc_xhat_cond(q_joint_hat_2)\n        \n        #Calculate qt+1(x|y_hat)\n        q_x_cond_yhat = calc_x_cond_yhat(q_x_cond_xhat_2, q_xhat_cond_yhat_2, x_hat_2, y_hat)\n        #Calculate p(x|y)\n        p_x_y = joint_dist / joint_dist.sum(axis=0).reshape(-1,1)\n        \n        #Get next cy\n        y_hat_2 = next_cy(q_x_cond_yhat, p_x_y, y_hat)\n        #Check if y_hat_2 is valid\n        y_hat_2 = valid_cluster(y_hat_2)\n        \n        \n        #Calculate qt+2(x_hat, y_hat)\n        q_joint_hat_3 = calc_joint(x_hat_2, y_hat_2, joint_dist)\n         \n        #Calculate qt+2(y|y_hat)\n        q_y_cond_yhat_2 = calc_y_cond(joint_dist, y_hat_2)\n        \n        #Calculate qt+2(y_hat|x_hat)\n        q_yhat_cond_xhat_2 = calc_yhat_cond(q_joint_hat_3)\n        \n        #Calculate qt+2(y|x_hat)\n        q_y_cond_xhat_2 = calc_y_cond_xhat(q_y_cond_yhat_2, q_yhat_cond_xhat_2, y_hat_2, x_hat_2)\n        \n        #Calculate q(x,y)\n        joint_first = joint_dist.sum(axis=1).reshape(-1,1) * q_y_cond_xhat\n        #Calculate qt+2(x,y)\n        joint_second = joint_dist.sum(axis=1).reshape(-1,1) * q_y_cond_xhat_2\n        \n        \n        kl_res_1 = np.sum(np.nan_to_num(kl_div(joint_dist, joint_first), posinf=10, neginf=-10))\n        kl_res_2 = np.sum(np.nan_to_num(kl_div(joint_dist, joint_second), posinf=10, neginf=-10))\n        \n        kl = kl_res_1 - kl_res_2\n        \n        if kl &gt; max_kl:\n            x_hat = x_hat_2\n            y_hat = y_hat_2\n        else:\n            print(f\"kl={kl} on iteration {_}\")\n            return x_hat, y_hat\n    \n    return x_hat, y_hat\n\n\n\ninit_x_hat: {0: array([0, 3]), 1: array([1, 5]), 2: array([2, 4])}, init_y_hat: {0: array([2, 5, 1]), 1: array([0, 4, 3])}\nkl=0.0 on iteration 2\n\n\n({0: [0, 1], 1: [2, 3], 2: [4, 5]}, {0: [0, 1, 2], 1: [3, 4, 5]})\n\n\nPutting all the pieces together from our code walk through, the output above shows that we have successfully implemented a ITCC algorithm (this utilized the test case from the paper).\n\n\nConclusion\nThough this tutorial was short and consisted of only one test case, we illustrated the usefulness of co-clustering algorithms in a general sense. Additionally, we walked through the steps to implement the information theoretic co-clustering algorithm, along with example code. Further test cases should utilized in production settings. Nonetheless, we hope this post encourages others to discover for themselves the usefulness of co-clustering in their data mining jobs.\n\n\n\n\n\nReferences\n\nDhillon, Inderjit S., Avinash Mallela, and Dharmendra S. Modha. 2003. “Information-Theoretic Co-Clustering.” In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 89–98. ACM."
  },
  {
    "objectID": "posts/cs_513_paper/cs_513.html",
    "href": "posts/cs_513_paper/cs_513.html",
    "title": "Agentic Marketing: Optimizing Out-of-app Messaging using Causal Inference",
    "section": "",
    "text": "In this paper, I demonstrate a prototype for utilizing reinforcement learning in out-of-app (OOA) messaging optimization to increase customer loyalty. The RL model utilizes causal inference to better utilize true cause-effect relationships to better target customers. You can read my paper at the link below.\nRead the Full Paper (PDF)\nCode for this prototype can be found in the github repo."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html",
    "href": "posts/dispatch_alg/dispatch_alg.html",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "",
    "text": "In this post, we present an implementation for Next-Generation Optimization for Dasher Dispatch at DoorDash [1] and Using ML and Optimization to Solve DoorDash’s Dispatch Problem [2]. We demonstrate this implementation in the context of a smaller, simpler food delivery system. We conclude by presenting some simple sensitivity analysis from changes in our hyperparameters weights in our scoring methods. The code for this simulation can be found at the github repo."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html#whats-the-system",
    "href": "posts/dispatch_alg/dispatch_alg.html#whats-the-system",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "What’s the system?",
    "text": "What’s the system?\nThe first question can be answered by understanding the business objective. We want customers to utilize our service for their food delivery needs. We promote our service by promising fast and accurate delivery right to their doors. Additionally, since our drivers are critical to the operations of this business, we want to keep our drivers as busy as possible to ensure they maximize earnings. Essentially, our business objective is to maintain optimal customer service to those who order while also providing good opportunities to our deliverers to make money. How can we optimize for both objectives simultaneously? Or rather, what system are we trying to control to fulfill this objective? The system we are aiming to control for Food-Ez is the queue. If we can control the queue (i.e. drive the queue to zero), customers orders will be assigned quickly and be delivered quickly. Subsequently, if the queue is managed correctly, drivers’ objective of maximizing earnings will also be optimized. The queue is the system for which we will be designing a controller."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html#what-should-we-feedback",
    "href": "posts/dispatch_alg/dispatch_alg.html#what-should-we-feedback",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "What should we feedback?",
    "text": "What should we feedback?\nNow that we have identified what the system is, we can identify what elements of the system we should report back to the controller. If we are aiming to optimize the queue, we should report back metrics of queue health. Things like queue length, status of orders, status of drivers, location of dropoff and pickup, etc. The list below enumerates some of the crucial feedback elements we will provide to the controller.\n\nOrder Status (Pending, Assigned, En Route, Delivered)\nDriver Status (Idle, Assigned, En Route, Delivered)\nOrder Location (Pickup Location, Dropoff Location)\nDriver Location\nDriver Assigned Orders\n\nFor question two, we are essentially defining the states of important elements in our system. In control theory, this fundamental approach is known as representing a system using state-space equations. The formal equations are found in Equation 3.1.\n\\[\n\\dot{\\mathbf{x}}(t) = \\mathbf{A}\\mathbf{x}(t) + \\mathbf{B}\\mathbf{u}(t)\n\\] \\[\n\\mathbf{y}(t) = \\mathbf{C}\\mathbf{x}(t) + \\mathbf{D}\\mathbf{u}(t)\n\\tag{3.1}\\]\nEquation 3.1 states that the rate of change of the state vector \\(\\dot{x}(t)\\) at time \\(t\\) is equal to the current state vector \\(x(t)\\) multiplied by the dynamics matrix \\(A\\), plus the input vector \\(u(t)\\) multiplied by the input matrix \\(B\\). The system’s output, \\(y(t)\\), is equal to the current state vector of the system \\(x(t)\\) multiplied by the output matrix \\(C\\) plus input vector \\(u(t)\\) multiplied by the feedthrough matrix \\(D\\). Relating these equations back to our control diagram, the controller observes \\(y(t)\\) and compares it against a predetermined reference or baseline. Based on that judgement, the controller sends inputs \\(u(t)\\) back to the system. This input, along with the current state of the system, determines the instantaneous rate of change of the system’s state. This state then produces output \\(y(t)\\), and the process repeats itself.\nA classic example of this is cruise control in a car. The controller observes the car’s speed and compares it against the set cruise speed. It then sends inputs to the engine to adjust the speed accordingly. These inputs, alongside other dynamics (e.g., drag, incline), influence the car’s state, which in turn contributes to the measured output \\(y(t)\\).\nWhile these explanations have been incredibly simplified, they illustrate the important principle that a system can be adjusted in a closed-loop by providing crucial feedback to the controller on its current operation. In our scenario, we want to know the locations and destinations of drivers, the status of food preparation, and other relevant parameters, in order to optimally assign orders within the queue and achieve our business objectives."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html#what-actions-can-we-take-to-adjust-the-system",
    "href": "posts/dispatch_alg/dispatch_alg.html#what-actions-can-we-take-to-adjust-the-system",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "What actions can we take to adjust the system?",
    "text": "What actions can we take to adjust the system?\nWe identified the system and listed some information that would be useful for us to understand how the system is operating. We now need to implement actions that optimize the system’s efficiency. In the previous section(s), we have identified the key objective. We need to assign orders to drivers such that orders get to their locations ASAP while also providing enough work to the drivers. There are several approaches to address this problem and each has its own advantages and disadvantages. We could implement logic such that we always assign the closest driver the order that comes in. Another approach would be holding orders for longer in the queue to see if a more “optimally” positioned driver comes along to take multiple orders nearby. We need sound strategy to identify which algorithmic process we should implement. For Food-Ez, it is optimizing the queue such that we minimize delivery time and maximize driver utilization (i.e. minimize driver idle time). Therefore, we need to design a controller that will balance these two objectives."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html#current-approach-bipartite-matching",
    "href": "posts/dispatch_alg/dispatch_alg.html#current-approach-bipartite-matching",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "Current approach: bipartite matching",
    "text": "Current approach: bipartite matching\nThe original logic followed by Food-Ez (and additionally DoorDash) was designed by framing the task as a bipartite matching problem. Essentially, this problem involves finding a set of edges in a bipartite graph such that no two edges share a common vertex, thereby matching elements from two distinct sets. In the context of food delivery, this essentially means every order can only have one driver and every driver can only have one order. To do this efficiently to fulfill business objectives such as minimizing delivery time, problems like this can be solved using algorithms like the Hungarian algorithm [3].\nWhile this approach fulfills the business objective, simple examples illustrate the weaknesses of this approach. Suppose a deliverer is assigned an order and is heading to the restaurant. On the way, the driver passes another restaurant that just received an order that will be ready shortly. Furthermore, the dropoff location of this order is nearby the dropoff location of the first order. Instead of assigning this order to the driver, the system must assign another driver to this order. While more drivers are utilized in this approach, drivers could be used more efficiently."
  },
  {
    "objectID": "posts/dispatch_alg/dispatch_alg.html#an-example-via-queue-theory",
    "href": "posts/dispatch_alg/dispatch_alg.html#an-example-via-queue-theory",
    "title": "Optimizing Optimization: A Tutorial on Dispatch Algorithms with Applications in the Gig Economy",
    "section": "An example via queue theory",
    "text": "An example via queue theory\nTo illustrate this, let’s pose the simple scenario of a single deliverer working for Food-Ez. Utilizing queue theory, we’ll assume this is an M/M/1 queue where the arrival process is modeled by a Poisson distribution with parameter \\(\\lambda = 1\\) (per hour), the service distribution is an exponential distribution with \\(\\mu = 2\\) (per hour), and there is a single server (deliverer).\nWith these metrics, we can calculate system utilization (or driver utilization in our case). This is defined as \\(\\rho = \\frac{\\lambda}{\\mu}\\), or the ratio between arrival rate and service rate. In our case, for a deliverer in the bipartite matching system, we get 50% driver utilization. While this is not bad, the average queue wait time for an order is defined as \\(W_{q} = \\frac{L_{q}}{\\lambda}\\), where \\(L_{q}\\) is average length of queue. The average length of the queue here is .5 so in our scenario, queue wait time is about .5 hours or 30 minutes. Since our deliverer can only handle one order at a time, every order on average would have to wait 30 minutes in the queue.\nIn a different approach, let’s say a deliverer can now take two orders concurrently and assume it is no further than a 10 minute drive to complete. In this case, a deliverer can fulfill 2 orders in about 40 minutes (1 order / 30 mins was the rate before). Converting this to hours, a deliverer can now fulfill 3 orders per hour, thus changing our deliverer utilization rate from 50% to 33%. More importantly, the queue wait time decreases from 30 minutes to 10 minutes (average queue length in this scenario is ~.167 orders, which is then divided by 1 order per hour). Not only could our drivers stay more busy if arrival rates increased, but our customers would have to wait only a third of the old time (if our simple assumptions hold)!"
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html",
    "href": "posts/doom_alg/doom_alg.html",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "",
    "text": "In this post, we present a prototype recommendation system based on Deep Neural Networks for YouTube Recommendations [1] and Explore, Exploit, Explain: Personalizing Explainable Recommendations with Bandits [2]. We present the prototype in the context of a company that operates as a short-video content platform. We define the strategy behind the prototype by presenting the business objectives. We follow this by walking through the architecture of the prototype and how it fulfills the objectives of the company. We end by illustrating potential next steps for our company. The code for this prototype can be found at the github repo."
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html#company-strategy",
    "href": "posts/doom_alg/doom_alg.html#company-strategy",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "Company Strategy",
    "text": "Company Strategy\nAs mentioned previously, in order for InstaTok to succeed, we need to develop a product that is appealing to both users and creators. In order to create a sound product that answers to both user and creator needs, we need to develop a robust strategy that will guide our approach to product creation. To do this, we need to answer four questions.\n\nWhere do we compete?\nWhat unique value do we bring to the market?\nWhat resources and capabilities can we utilize to deliver unique value?\nHow do we sustain our ability to provide unique value?\n\nWe have established that we are competing in the mobile application market, specifically within the short-video platform arena. We hope to appeal to all ages, but are aiming to capture the college-young professional market. Furthermore, we are looking for those who are looking to separate the world of social media and creative short-video content.\nWe need to provide unique value to both sides of our market. For our users, we are providing a platform where all content is created by official creators. There are no random company ads appearing in the FYP nor social connection content. We focus the user experience on entertainment content. On top of that, we provide personalization to our users by algorithmically matching them with content they would enjoy.\nFor our creators, we provide unique value through monetization and verified creator status. Creators compete solely with other creators for views, not with general social media users. Additionally, other platforms collect the ad revenue generated by creator content. On our platform, we provide a way for creators to directly collect ad revenue from brands.\nOur key resource and capabilities include our proprietary algorithms, our brand partnerships, and deep technology expertise. These are also key in how we sustain our unique value. By continuing to iterate our algorithms, grow our partnerships, and deepen our technology expertise, we can sustain a unique advantage in this marketplace."
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html#from-strategy-to-product",
    "href": "posts/doom_alg/doom_alg.html#from-strategy-to-product",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "From Strategy to Product",
    "text": "From Strategy to Product\nWe’ve defined our company strategy, and now we must use it to create a viable product. Our minimum viable product (MVP) should be a mobile application with a user-friendly interface. This interface should provide short-video content to our users one video at a time. Users advance to the next video by swiping up on the screen. These functions should be designed in a way that is intuitive for our target demographic. Additionally, the algorithms powering the service need to be personalized to maximize each user’s specific utility.\nIn this post, we will focus our attention on the personalization algorithms powering our platform. As mentioned, our personalization algorithm is one of our key resources/capabilities in our company strategy. We need it to be incredibly valuable in the eyes of our users and creators. To achieve this, we utilize data to drive our decision making. Table 2.1 shows an example of this kind of data.\n\n\nTable 2.1: User Interests by Region\n\n\nRegion\nInterest\n\n\n\n\nwest\ntech\n\n\nmid\nanimals\n\n\neast\nfood\n\n\n\n\nAccording to our market research, users generally have two categories of videos that entice them to keep watching. Furthermore, we learned that users are open to exploring new categories of videos based on their geographic location. For example, those in the midwest appear to be open to exploring videos featuring animals. These insights should drive product development for our personalization algorithm."
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html#system-design",
    "href": "posts/doom_alg/doom_alg.html#system-design",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "System Design",
    "text": "System Design\nWhile previous sections have focused on the business aspects of our product, this section describes the technical details needed to design and implement a personalization system. Our problem can be boiled down to this one question: how can we optimize our content offerings for any given user such that a creator maximizes their revenue? To do this, we begin with simple microeconomic theory. We believe that any given user is a rational agent seeking to maximize their utility. We further believe that users have downloaded InstaTok to maiximize their entertainment utility. Therefore, if we provide them with videos that help maximize this utility, they will continue to use the platform (i.e. maximize their watch time per session).\nTo answer our question on how we can maximize entertainment utility for a given user, we propose a system in Figure 3.1 that illustrates how we can capture user data to provide meaningful personalization.\n\n\n\nFigure 3.1: Personalization System Design\n\n\nFigure 3.1 begins with a user opening our app and interacting with a shown video. The interaction data is sent to our data warehouse where it is stored with historical interaction data. Data is then sent from the warehouse to three functions. These functions help define the current state of our user. The first function retrieves current session data pertinent to our given user (e.g. demographic data, location data, etc.).\nThe second function is our base recommendation system. The system parses our large video catalog and generates a candidate list of videos that best match user preferences. This list is further refined by adjusting the scores from the base recommendation with additional business logic. We will discuss the base recommendation algorithm in detail later on.\nThe third function is our interest prediction layer. To better inform our decision on which video to show the user, we use past interaction data to predict user interest categories. Note: In a true production environment, this would be our approach. However, for the purposes of this post, we randomly generate these interests for each user.\nThese functions provide output that defines the current state of the user. This state is fed into our reinforcement learning agent (RL agent) which then chooses which action to take (i.e. which video to show the user next). The selection is sent back to the user and the process repeats itself until the user closes the app. We will provide more detail on the RL agent later on."
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html#recommendation-system-architecture",
    "href": "posts/doom_alg/doom_alg.html#recommendation-system-architecture",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "Recommendation System Architecture",
    "text": "Recommendation System Architecture\nOur recommendation system is a simple two-phase architecture. The first phase is the initial candidate list generation. To do this, we first assign scores to the interaction data. Table 3.1 shows our assignment scores.\n\n\nTable 3.1: Interaction Scores\n\n\nInteraction\nScore\n\n\n\n\nwatched\n1\n\n\nskipped\n-2\n\n\nliked\n2\n\n\nother\n-0.5\n\n\n\n\nThese scores are aggregated to create a item-user matrix, as depicted in Equation 3.1. Note: For faster computation, we transform this into a sparse matrix using scipy.\n\\[\n\\begin{pmatrix}\n2 & -.5 & 1 \\\\\n1 & 1 & 1 \\\\\n2 & -2 & 1\n\\end{pmatrix}\n\\tag{3.1}\\]\nEquation 3.1 is a \\(n\\) by \\(m\\) matrix where each row \\(i\\) is an item and each column \\(j\\) is a user. The goal of setting the matrix like this is to identify relationships between items. We want to identify which items are most similar to each other based on similar scores from users. To calculate this, we utilize cosine similarity [3] as shown in Equation 3.2.\n\\[\n\\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n\\tag{3.2}\\]\nEquation 3.2 allows us to measure the cosine of the angle between two item vectors. A smaller angle means higher similarity. Once these are all calculated, the results are stored in a \\(n\\) by \\(n\\) matrix, where each row and column corresponds to a specific item (i.e. an item-item matrix). We can then perform lookups on this matrix to recommend items that are most similar to those a user previously interacted positively with. Once we have \\(r\\) number of recommendations for a user, these items with their respective scores are sent to our weighted-sum function for weighting based on important business objectives. This process of using item-item similarity and weighting according to business objectives is known as collaborative filtering [4].\n\\[\n\\text{Final Score} = w_{1}\\text{Similarity Score} + w_{2}\\text{Num Views} + w_{3}\\text{Is Interest}\n\\tag{3.3}\\]\nThe final score of a video is weighted by three key attributes: the similarity score (produced from Equation 3.1), the number of views a video has (the popularity of it), and a boolean variable indicating whether the video aligns with the user’s interests."
  },
  {
    "objectID": "posts/doom_alg/doom_alg.html#rl-agent-architecture",
    "href": "posts/doom_alg/doom_alg.html#rl-agent-architecture",
    "title": "The Doomscrolling Algorithm: Optimizing Content to Maximize Revenue",
    "section": "RL Agent Architecture",
    "text": "RL Agent Architecture\nOur recommendation system currently outputs the top \\(r\\) recommendations for a user based on their past interaction history. This is a great feature, but it does not capture the entire story of a user. Each time a user opens our app, they are in a different state of being. They could be bored, anxious, excited, or experiencing any other emotion. To better understand our users and present them content that aligns with their current state, we utilize reinforcement learning (RL) to model these states and their subsequent feedback. RL is the industry standard for capturing sequential decision making.\nFrom a high-level, we refer back to Figure 3.1. The RL agent takes in current session information (number of videos watched so far, number of skips, last video category show, etc), the candidate list of videos with their final scores, and current “predicted” user interests. The RL agent then chooses an action (i.e. which video to show), sends that video to the user, and records the interaction. As the RL agent continues to do this, it learns which states matches certain videos. The more data it has around the interactions between users and videos given current states, the more optimized its policy will be at choosing the correct next best video.\nTo model this system, we use a Deep Q Network (DQN) [5]. The DQN utilizes neural networks to approximate the Q Function [6], as shown in Equation 3.4.\n\\[Q^*(s, a) = \\mathbb{E}_{s' \\sim P(\\cdot|s,a)} \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\right] \\tag{3.4}\\]\nEquation 3.4 is the optimal Q-value function for a Q-learning algorithm. The equation basically says that the optimal Q-value for taking action \\(a\\) in state \\(s\\) is equal the expectation of the sum of the immediate reward of taking action \\(a\\) in state \\(s\\) and the optimal Q-value for the next state \\(s\\) over all possible next actions \\(a\\). Simply put, this function estimates the expected future cumulative reward of taking action \\(a\\) in state \\(s\\).\nIn our DQN, we estimate these Q-values using neural networks. The neural network takes the current state \\(s\\) as input, passes it through the hidden layers, and produces \\(r\\) number of Q-values. The DQN then chooses a video to show the user. To do this, we use an epsilon-greedy [7] approach to balance exploration with exploitation. The video is then shown to the user, the user interacts with the video, the interaction is recorded, and the process begins again.\nOur DQN learns via experience replay. The RL agent learns via mini-batches of experiences that are randomly sampled from a “memory storage”. This breaks correlation and improves training stability. The training is performed like many other DL algorithms where we attempt to minimize a loss function. In our prototype, we use mean squared error. The loss is calculated as the difference between our target Q-value and the predicted Q-value, as shown in Equation 3.5.\n\\[\\min_{\\theta} \\mathcal{L}(\\theta) = \\min_{\\theta} \\mathbb{E} \\left[ \\left( Y - Q(s, a; \\theta) \\right)^2 \\right] \\tag{3.5}\\]\nSince we don’t directly observe target Q-values in the real world, we estimate them via a target network in our RL agent. Our predicted Q-values are derived from an entirely different network known as the online network.\nAll specific technical details of the DQN, state and action space, etc. can be found at the github link in the abstract."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html",
    "href": "posts/fan_causal/fan_causal.html",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "",
    "text": "In this post, we present a scenario where a university seeks to identify the causal effects of sports passes on alumni donation likelihood. Using a synthetic dataset mimicking a random sample of alumni five years post-graduation, we explore the data to gather insights and illustrate the flow of causality via a Directed Acyclic Graph (DAG). We then demonstrate how a university can calculate these causal effects using two different models: logistic regression and causal forests. Finally, we discuss how a university might strategically utilize these findings to increase philanthropic activity among alumni."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#gathering-data",
    "href": "posts/fan_causal/fan_causal.html#gathering-data",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Gathering Data",
    "text": "Gathering Data\nIn order to attempt to establish a causal relationship, we first need to identify the nature of our data. The gold standard for establishing causal relationships is performing a randomized controlled trial (RCT). However, in our situation, this is infeasible. We would not want to randomly assign sports passes to some students and withhold them from others, as this would significantly reduce potential revenue from pass sales. Furthermore, we cannot guarantee that students assigned a pass would actually utilize it.\nFrom a university’s perspective, we have a long history of selling sports passes and have significantly enhanced our ability to track student activity through student IDs across various campus engagements. While we lack experimental data, we have a plethora of observational data. Although extracting causal relationships from observational data is more complex, recent advancements in causal inference have made these estimates considerably more robust [3].\nEven though we have a plethora of observational data, we still need to design a sampling strategy in order to set up our analysis correctly. To do this, we’ll use a stratified sampling approach where we take equal random samples across different university majors. Furthermore, we gather other important covariates for modeling. Below is a sample of our gathered data.\n\n\n\n\n\n\n\n\n\nmajor\nGPA\ngender\nalumni_parents\nplayed_intramurals\ncurrent_salary\nsports_pass\nhas_donated\n\n\n\n\n0\nHumanities\n2.44\nf\n0\n0\n64.78\n0\n0\n\n\n1\nComputer Science\n2.71\nf\n0\n1\n97.37\n1\n0\n\n\n2\nEngineering\n2.87\nm\n0\n1\n89.18\n0\n0\n\n\n3\nComputer Science\n2.85\nf\n1\n1\n99.85\n1\n0\n\n\n4\nHumanities\n2.99\nf\n0\n1\n44.13\n1\n0\n\n\n\n\n\n\n\nThe samples we gathered were from alumni who graduated five years ago. Our response variable has_donated is 1 for donated and 0 for no donation. Our treatment variable sports_pass is also binary where 1 is for purchased a sports pass and 0 for no. Since we are dealing with establishing causality, we want to include potential confounders as part of the dataset. To account for this, we gathered data on alumni majors, GPA, gender, if their parents are alumni, if they played intramurals, and their current salary. Our data in total contains 3000 observations."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#exploring-the-data",
    "href": "posts/fan_causal/fan_causal.html#exploring-the-data",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nBefore diving into the causal models, we first explore the data to see if there is any potential evidence of causality. Our first graph Figure 4.1 shows the count between non-donors and donors in our dataset.\n\n\n\n\n\nFigure 4.1: Count of Non-Donors vs Donors\n\n\n\n\nFigure 4.1 clearly shows that the majority of our observations are non-donors. We can see how these numbers are across different majors in Figure 4.2.\n\n\n\n\n\nFigure 4.2: Count of Non-Donors vs Donors by Major\n\n\n\n\nFigure 4.2 reveals an interesting relationship between donors and major. It seems that, according to our sample, donation likelihood across majors varies. Business majors appear to have the highest percent of donors whereas engineering appears to have the lowest. Figure 4.3 continues this investigation by seeing the proportion of alumni who purchased sports passes in their undergrad split by major.\n\n\n\n\n\nFigure 4.3: Count of Non-Sports Pass Holders vs Sports Pass Holders by Major\n\n\n\n\nFigure 4.3 reveals an interesting finding: Business majors exhibit the highest proportion of sports pass holders, with engineering majors once again showing the lowest. While we can’t immediately attribute the higher number of donors from business majors solely to their high proportion of sports pass ownership, we are encouraged by this finding as it presents a plausible explanation for the observed donation patterns. To further investigate this possibility, we can look at Figure 4.4.\n\n\n\n\n\nFigure 4.4: Count of Non-Donors vs Donors by Sports Pass\n\n\n\n\nFigure 4.4 is very informative. There is a much higher proportion of donors who come from those who own sports passes than those who did not own a sports pass. Again, it is too early to attribute causality as the reason behind these relationships. Nonetheless, it is once again an encouraging sign for us attempting to discover a causal relationship between sports pass and donor likelihood. For example, Figure 4.5 shows a potential confounder to donation.\n\n\n\n\n\nFigure 4.5: Salary Distribution between Donors and Non-Donors\n\n\n\n\nfig-eda-5 shows current salary as a potential confounder. The distribution of donors appears to be more left-skewed than that of non-donors, signifying that the mean and median salary of donors are higher. While we don’t know if this difference is statistically significant, it suggests that a person’s current salary, rather than their sports connection to the university, could be a reason for their donation behavior."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#illustrating-the-flow-of-causality",
    "href": "posts/fan_causal/fan_causal.html#illustrating-the-flow-of-causality",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Illustrating the Flow of Causality",
    "text": "Illustrating the Flow of Causality\nThe best way to begin to understand all the potential causal influences on a response variable in observational studies is via a directed acyclic graph (DAG). DAGs are the language of causal inference since we can visually see the flow of causality from our potential confounders and treatment to our response variable. Figure 4.6 shows our proposed DAG.\n\n\n\n\n\nFigure 4.6: DAG representing the flow of causality in our observational data\n\n\n\n\nFigure 4.6 clearly shows many different influences on likelihood of donation. For example, the choice of major causes the current salary which then flows into how likely an alumni is to donate. We also hypothesize the potential that those who have parents that are alumni has a causal relationship with donation and sports pass. We also illustrate the potential of unobserved confounders influencing the likelihood of purchasing sports passes and the likelihood of donating.\nTo identify a causal relationship between sports pass and donation, we need to isolate the flow of causality between sports pass and donation. To do this, we need to block any backdoor paths of causality. For example, since having parents who are alumni is a cause of both donation and sports pass, this introduces a backdoor flow of causality that confounds the effects of sports pass. In total, we need to block Major and Parent Alumni to eliminate backdoor causal paths.\nUnfortunately, we can’t block unobserved confounders since there is no way of measuring them. We’ll have to address this problem later on in our modeling through sensitivity analysis."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#logistic-regression-to-estimate-average-treatment-effect-ate",
    "href": "posts/fan_causal/fan_causal.html#logistic-regression-to-estimate-average-treatment-effect-ate",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Logistic Regression to estimate Average Treatment Effect (ATE)",
    "text": "Logistic Regression to estimate Average Treatment Effect (ATE)\nAfter establishing our causal relationships via Figure 4.6, we can begin to define a model that will estimate our average treatment effect (ATE). A simple way we can do this is via logistic regression, as shown in Equation 4.1.\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta^{T}X\n\\tag{4.1}\\]\n\\(X\\) is our design matrix, \\(\\beta\\) is our vector of coefficients, and the left hand side of the equation is our log-odds ratio. Logistic regression aims to maximize the log likelihood through finding the proper \\(\\beta\\) values. In other words, it aims to find the most likely \\(\\beta\\) values that would occur with the \\(X\\) and \\(y\\) combinations.\nIn our scenario, we identified that we need to include major and parent alumni in our design matrix to isolate the effects of sports pass on likelihood of donation. However, since including other covariates wouldn’t introduce confoundness, we can include the rest of the covariates from Figure 4.6. Our model output is shown below.\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-3.9580\n0.618\n-6.407\n0.000\n-5.169\n-2.747\n\n\nC(major)[T.Computer Science]\n-0.8742\n0.139\n-6.285\n0.000\n-1.147\n-0.602\n\n\nC(major)[T.Engineering]\n-2.2845\n0.209\n-10.917\n0.000\n-2.695\n-1.874\n\n\nC(major)[T.Humanities]\n0.0752\n0.237\n0.318\n0.751\n-0.389\n0.539\n\n\nC(major)[T.Life Science]\n-1.4476\n0.232\n-6.234\n0.000\n-1.903\n-0.993\n\n\nsports_pass\n1.4989\n0.106\n14.181\n0.000\n1.292\n1.706\n\n\nalumni_parents\n0.9650\n0.109\n8.880\n0.000\n0.752\n1.178\n\n\nGPA\n-0.1134\n0.106\n-1.065\n0.287\n-0.322\n0.095\n\n\nplayed_intramurals\n0.0484\n0.104\n0.465\n0.642\n-0.156\n0.253\n\n\ncurrent_salary\n0.0289\n0.005\n5.729\n0.000\n0.019\n0.039\n\n\n\n\n\nOur primary parameter of interest is sports_pass. Our log-odds increase in donation likelihood is 1.4989, which on the odds scale about 4.47. This means that on average, holding all else constant, those with sports passes have 4.47 higher odds of donating than those who do not have sports passes. While this insight is informative, a better metric of the causal effect of sport pass would be the ATE. The equation for ATE and the corresponding result are shown below.\n\\[\n\\text{ATE} = E[Y(1) - Y(0)] = E[Y(1)] - E[Y(0)]\n\\tag{4.2}\\]\n\n\nATE: 0.22\n\n\nAs shown in Equation 4.2, the ATE is the difference in expectation between the treated and the untreated groups across the entire population. To calculate this with logistic regression, we fit the model, then predict outcomes for all individuals under both treatment and non-treatment scenarios, and finally take the difference between the two resulting averages. Our result above shows that we obtained an ATE of 0.22. This means that, on average, those who purchased sports passes have a 22 percentage point increase in the probability of becoming a donor compared to those who did not purchase a sports pass.\nThis result is a strong indication of a causal relationship between sports pass and donation activity. In order to verify this result, we should check that our key causal assumptions hold under the potential outcomes framework [4]. The first assumption is the stable unit treatment value assumption (SUTVA). This says that treatment assignment of one individual does not affect the outcome of another individual and that treatmnet is a consistent experience for each individual. We will say this assumption holds since there is just one version of sports pass in our study and there is no information saying that individuals who purchase a pass affect the donation behavior of another person.\nThe second assumption we need to verify is unconfoundness. This states that treatment assignment is independent of potential outcomes, conditioned on the observed covariates. This assumption is currently violated with the presence of our unobserved confounders. However, we can relax this assumption by calculating E-values to perform sensitivity analysis. E-values quantify the minimum “strength” that an unmeasured confounder would need to have on the treatment and outcome to explain away any observed association. In our E-value calculation, we use the lower bound of our 95% CI. Our result is shown below.\n\n\nE-value for low CI estimate: 6.74\n\n\nOur lower bound result is strong with 6.74. This would mean that any unobserved confounder would need to shift the confidence interval of sports pass by this amount to include the null hypothesis. While we still techinically have a biased estimate of ATE, because of our e-value, we can at least assume that this causal relationship of sports pass on donation is robust to a fair amount of unobserved confounding."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#conditional-average-treatment-effect-cate-via-causal-forests",
    "href": "posts/fan_causal/fan_causal.html#conditional-average-treatment-effect-cate-via-causal-forests",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Conditional Average Treatment Effect (CATE) via Causal Forests",
    "text": "Conditional Average Treatment Effect (CATE) via Causal Forests\nWhile our ATE estimate is a good indication of a causal relationship between sports pass and donor behavior, we don’t expect this effect to be consistent across different demographics. In order to quantify the impact of our treatment on different alumni demographics, we need to obtain the conditional average treatment effect (CATE). The CATE is very similar to Equation 4.2, but differs in that it is conditioned on a set of covariates, as shown in Equation 4.3.\n\\[\n\\text{CATE}(X) = E[Y(1) - Y(0) | X] = E[Y(1) | X] - E[Y(0) | X]\n\\tag{4.3}\\]\nWhile we technically could obtain CATE estimates from our logistic regression model, one potential downfall to our logistic model is that it takes on a parametric form. In other words, we have strong assumptions about the nature of our data generating problem. To relax this assumption, we can use a nonparametric learner to understand the patterns in our data. In our case, we can utilize Causal Forests [5]. We briefly walk through how Causal Forests estimate CATE below.\n\nCreate Causal Trees: This step involves creating multiple different trees just as a decision tree does, only these trees are to identify different subgroup effects.\nSplit Nodes Based on Heterogeneity: At each node, the algorithm finds the split that maximizes the heterogeneity of treatment effect between resulting children nodes.\nDebias using Honesty: To debias the estimates, Causal Forests utilize an honesty metric, which means the splitting metric is independent of the treatment effect metric, as they use different data\nEstimate Local ATE: Once a tree is full, an observation passes through the tree and lands in a terminal node, where then the ATE is calculated between those in treated and those in control.\nAverage Across Trees: For new observation with X covariates, we gather the ATE across all the terminal nodes where this observation would land and take the average, resulting in the CATE.\n\nWe use econML [6] to provide us with an implementation of Causal Forests. After fitting our data to the forest, we obtained the following CATE estimates for different majors in Figure 4.7.\n\n\n\n\n\nFigure 4.7: CATE across Different Majors\n\n\n\n\nTo reiterate what the CATE represents, it is the incremental, or uplift, effect of our treatment. For example, business majors who purchase a sports pass experience, on average, a little more than a 25 percentage point increase in the probability of donating compared to those who do not purchase a sports pass. As we can see, the CATE is fairly different across majors. The effect of the sports pass appears to have the highest impact on Business and Computer Science majors, with the lowest being Engineering. However, since all these CATEs appear to be above 0.2, this strongly indicates that encouraging sports pass purchases can be an effective strategy to increase future donor likelihood among alumni.\nWe can further explore other covariates such as parent alumni status, as shown in Figure 4.8.\n\n\n\n\n\nFigure 4.8: CATE across Non-Alumni Parents vs Alumi Parents\n\n\n\n\nBuilding on the insights from Figure 4.8, the CATEs appear quite similar and both remain positive. While we do not know the baseline probability of donation for these two categories, our analysis indicates that individuals within both groups react positively towards donating in the future when they purchase a sports pass."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#strategy-based-on-analysis",
    "href": "posts/fan_causal/fan_causal.html#strategy-based-on-analysis",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Strategy Based on Analysis",
    "text": "Strategy Based on Analysis\nDespite the relatively concise nature of our analysis, our findings are clearly impactful. Assuming the validity of our causal assumptions and sampling design, we have identified a controllable lever that universities can utilize to increase alumni donor likelihood. If these findings were to guide university strategy, actively encouraging more students to purchase sports passes would require careful strategic planning. For example, our analysis could inform targeted advertisements or promotions aimed at students identified as more likely future donors. Alternatively, the university might consider expanding the number of sports passes offered to broaden the pool of potential donors. Another valuable avenue for future research would be to further analyze which specific aspects of sports pass ownership cultivate stronger donor relations (e.g., fostering positive memories, facilitating social connections). In order to best utilize our findings, we would need to develop sound strategy on how to implement these findings and align it with the mission of the university."
  },
  {
    "objectID": "posts/fan_causal/fan_causal.html#wrap-up",
    "href": "posts/fan_causal/fan_causal.html#wrap-up",
    "title": "The Impact of Sports on University Philanthropy: A Causal Analysis",
    "section": "Wrap-up",
    "text": "Wrap-up\nOverall, this simple analysis demonstrated key frameworks and models to help administrators identify levers within their university to increase philanthropic behavior from their alumni. While we made some strong assumptions on the validity of our data and the nature of our findings, the principles illustrated in this case study can be useful for many different scenarios. In the end, we hope this analysis provided you with the opportunity to see causal inference in action, from problem formulation to analysis and finally realizing that next steps include more strategic planning and research."
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html",
    "href": "posts/grocery_ops/diff_diff.html",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "",
    "text": "In this post, we present the situation of a grocery store chain looking to decrease checkout-time by supplying scanner guns at self-checkout registers. We demonstrate first the usefulness of difference-in-differences (DiD) methodology in calculating causal effects of the intervention. We then demonstrate how to utilize synthetic control methods (SCM) to perform DiD when there is not a single suitable control unit available. The code for this analysis can be found in the github repo."
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html#understanding-optimal-efficiency-with-cycle-time-and-throughput-rate",
    "href": "posts/grocery_ops/diff_diff.html#understanding-optimal-efficiency-with-cycle-time-and-throughput-rate",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "Understanding Optimal Efficiency with Cycle Time and Throughput Rate",
    "text": "Understanding Optimal Efficiency with Cycle Time and Throughput Rate\nTo begin our analysis, we establish the current flow of traffic in the store, specifically at checkouts. To do this, we’ll look at the current checkout time distribution at SEA, shown in Figure 3.1.\n\n\n\n\n\nFigure 3.1: Checkout time distribution in seconds for SEA store.\n\n\n\n\nFrom Figure 3.1, we expect a customer at the SEA store to take about 120 seconds on average to complete the checkout process. We’ll assume the process of checkout to be as follows.\n\nCustomer scans membership card (begins timer).\nCustomer begins scanning each individual item and places item on side platform next to register.\nAfter scanning all items, customer pays for items (only accepts cards at self-checkouts).\nCustomer returns all items to cart and transaction ends.\n\nThe time to move one unit (ie one customer) through the above process is known as cycle time (CT). In MW stores, there are 4 self-checkout registers available for customers to use. In the optimal scenario, each station would begin and end in parallel and the demand for each station would immediately be filled. The total output from these 4 stations over a specified time interval is defined as throughput rate (TPR). The relations between CT and TPR are shown in Equation 3.1.\n\\[\nCT = \\text{average time to process one unit}\n\\] \\[\nTPR = \\frac{\\text{num units produced}}{\\text{total time elapsed}}\n\\] \\[\nTPR = \\frac{1}{CT}\n\\tag{3.1}\\]\nFrom Equation 3.1, TPR is the reciprocal of CT. Since we have CT, we know TPR must equal 1 / 120 seconds. To gain a more practical view of the operations, we’ll convert this to minutes and say that our TPR is 1 / 2 minutes, or half a customer per minute is our TPR. Additionally, since we have 4 stations, we multiply this TPR by 4 to get 2 customers per minute on average. This is the assumed peak efficiency of our current system."
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html#queue-theory",
    "href": "posts/grocery_ops/diff_diff.html#queue-theory",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "Queue Theory",
    "text": "Queue Theory\nIn the previous section, we established the peak efficiency the system can handle through CT and TPR. One of the big assumptions we placed on this system was a steady-state flow, meaning there would be immediate demand for each station once they became free. In the real-world, steady state is nearly impossible to achieve. Additionally (and possibly more importantly for us), understanding the current wait time of customers will help us to identify how speeding up the above system can help decrease wait time. This branch of operations is known as queue theory.\nWhile we will not provide an exhaustive tutorial on queue theory in this post, we will provide the necessary overview for this section. To do this, we begin with Kendall’s Notation[1]. Kendall’s notation provides a framework where we identify the 3 core components of queue creation, shown in Equation 3.2.\n\\[\nA: \\text{arrival process distribution (how often customers arrive)}\n\\] \\[\nB: \\text{service time distribution (how long it takes to serve a customer)}\n\\] \\[\nC: \\text{number of servers}\n\\tag{3.2}\\]\nFrom the previous section, we learned B (defined as our CT) and C (4 self-checkout stations). We must now identify A. Let \\(\\lambda\\) denote the average arrival rate of customers. In the context of our problem, we will suppose \\(\\lambda\\) to be the average arrival rate during our peak hours (since slow hours do not tend to have any kind of queue). The arrival distribution (which we model as a poisson distribution) is shown below in Figure 3.2.\n\n\n\n\n\nFigure 3.2: Arrival distribution of customers during peak hours.\n\n\n\n\nOn average (during peak hours), we have about 2 customers arriving per minute. To better know the amount of time our registers are busy, we calculate system utilization, commonly denoted by \\(\\rho\\). The formula for \\(\\rho\\) is found in Equation 3.3.\n\\[\n\\rho = \\frac{\\lambda}{c\\mu}\n\\tag{3.3}\\]\nSystem utilization is the defined as the ratio between the average arrival rate and the TPR we previously calculted. In our case, \\(\\rho\\) comes out to be .95, or 95%. According to queue theory, a system is stable if \\(\\rho\\) is &lt; 1. Our current system is therefore stable and allows us to use M/M/c formulas to calculate relevant wait time and queue length metrics. The formulas used all depend on Little’s Law[2], illustrated in Equation 3.4.\n\\[\nL = \\lambda W\n\\tag{3.4}\\]\nL represents the number of customers in the queue, \\(\\lambda\\) is the arrival rate of the customers, and W is the wait time of the customers. For our system specifically, we use an adapted version of Equation 3.4, as shown in Equation 3.5.\n\\[\nL_{q} = \\frac{P_w \\rho}{1-\\rho}\n\\tag{3.5}\\]\nEquation 3.5 utilizes Erlang’s formula[3] to get \\(P_w\\), which is the probability of an arriving customer having to wait in the queue. This is scaled by result of \\(\\frac{\\rho}{1-\\rho}\\). The result gives us the average length of the queue at any point in time (during peak hours). While we won’t cover the full derivation and calculation of \\(P_w\\), we provide that the value of it is approximately .9. Plugging this value in with our \\(\\rho\\) value, we get \\(L_q\\) is equal to about 16.93, or about 17 people on average are waiting in the queue during peak shopping hours. That’s a lot of people!\nTo calculate the average time a customer waits in the queue, we utilize Equation 3.4 by rearranging the equation to solve for W. After performing some calculations, we get W is equal to 8.91 minutes. MW customers on average wait almost 9 minutes in line during peak shopping hours!"
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html#experiment-design-1-difference-in-differences",
    "href": "posts/grocery_ops/diff_diff.html#experiment-design-1-difference-in-differences",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "Experiment Design 1: Difference-in-Differences",
    "text": "Experiment Design 1: Difference-in-Differences\nOne approach we can take to help MW is to assign just one store the treatment and compare it with another store that is fairly comparable to the treated store. This approach is know as the Difference-in-Differences (DiD)[4] model. DiD is known as a quasi-experimental design since it doesn’t rely on random assignment. Instead, it “mimics” the effects of RCTs by accounting for unobserved confounders by measuring the change in outcomes to a similar unit. Specifically, this is known as the parallel trends assumption. DiD assumes that both the treated unit and the control unit follow parallel trends had there not been any intervention. Mathematically, this is represented as a linear model as shown in Equation 4.1.\n\\[\nY_{i} = \\beta_{0} + \\beta_{1}SEA_{i} + \\beta_{2}POST_{i} + \\beta_{3}(SEA_{i}*POST_{i}) + \\epsilon_{i}\n\\tag{4.1}\\]\n\\(Y_{i}\\) is the outcome being measured (average checkout time), \\(SEA_{i}\\) is a dummy variable indicating whether the observation is in Seattle or not. \\(POST_{i}\\) is a dummy variable indicating whether or not the observation occurred after the intervention. We then have the interaction term where \\(\\beta_{3}\\) would show the average treatment effect between Seattle and the control city after the intervention was applied.\nTo get a little more granular into how a simple OLS regression model derives causal effects from this design, consider the basic case of the potential outcomes framework[5]. We extend this to DiD by saying each observation takes on an outcome of \\(Y_{D}(T)\\), where D is the treatment assignment (1 for Seattle and 0 for the control) and T is the time period (1 for post intervention and 0 for pre intervention). In the counterfactual world, we would be able to calculate the average treatment effect of treated (ATT) by computing \\(E[Y_{1}(1) - Y_{0}(1)|D=1]\\). However, \\(Y_{0}(1)\\) is the counterfactual for any given observation where we observe the treated in post intervention. To work around this, we replace the missing counterfactual by setting it equal to \\(E[Y_{0}(0)|D=1] + (E[Y_{0}(1)|D=0] - E[Y_{0}(0)|D=0])\\). With a bit of rearranging to the original “preferred” equation, we arrive at Equation 4.2.\n\\[\nATT = (E[Y(1)| D=1] - E[Y(0)| D=1]) - (E[Y(1)| D=0] - E[Y(0)| D=0])\n\\tag{4.2}\\]\nIn essence, we take the difference between the treatment post and pre intervention and the difference between the control post and pre intervention, then take the difference of those differences. Hence, we arrive at the name “difference-in-differences”."
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html#did-results",
    "href": "posts/grocery_ops/diff_diff.html#did-results",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "DiD Results",
    "text": "DiD Results\nSince we’ve now established how we can model our problem using DiD, we can now get into the results of our model. As stated previously, Seattle is the location our treatment (i.e. implementing scanner guns for check-outs). From previous analysis, we believe a good control candidate that follows our DiD assumptions would be San Francisco (SF). We collected data over 100 days from both locations. At the 50 day mark, we rolled out the scanner guns in Seattle. Figure 4.1 shows the data we collected over time at both SEA and SF.\n\n\n\n\n\n\n\n\n\navg_ct\nsea\npost_int\n\n\n\n\n0\n115.429541\n1\nFalse\n\n\n1\n130.756135\n0\nFalse\n\n\n2\n113.958007\n1\nFalse\n\n\n3\n122.035513\n0\nFalse\n\n\n4\n114.782158\n1\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Seattle vs San Francisco average checkout time over 100 days.\n\n\n\n\nFrom Figure 4.1, we see that SEA and SF follow similar trends up till time of intervention. When the scanner guns are implemented, there is a sharp decrease in average check-out time with a continual downward trend. This is a great indication that our program provided significant decrease to CT. To get a numerical value from this graph, we can extract it manually by performing the above calculations using the data or by fitting a OLS model (they both provide the same result. Below we provide both results.\n\n\nTrue DiD Estimate: -27.421232456603718\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n130.4273\n0.607\n214.823\n0.000\n129.230\n131.625\n\n\npost_int[T.True]\n6.8350\n0.867\n7.880\n0.000\n5.125\n8.546\n\n\nsea\n-12.1969\n0.859\n-14.205\n0.000\n-13.890\n-10.504\n\n\nsea:post_int[T.True]\n-27.4212\n1.227\n-22.355\n0.000\n-29.840\n-25.002\n\n\n\n\n\nThe added value we gain from fitting the OLS is that we gain SE estimates and subsequent CI estimates. According to our model, scanner guns provide an on average decrease of about 27 seconds. Recalling what we calculated in the operations section, our new CT would be approximately 93 seconds. This means our new TPR would be approximately 2.58 customers per minute (a 29% increase in TPR). This results in a change in system utilization from 95% to about 74%. From here, we calculate our average queue length to be 1.36 customers (a 92% decrease in average queue length!) which then results in average wait time in the queue to be about 43 seconds (or .715 minutes, also a 92% decrease!).\nAccording to our models, the scanner guns are a huge success and represent a major opportunity for MW to increase customer satisfaction through higher operational efficiency. While we did derive useful causal effects from this experiment, MW should obviously consider other factors like feasability of company wide adoption, cost and maintenance of the equipment, and potential other factors like increases in theft from not scanning all items."
  },
  {
    "objectID": "posts/grocery_ops/diff_diff.html#experimental-design-2-synthetic-control",
    "href": "posts/grocery_ops/diff_diff.html#experimental-design-2-synthetic-control",
    "title": "Do Scanner Guns Decrease Checkout Time? An Analysis Using Diff-in-Diff and SCM",
    "section": "Experimental Design 2: Synthetic Control",
    "text": "Experimental Design 2: Synthetic Control\nAs explained in the DiD section, one of the key assumtions behind the DiD model is the parallel trends assumption. In order for our model to be valid, we need to find an adequate enough control unit that follows a close enough parallel trend, along with the other usual assumptions of causality. This assumption is regularly broken in the real world as it is difficult to find a control unit that satisfies the model assumptions.\nInstead of attempting to find a single control unit to compare our control to, we can “create” our own control by using synthetic control methods[6]. We create a synthetic control by taking a weighted average of all our possible controls (i.e. our donor pool) that best mimics the pre-intervention behavior of our treatment unit. An popular example of this is also found in [7] where the authors used SCM to model the effect of Califronia’s tobacco tax policy.\nIn our example, we would take a weighted average of our other 10 stores to form a synthetic control to compare against Seattle. Let’s quickly walk through what the math looks like in SCM.\nSCM attempts to estimate the counterfactual \\(Y_{t}^{N}\\), that is, the response of our treatment unit had the intervention never happened (i.e. the response of Seattle had it never received scanner guns). SCM estimates this counterfactual through a weighted average across the responses of the donor pool, as shown in Equation 4.3.\n\\[\nY_{t}^{N} \\approx \\sum_{j=2}^{J+1} w_{j} Y_{jt}\n\\tag{4.3}\\]\nThe weights are calculated by finding the minimum distance between \\(X_{1}\\) (the pre-intervention characteristics of the treated unit) and \\(X_{0}W\\) (the pre-intervention characteristics of a control unit). This minimization is subject to two constraints: all weights must be greater than or equal to 0 and all weights must sum to 1. Once these weights are calculated, they are applied to the post-intervention responses of the control units. The distance between the treated unit and the synthetic control is then the estimated ATT.\nIn our analysis, we used pysyncon[8] to create our synthetic control. After preparing our data for it, we obtained the following weights.\n\n\n1     0.028\n2     0.000\n3     0.000\n4     0.000\n5     0.297\n6     0.000\n7     0.000\n8     0.525\n9     0.149\n10    0.000\nName: weights, dtype: float64\n\n\nAs we can see, the closest city is city 8, followed by city 5, 9, and 1. The rest of the cities are not included in the weighted average. The plot of our synthetic control vs treated is found below in Figure 4.2.\n\n\n\n\n\nFigure 4.2: Seattle vs Synthetic control over time.\n\n\n\n\nOur SC appears to follow the trajectory and variation of Seattle fairly well (in a real SCM, we’d want it better but this suffices for this analysis). As such, when the intervention occurs, we can estimate the ATT via the distance between these two units. The distance overtime between these two are shown in Figure 4.3. The ATT immediately follows below it.\n\n\n\n\n\nFigure 4.3: Gap plot for Seattle vs Synthetic control.\n\n\n\n\n\n\n{'att': -20.67706963332358, 'se': 0.8177388666624705}\n\n\nFrom both Figure 4.2 and Figure 4.3, as well as the estimated ATT, the scanner guns appear to be a viable program for MW. Since we covered the effects on the operational efficiecny of this program in the previous experiment, we won’t go through it as detailed here. As mentioned earlier and as noticed in Figure 4.3, we’d want to aim for a better SC to avoid the huge spikes in distance from the treated unit pre-intervention."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html",
    "href": "posts/kind_alg/kindred_match.html",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "",
    "text": "In this post, we introduce Kindred, a new way to travel [1]. We first espouse the company mission, vision, and values through the lens of business strategy. We continue this discussion by moving into the more technical details of how the platform operates via market design. This market design is then used to illustrate a prototype dispatch algorithm that Kindred could use to implement their strategies. We conclude by discussing potential next steps for the company."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#rule-1-host-to-travel",
    "href": "posts/kind_alg/kindred_match.html#rule-1-host-to-travel",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Rule #1: Host to Travel",
    "text": "Rule #1: Host to Travel\nNow that the objective is established and a rule defined for adding members to the market, we can outline the rules for players within that market.\nFirst, to travel on Kindred, members must be willing to either host other members via a home swap or exchange accumulated credits for stays. This means a traveling member can contribute both demand and supply to the market (via home swap) or just demand (via credits).\nHowever, to travel using credits, a member must have hosted at least the equivalent number of nights they wish to stay at their destination. Crucially, credits cannot be purchased or exchanged between members; they are granted by the platform as a reward for active participation in the network.\nThese rules establish the market’s fundamental supply and demand mechanics. While credits offer deferred reciprocity, allowing members to leverage past hosting efforts for future travel and significantly reducing friction, they introduce a tradeoff: we aren’t guaranteed a unit of supply for every unit of demand as we would be in a pure barter system. Nonetheless, since our goal is to ensure guests feel at home by minimizing travel friction, the credit system, alongside the barter system, reduces barriers to travel while still incentivizing members to host in order to travel."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#rule-2-guest-control",
    "href": "posts/kind_alg/kindred_match.html#rule-2-guest-control",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Rule #2: Guest Control",
    "text": "Rule #2: Guest Control\nThe second rule within our market ensures members retain 100% control over who stays in their home. Even though the network is vetted upon application, members still have the power to approve or deny requests for home swaps or credit stays. This rule is specifically designed to promote supply. By giving members the final say on guests, they’re more likely to feel comfortable and willing to list their homes.\nWhile members have the power to approve or deny requests, they are still motivated to host guests due to rule #1. The more guests a member hosts, the more credits they earn and the more they can travel without a perfect house-swap match.\nThe tradeoff with this rule is increased matching complexity. While we can facilitate matches between members seeking similar location swaps, the ultimate power of approval rests with the members. Therefore, our matching service must provide each member with relevant information to guide their decision-making."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#rule-3-cost-allocation",
    "href": "posts/kind_alg/kindred_match.html#rule-3-cost-allocation",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Rule #3: Cost Allocation",
    "text": "Rule #3: Cost Allocation\nThe final rule we cover addresses how costs are allocated within the network. Costs are determined by the two travel propositions: a direct home swap with another member or using accrued credits to stay at a Kindred member residence. For a direct home swap, each member involved pays the cleaning fees for the location they are staying at (both before and after their visit), as well as a platform fee. In the credit exchange scenario, members must pay one credit per night of their stay, in addition to the cleaning and platform fees.\nThis rule further incentivizes members to host, as they don’t have to worry about the hassle of pre- or post-guest cleaning. Furthermore, members can expect the same seamless experience when traveling, knowing their destination will be cleaned before and after their stay. Lastly, this rule provides a transparent fee structure, ensuring members understand the costs associated with both travel options on the network."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#key-assumptions",
    "href": "posts/kind_alg/kindred_match.html#key-assumptions",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Key Assumptions",
    "text": "Key Assumptions\nThe rules of our marketplace are implemented with key assumptions in mind. These are enumerated below.\n\nKindred members desire to increase their travel by entering a network with clear expectations.\nKindred members share their true travel preferences.\nKindred members want to retain control over who stays at their residence.\nMembers will increase their trust in the platform with continued use.\n\nIn short, the assumptions state that we designed the market to work for members who operate under these assumptions. For example, we don’t expect someone to search for home-swap opportunities in NYC if they truly don’t want to go to NYC. Essentially, we expect those who want to use Kindred will desire to use the platform to maximize their travel utility within the parameters we set in this market."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#algorithmic-matching-via-gale-shapley",
    "href": "posts/kind_alg/kindred_match.html#algorithmic-matching-via-gale-shapley",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Algorithmic Matching via Gale-Shapley",
    "text": "Algorithmic Matching via Gale-Shapley\nThe process of finding matches within our market design can be greatly enhanced via game theory [4]. In our network, we have agents whom we assume are rational and possess weighted preferences. They strategically share this information on the platform to find their ideal matches. Once they find a suitable match, they move forward with their travel plans. This dynamic is precisely what game theory studies.\nIn our network, as illustrated in Figure 4.2, we can encounter overlapping scenarios. For instance, a user desiring a 21-day stay might match with one member hosting for 15 days and another for 6. Simultaneously, they might be willing to host two members themselves during that period – one for 14 days and another for 7. This type of overlap is characteristic of two-sided matching markets [5]. The algorithm commonly used to address such problems is known as the Gale-Shapley algorithm [5].\nThe Gale-Shapley algorithm is used to find stable matches among agents. This means finding matches that sufficiently address their needs, to the point where they wouldn’t gain additional utility by seeking a different match. While this algorithm is primarily applied in scenarios with distinct sets of proposers and receivers, we can extend it to our situation where agents act as both proposers and receivers."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#calculating-similarity",
    "href": "posts/kind_alg/kindred_match.html#calculating-similarity",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Calculating Similarity",
    "text": "Calculating Similarity\nThe first step in the algorithm is to calculate similarity scores between a user’s search criteria and our set of listings. For simplicity in this prototype, let’s assume a user can only search or request listings based on: location, duration, and number of bedrooms. We need to identify which listings in our dataset best correspond with the user’s search. Let \\(L\\) represent the set of all listings in our Kindred network. The initial step for calculating similarity involves filtering \\(L\\) to create a subset containing only listings within the specified location. (Note: We assume all listings are properly geotagged, accurately representing their respective cities.)\nThe second step is to calculate the similarity in date availability. To do this, we can use the Jaccard Similarity [8] measure to get the similarity between date ranges. This is illustrated in Equation 5.1.\n\\[\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\\tag{5.1}\\]\nThe numerator represents the intersection of our date overlap—that is, the number of days our date ranges coincide. The denominator is the union of our date ranges, covering the period from the earliest start date to the latest end date. A perfect match would yield a similarity score of 1, while any less-than-perfect match would naturally result in a lower score.\nThe last step is to calculate the similarity in number of bedrooms. A simple measure for this would be absolute difference, as shown in Equation 5.2.\n\\[\nS(A, B) = 1 - \\frac{|A - B|}{max - min}\n\\tag{5.2}\\]\nEquation 5.2 measures the difference between a perfect match (1) and the absolute difference between our search request and a listing’s offering, all normalized by a maximum range.\nOur three similarity scores are then combined into a weighted sum function to calculate an overall similarity score. This is shown in Equation 5.3.\n\\[\n\\text{Total Similarity Score} = w^{T}X\n\\tag{5.3}\\]\n\\(X\\) is our search criteria vector and \\(w^{T}\\) is our weight vector. This weight vector corresponds to the priority we place on the search criteria. For example, in our criteria, \\(w^{T}\\) might be \\([.7, .3]\\). This would mean that we put the most weight on our date overlap similarity score, followed by number of bedroom similarity. These scores are then ranked in descending order."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#weighting-other-factors",
    "href": "posts/kind_alg/kindred_match.html#weighting-other-factors",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Weighting Other Factors",
    "text": "Weighting Other Factors\nAfter calculating listing similarities, we also need to incorporate other metrics that enhance the user’s finding experience. For this prototype, we focus on three key metrics: queue length (the number of requests for a current listing within a given date range), host time to response, and listing rating. We prioritize these metrics to address supply-demand balance (ensuring listings get booked), connect users with responsive hosts, and pair them with listings that have provided positive experiences in the past. This provides an overall listing score shown in Equation 5.4.\n\\[\n\\text{Listing Score} = w^{T}X\n\\tag{5.4}\\]\nJust as in Equation 5.3, we use a weighted sum in Equation 5.4 to calculate the listing score. This listing score incorporates both the similarity score from Equation 5.3 and the other core experience metrics we explained earlier in this section. Once all listing scores are calculated, we rank these in descending order and present them to the user."
  },
  {
    "objectID": "posts/kind_alg/kindred_match.html#connection-to-market-design",
    "href": "posts/kind_alg/kindred_match.html#connection-to-market-design",
    "title": "There and Back Again: Optimizing Home-Swapping on Kindred",
    "section": "Connection to Market Design",
    "text": "Connection to Market Design\nOverall, while our algorithm is a simple prototype, it captures the essence of how we designed our market. It encourages users to accurately share their preferences (such as location and travel dates) while simultaneously encouraging hosts to do the same. Through this mutual information sharing, we rank a searcher’s preferences against those of hosts. We then return the top option for a searcher, along with a list of other ranked choices. Using these core principles, we can eventually extend this algorithm to become much more sophisticated and capable of handling matching at scale for a platform like Kindred."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html",
    "href": "posts/rdd_music/rdd_music.html",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "",
    "text": "A/B testing is the usual standard for understanding the effects of different treatments. A prominent use case is testing potential new product features. While A/B testing is great for clearly identifying the difference in important business metrics, it lacks the ability to purposely target specific customers. To accomodate for this change, companies can use quasi-experimental methods to extract a difference in experience measurement. In this post, we demonstrate regression discontinuity in the context of a music streaming company. Using this quasi-experimental method, we show that a company can offer an “exclusive” product experience while still holding the ability to extract causal effects."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html#search-theory",
    "href": "posts/rdd_music/rdd_music.html#search-theory",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "Search Theory",
    "text": "Search Theory\nThe AI DJ and other PlayMe features are practical applications of search theory [1]. From an economics standpoint, users are agents searching for “information” or “goods”—in this case, music. Search theory provides frameworks that help us reduce the friction, or cost, for our users to find what they want.\nOn PlayMe and other streaming services, it’s essential to maintain a large music library to ensure users have a wide range of choices. The primary challenge, then, is to design search features that minimize the time users spend trying to find their desired music.\nWhen users begin a session on PlayMe, they are typically in one of two mindsets: focused search or exploratory search [2]. To help those in the focused category, the search function allows them to find specific songs. For users in the exploratory group, the app generates curated playlists and song suggestions based on their listening history.\nBy lowering the cost of search, we gather more user data. This data then enhances our personalization algorithms and informs our overall product strategy. Furthermore, this process increases user loyalty by raising their switching costs. The more a user listens on PlayMe, the more the app learns their tastes, making it increasingly difficult for them to switch to a competitor and start their personalization journey over."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html#testing-a-premium-product",
    "href": "posts/rdd_music/rdd_music.html#testing-a-premium-product",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "Testing a Premium Product",
    "text": "Testing a Premium Product\nIn most cases, a company would test a new product through some user tests done in-house and then a larger rollout through A/B testing. However, PlayMe doesn’t want to perform just a normal A/B test. They want this new feature to an “exclusive reward” for their most loyal and active customers.\nPlayMe plans to offer the new AI DJ feature exclusively to users who average at least 30 minutes per session. This strategy serves a dual purpose. First, it rewards the app’s most active customers with early access. Second, it creates an exclusive “club” that may motivate other users to increase their session lengths in hopes of being included in future product tests.\nIn a standard A/B test, it’s fairly straightforward to measure the effects of distinct treatments since we have clear control and experimental groups. However, in this scenario, the product is rolled out exclusively to a group of users who already have high engagement metrics. The challenge, therefore, is to identify a suitable comparison group to ensure we can draw a valid inference about the success of the new AI DJ feature."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html#eda",
    "href": "posts/rdd_music/rdd_music.html#eda",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "EDA",
    "text": "EDA\nWe begin our EDA by modeling plotting the distribution of average session length in minutes (Note: This distribution only contains our more “active” listeners). This distribution is shown in Figure 3.1.\n\n\n\n\n\nFigure 3.1: Top listener average session length distribution in minutes\n\n\n\n\nThe distribution appears to be approximately normal and fairly centered around 30 minutes. This would imply that we are applying the treatment to those who are above average listeners in our core listener group. The next plot, Figure 3.2, shows the count of those who received treatment (DJ) vs no treatment.\n\n\n\n\n\nFigure 3.2: Barplot of count data for those untreated (0) and treated (1)\n\n\n\n\nThe barplot clearly indicates a fairly even split between treated vs untreated individuals. To illustrate if there is a jump in the data, we can view Figure 3.3.\n\n\n\n\n\nFigure 3.3: Scatterplot showing average session length by average time to first skip, split by treated and untreated\n\n\n\n\nWe can see a slight jump in the data at the 30 minute mark along the x axis. It appears that most of those who were treated have higher average time to skip than those who were untreated. However, we see that there are a few treated dots that follow the trend of the untreated.\nIn our scenario, this is a case of non-compliance. That is, even though some users qualified for the DJ, they chose not to use the DJ. We will have to account for this in our model later on.\nTo continue with the EDA, our data also includes primary device type of users. Figure 3.4 shows the distribution for each respective device.\n\n\n\n\n\nFigure 3.4: Distributions of average time to skip for phone users (top panel) and laptop users (bottom panel)\n\n\n\n\nBoth distributions appear to be bimodal (probably due to difference in treated vs untreated). Furthermore, the right peak on the phone distribution appears to have a higher mean than that of the laptop distritbution. There could be some interactive effect between having the DJ and the kind of device used. We can include this in our model specification.\nReturning for a moment to the potential non-compliance issue, we can verify this by checking the proportion of users who used the DJ, given they were assigned the DJ.\n\n\nuse_dj\n1    0.895582\n0    0.104418\nName: proportion, dtype: float64\n\n\nAbout 10% of users who were assigned the DJ did not use the DJ feature. This is a strong indicator of non-compliance which we should account for in our model."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html#model-specification",
    "href": "posts/rdd_music/rdd_music.html#model-specification",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "Model Specification",
    "text": "Model Specification\nThe goal of our model is to extract a valid causal estimate of our DJ feature on average time to skip. To do this, we need to specify a model that accounts for: the discontinuity between treated and untreated and the non-compliance of some in the treated group. The first part is just utilizing a linear regression model. The second part means we need to adjust this regression model to be a two-stage least squares (2SLS) [4] model.\nA 2SLS model contains two parts: the first-stage regression and the second-stage regression (hence, two-stage). The first-stage model is found in Equation 3.1.\n\\[\nD_{i} = \\gamma_{0} + \\gamma_{1}X_{i} + \\gamma_{2}T_{i} + \\sum_{j=3}^{m}\\gamma_{j}Z_{ji} + \\epsilon_{i}\n\\tag{3.1}\\]\nThe first-stage regression models the treatment status (1 for DJ, 0 for no DJ) for each individual \\(i\\). \\(X_{i}\\) is our running variable (average session length) for each individual \\(i\\). \\(T_{i}\\) is the instrument variable. We need the instrument variable to adjust for non-compliance since the treatment status is endogenous (i.e. correlated with the error term). To get an unbiased estimate, we use the instrument of assignment. That is, the treatment variable is a binary indicator of 1 if the user used the DJ and 0 otherwise. The instrument variable is a binary indicator of 1 if the user was assigned the DJ and 0 otherwise. The rest of the model are other covariates that help us to control for potential confounders (e.g. age, device type, etc.).\nThe second-stage of the 2SLS estimates our variable of interest. In this case, average time to first skip. The second-stage model is shown in Equation 3.2.\n\\[\nln(Y_{i}) = \\beta_{0} + \\beta_{1}X_{i} + \\beta_{2}\\hat{D}_{i} + \\sum_{j=3}^{m}\\beta_{j}Z_{ji} + u_{i}\n\\tag{3.2}\\]\nIn our model, we perform a log transformation to understand the percent change our treatment yields compared to the control group. The only real difference between Equation 3.1 and Equation 3.2 is \\(\\hat{D}\\). This is the predicted treatment status from Equation 3.1. Using this model, we can calculate the local average treatment effect (LATE), since we only want to measure the effect between those who complied with the treatment (i.e. those who actually used the DJ and were assigned it vs those who did not use the DJ and were not assigned it)."
  },
  {
    "objectID": "posts/rdd_music/rdd_music.html#model-results",
    "href": "posts/rdd_music/rdd_music.html#model-results",
    "title": "DJ RD: Using regression discontinuity to measure music personalization",
    "section": "Model Results",
    "text": "Model Results\nThe key parameter of interest is the treatment parameter, use_dj. Based on our EDA, we should expect a positive effect of DJ. Another parameter of interest would be the interaction term between use_dj and device_laptop to see if there is a difference of DJ effect by device type (Note: Mobile device is the baseline device in the model). Below is the output from our model.\n\n\n\nParameter Estimates\n\n\n\nParameter\nStd. Err.\nT-stat\nP-value\nLower CI\nUpper CI\n\n\nIntercept\n1.2795\n0.0997\n12.830\n0.0000\n1.0841\n1.4750\n\n\navg_sess\n0.0087\n0.0025\n3.5188\n0.0004\n0.0039\n0.0136\n\n\nage\n-0.0031\n0.0030\n-1.0469\n0.2951\n-0.0090\n0.0027\n\n\ngender_f\n-0.0116\n0.0129\n-0.9007\n0.3677\n-0.0368\n0.0136\n\n\nmain_device_laptop\n-0.2214\n0.0206\n-10.737\n0.0000\n-0.2618\n-0.1810\n\n\nuse_term_device\n-0.1514\n0.0256\n-5.9048\n0.0000\n-0.2016\n-0.1011\n\n\nuse_dj\n0.7834\n0.0336\n23.300\n0.0000\n0.7175\n0.8493\n\n\n\n\n\nAt face value, the use_dj parameter shows a significant result. On average, users who used the AI DJ saw a log increase in average time to first skip of 0.7834. Additionally, those who used a laptop instead of a mobile device with the DJ saw a decrease in the DJ effect. That is, the DJ effect for laptop users was a log increase of ~0.63. To understand these values on a percentage scale, we can transform them by \\((e^{X} - 1) * 100\\). Below is the tranformation for use_dj for mobile and use_dj for laptop.\n\n\nuse_dj: 118.9002474395752\nuse_term_device: 88.1520550200446\n\n\nAccording to our model, those who used the DJ saw a 118% increase in average time to first skip. That is a huge improvement! This effect represents the average percent increase in users who use mobile as their primary device (along with other covariates but won’t mention here). To calculate the effect of the treatment for laptop users, we combine the baseline effect with the interaction term in the transformation formula. This yields a 88% increase in average time to first skip for laptop users (still a great increase!).\nWhile these estimates are exciting, we would want to verify that the model passes the standard assumptions for regression discontinuity. We won’t go into too much detail in this post on how to verify, but readers should be aware of key assumptions like continuity of the conditional mean, no manipulations of the running variable, etc."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html",
    "href": "posts/stat_685_proj/final_project.html",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "",
    "text": "Forecasting enrollment for individual classes is important for departments and colleges at universities. It allows them to properly prepare adequate resources for students, such as: hiring the right number of TAs, offering the correct number of sections for classes, and assigning classes to the proper lecture halls. Needless to say, forecasting accurately the number of students signing up for a particular class is of high interest and need. Additionally, if possible, understanding the driving factors behind why certain classes are more popular or which classes are the most variable semester-to-semester is of particular interest.\nWe demonstrate in this post the power of Bayesian forecasting, utilizing a two-part model. The first part forecasts the percent change in enrollment for the upcoming semester compared to the previous semester. The second part forecasts the percent change in percent share of total enrollment at Texas A&M University."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#feature-engineering-and-data-preprocessing",
    "href": "posts/stat_685_proj/final_project.html#feature-engineering-and-data-preprocessing",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "Feature Engineering and Data Preprocessing",
    "text": "Feature Engineering and Data Preprocessing\nNow that we are somewhat familiar with the data we are working with and the possibilities of how the data can be modeled, we begin compiling the data into one large dataframe. Below is a beginning result of our mergers between dataframes.\n\n\n\n\n\n\n\n\n\nTERM\nCollege\nSubject\nCourse_Number\nis_fall\ncount_students\ncount_freshmen\n\n\n\n\n950\n20213.0\nEN\nENGR\n102.0\nTrue\n4549\n4549\n\n\n2475\n20221.0\nEN\nENGR\n102.0\nFalse\n278\n81\n\n\n4351\n20223.0\nEN\nENGR\n102.0\nTrue\n4993\n4945\n\n\n6583\n20233.0\nEN\nENGR\n102.0\nTrue\n4925\n4869\n\n\n\n\n\n\n\nFrom our sample taken from our newly merged dataframe, we have successfully important data points down to a class and term specific level. We will continue to modify this by adding and removing variables as we see fit. Please refer to the complete code found in the github repo for full information on how the complete data cleaning process was completed."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#model-1-overall-pct_change-in-enrollment",
    "href": "posts/stat_685_proj/final_project.html#model-1-overall-pct_change-in-enrollment",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "Model 1: Overall pct_change in Enrollment",
    "text": "Model 1: Overall pct_change in Enrollment\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n        mu      0.02      0.01      0.02      0.00      0.04    682.95      1.00\n     sigma      0.01      0.01      0.01      0.00      0.03    596.88      1.00\n\nNumber of divergences: 62\n\n\nTraining our model on just the percent change between 2021-2022 fall terms, we see our model has shifted the mean of our parameter \\(\\mu\\) from 0.03 to 0.02. Additionally, our sd shrunk as well from 0.02 to 0.01. Our r_hat values of 1.00 and high n_eff scores are good indicators that our model achieved stationarity and good mixing. To continue to confirm that our model has good MCMC diagnostics, we view the trace plots for each of our parameters of interest (see Figure 4.1).\n\n\n\n\n\nFigure 4.1: Trace plots for mu and sigma\n\n\n\n\nFigure 4.1 shows that our sampler mixed well and converged well, as both plots don’t appear to get stuck in any one particular region for too long.\nFigure 4.2 below shows how our prior has been updated using the MAP estimates from our sampler. We see visually what we noticed in the table above, that our posterior became more peaked and shifted left about the x axis.\n\n\n\n\n\nFigure 4.2: Prior and posterior distributions based on MAP values\n\n\n\n\nWhile Figure 4.2 is informative in showing how the MAP values update our prior, the power of Bayesian analysis comes from viewing the entire distribution of each parameter of interest and subsequently using those distributions to provide a predictive distribution, or posterior predictive distribution from which we can gather inference and make forecasts. Below in Figure 4.3 we see the complete distributions for each parameter of interest.\n\n\n\n\n\nFigure 4.3: Posterior distributions of mu and sigma\n\n\n\n\nFigure 4.3 shows that our mu distribution still appears to be approximately normal, centered around 0.02. Our posterior of sigma is less peaked (almost bimodal) but is generally centered around 0.01. Our mu distribution shows that while it is most likely that we have an increase of student enrollment around 2% for this upcoming year, it is within reasonable probability that we could see no increase (eg 0% change). We can use 95% credible intervals to demonstrate what plausible values are within 95% probability for each parameter or interest.\n\n\n95% CI for mu: (-0.001442579022841527, 0.046437517087906624)\n\n\n\n\n95% CI for sigma: (0.0015739109367132187, 0.04049663208425045)\n\n\nOur 95% CI for mu shows that within our 95% probability range, we could possibly see a decrease in student enrollment (be it a small one), but that the majority of our 95% CI includes positive values, meaning we would more than likely see some kind of increase for the next fall term."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#posterior-predictive-distribution-change-in-enrollment",
    "href": "posts/stat_685_proj/final_project.html#posterior-predictive-distribution-change-in-enrollment",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "Posterior Predictive Distribution (% Change in Enrollment)",
    "text": "Posterior Predictive Distribution (% Change in Enrollment)\nAs mentioned previously, we can now make predictions utilizing our posterior distributions. This is done by integrating out all of our parameters of interest (see Equation 4.1).\n\\[\np(\\overset{\\sim}{y}|y_{1}...y_{n}) = \\int_{\\Theta} p(\\overset{\\sim}{y}|\\theta)p(\\theta|y)\n\\tag{4.1}\\]\nWhat Equation 4.1 says is that in order for us to derive the posterior predictive distribution, integrate across all possible parameters of interest (in our case, mu and sigma) to derive a single distribution, conditioned upon our observed data. Our resulting posterior predictive distribution is found in Figure 4.4.\n\n\n\n\n\nFigure 4.4: Posterior predictive distribution for predicting percent change in enrollment\n\n\n\n\n\n\nMean of PPD: 0.019076179713010788\n\n\n\n\n95% CI of PPD: (-0.023650801740586758 , 0.07064365874975921)\n\n\nOur posterior predictive distribution is centered around 2% still but appears to be a wider distribution. This is because of our wider interval with our sigma distribution. Nonetheless, it is still an approximately normal distribution centered around 2% with corresponding 95% CI (or PI in this case) stating that with 95% probability the percent change in enrollment for the upcoming fall term will be between -2% and 7%. While this interval may be wider than desired, for utilizing only one training point of data and our weakly informative prior, it is a sufficient resource in predicting our fall enrollment for TAMU.\nTo make specific predictions using this distribution, we can utilize the MAP for a point estimate and the 95% CI for a range of predictions. Those results are shown below.\n\n\nBayesian Prediction of Enrollment: 57346\nActual Enrollment: 58933\n\n\n\n\n95% Prediction Interval for Enrollment: (55098, 60158)\n\n\nAs we can see, our Bayesian prediction performed much better than our ARIMA model (granted it was a (0,0,0) model). Additionally, our 95% PI captures the actual value for fall 2023 enrollment at TAMU. As our model continues to receive more data, it will be able to shrink its PI range and have more precise predictions for our forecasts. Nonetheless, we have a viable first model at predicting enrollment at TAMU."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#more-exploration-for-enhanced-bayesian-modeling",
    "href": "posts/stat_685_proj/final_project.html#more-exploration-for-enhanced-bayesian-modeling",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "More exploration (For enhanced Bayesian Modeling)",
    "text": "More exploration (For enhanced Bayesian Modeling)\nWhile the above model successfully performed better than our “frequentist” approach, it still has not accomplished our ultimate goal of forecasting down to the classroom level. How can we utilize our first model as a viable resource to build a second model that can forecast down to the classroom level?\nThe question to explore now is “do we believe that the percent share of enrollment by class varies or is relatively constant?”. Essentially, if we discover that classes are generally consistent in how much percent of enrollment they take each fall term, then we can build a model that forecasts from our first model and utilizes class specific information to predict its percent share of enrollment. For example, suppose we have an intro level math class that for 3 fall terms had percent share of enrollment of: 2%, 3%, 4%. Utilizing our first model that predicts the entire university enrollment, we can generate a posterior predictive distribution that can predict the percent change in percent share of enrollment and multiply that change in value by the first predicted value (total university enrollment) to get a class specific enrollment value. Equation 4.2 shows the more pretty math behind this intuition.\n\\[\n\\text{class level enrollment} = (\\text{LY } \\% \\text{ enrollment} * (1 + \\% \\text{ change in enrollment})) * (\\text{forecasted enrollment})\n\\tag{4.2}\\]\nBelow is an example table of the data we will utilize to forecast percent share of total enrollment by class.\n\n\n\n\n\n\n\n\n\nSubject\nCourse_Number\nis_fall\nyear\ncount_students\npct_of_enroll\n\n\n\n\n815\nAERS\n101.0\nTrue\n2021\n250\n0.004513\n\n\n5784\nAERS\n101.0\nTrue\n2022\n293\n0.005211\n\n\n8394\nAERS\n101.0\nTrue\n2023\n339\n0.005752\n\n\n821\nAERS\n105.0\nTrue\n2021\n607\n0.010957\n\n\n5790\nAERS\n105.0\nTrue\n2022\n589\n0.010476\n\n\n\n\n\n\n\nWe can build out our formula from Equation 4.2 by creating a new column in our full dataframe to calculate the percent change in percent share of enrollment. Below is an example of this using an intro math class (MATH 140) at TAMU (see last column).\n\n\n\n\n\n\n\n\n\nCollege\nSubject\nCourse_Number\nis_fall\ncount_students\ncount_freshmen\ncount_sophomore\ncount_junior\ncount_senior\ntarget_fresh_adm\n...\ncon_over_target\ncon_over_count_fresh\nyear_adm\ncount\nyr_count\npct_share\npct_class_share\ntotal_enroll\npct_of_enroll\npct_enroll_yoy\n\n\n\n\n107\nAT\nMATH\n140.0\nTrue\n3594\n3190\n294\n72\n38\n2650\n...\n0.953962\n0.792476\n2021\n4158\n16207\n0.256556\n0.196829\n55400\n0.064874\nNaN\n\n\n5049\nAT\nMATH\n140.0\nTrue\n3552\n2975\n412\n114\n51\n2650\n...\n0.909057\n0.809748\n2022\n3924\n15956\n0.245926\n0.186450\n56222\n0.063178\n-0.026136\n\n\n7656\nAT\nMATH\n140.0\nTrue\n4392\n3744\n456\n139\n53\n2650\n...\n0.961887\n0.680823\n2023\n4076\n17415\n0.234051\n0.214987\n58933\n0.074525\n0.179606\n\n\n\n\n3 rows × 26 columns\n\n\n\nAnd another example of an intro aerospace course (AERS 101).\n\n\n\n\n\n\n\n\n\nCollege\nSubject\nCourse_Number\nis_fall\ncount_students\ncount_freshmen\ncount_sophomore\ncount_junior\ncount_senior\ntarget_fresh_adm\n...\ncon_over_target\ncon_over_count_fresh\nyear_adm\ncount\nyr_count\npct_share\npct_class_share\ntotal_enroll\npct_of_enroll\npct_enroll_yoy\n\n\n\n\n815\nEN\nAERS\n101.0\nTrue\n250\n243\n5\n1\n1\n3250\n...\n0.920000\n12.304527\n2021\n4871\n16207\n0.300549\n0.014994\n55400\n0.004513\nNaN\n\n\n5784\nEN\nAERS\n101.0\nTrue\n293\n286\n7\n0\n0\n3250\n...\n1.110154\n12.615385\n2022\n5240\n15956\n0.328403\n0.017924\n56222\n0.005211\n0.154865\n\n\n8394\nEN\nAERS\n101.0\nTrue\n339\n329\n9\n0\n1\n3250\n...\n1.052000\n10.392097\n2023\n5263\n17415\n0.302211\n0.018892\n58933\n0.005752\n0.103773\n\n\n\n\n3 rows × 26 columns\n\n\n\nAnother thing we notice about these classes is an obvious one: they’re different sizes! However, we know that they aren’t all unique in their own size. We can bucket these classes into arbitrary sizes (small, medium, large, etc.) to aid our model in identifying more specifically how certain class sizes vary on their percent change in percent share of enrollment. Below in Figure 4.5 shows a distribution of count of students for classes.\n\n\n\n\n\nFigure 4.5: Histogram of count of students in classes\n\n\n\n\nAs we can see, the great majority of class sizes are found below 1000 students. We will need to take this into account when deciding how to divide our class size lines. While there are different ways of performing this in a more analytical way (i.e. segmentation analysis), we will arbitrarily decide on these buckets based on our EDA (see github repo for full EDA code).\nOne plot that we will include in this post is found in Figure 4.6. This figure shows the distribution for percent change in percent share of enrollment for each of our arbitrarily selected class sizes.\n\n\n\n\n\nFigure 4.6: 2x4 grid of histograms showing the distribution of percent change in percent share of enrollment for each class size\n\n\n\n\nFigure 4.6 is telling in many ways. First, x-small classes are generally non-variable with a few large outliers (this also is acceptable because a x-small class can double in size fairly easily). Small, medium, and large classes are approximately normal in their distribution because these are the class sizes with the most observations (kind of like a fun way of seeing the CLT). All other larger classes appear almost uniform in their distributions due to their small respective sample sizes."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#model-2-bayesian-hierarchical-model-class-size",
    "href": "posts/stat_685_proj/final_project.html#model-2-bayesian-hierarchical-model-class-size",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "Model 2: Bayesian Hierarchical Model (class size)",
    "text": "Model 2: Bayesian Hierarchical Model (class size)\nAs noted in some of the EDA above, the effects of each class size vary. While they are different, they aren’t completely different (as noted in Figure 4.6). To account for each class size effect individually, we utilize a hierarchical model to allow each class size to pull from a common distribution but to allow their respective effects to take update our prior beliefs. Below is the mathematical representation of our hierarchical model.\n\\[\n\\theta \\sim N(\\mu_{j}, \\sigma_{j})\n\\]\n\\[\n\\mu_{j} \\sim N(0, 0.2)\n\\]\n\\[\n\\sigma_{j} \\sim HN(0.2)\n\\]\nModeling it this way essentially allows our parameter of interest, \\(\\theta\\) to account for between-group variability based on the \\(\\mu_{j}\\) and \\(\\sigma_{j}\\) values while each respective \\(\\mu{j}\\) and \\(\\sigma_{j}\\) accounts for the within-group variability. This kind of flexibility will allow our model to predict down to specific classes based on the corresponding class size. After fitting our model to the data, we obtain the below results.\n\n\n\n                    mean       std    median      5.0%     95.0%     n_eff     r_hat\n   class_mu[0]      0.13      0.02      0.13      0.09      0.17   3864.53      1.00\n   class_mu[1]      0.12      0.02      0.12      0.09      0.16   4569.29      1.00\n   class_mu[2]      0.09      0.01      0.09      0.07      0.12   5660.38      1.00\n   class_mu[3]      0.11      0.02      0.11      0.07      0.15   5829.37      1.00\n   class_mu[4]      0.08      0.04      0.08      0.01      0.15   2455.60      1.00\n   class_mu[5]      0.02      0.02      0.02     -0.01      0.05   3977.78      1.00\n   class_mu[6]     -0.03      0.04     -0.03     -0.09      0.03   1414.55      1.00\n   class_mu[7]      0.01      0.05      0.01     -0.06      0.09   2553.92      1.00\nclass_sigma[0]      1.22      0.02      1.22      1.19      1.25   5456.28      1.00\nclass_sigma[1]      0.55      0.02      0.55      0.53      0.58   2663.55      1.00\nclass_sigma[2]      0.44      0.01      0.44      0.42      0.46   3291.40      1.00\nclass_sigma[3]      0.36      0.02      0.36      0.33      0.38   2453.18      1.00\nclass_sigma[4]      0.24      0.03      0.23      0.19      0.28   2273.90      1.00\nclass_sigma[5]      0.07      0.01      0.07      0.05      0.09   2460.03      1.00\nclass_sigma[6]      0.08      0.04      0.07      0.03      0.13    965.08      1.00\nclass_sigma[7]      0.13      0.05      0.12      0.07      0.19   2506.08      1.00\n            mu      0.05      0.11      0.05     -0.13      0.23   3127.97      1.00\n         sigma      0.37      0.06      0.36      0.27      0.46   2508.96      1.00\n\nNumber of divergences: 0\n\n\nclass_mu[0] represents the distribution for class size x-small while class_mu[7] represents class size giant. All other class sizes are between these ranges in ascending order (x-small to giant). Additionally, our n_eff and r_hat scores show that our sampler performed very well and that each distribution achieved sufficient stationarity and good mixing. To confirm our results using visuals, we can look at Figure 4.7.\n\n\n\n\n\nFigure 4.7: 2x8 trace plots of each class size. Top row are mu trace plots and bottom row are sigma trace plots\n\n\n\n\nFrom Figure 4.7, we see that each mu and sigma trace plot per class appear to have good mixing and acceptable stationarity. Now that we have checked our MCMC diagnostics, we can begin exploring the generated posteriors from our model. Figure 4.8 shows respective distributions for class size’s mu parameter.\n\n\n\n\n\nFigure 4.8: 2x4 grid of posterior distributions for each class size for parameter mu\n\n\n\n\nFigure 4.8 shows each class size mu parameter is approximately normally distributed. It is interesting to note where each distribution is centered. X-small is centered around .13 whereas larger classes like xxx-large are more centered around 0. It appears the there is a higher chance for smaller classes to have an increase in percent change in percent share of enrollment, whereas larger classes don’t vary as much. Larger classes seem to stay most constant term by term, which is not too surprising. A similar plot for class size sigma parameter is found in Figure 4.9.\n\n\n\n\n\nFigure 4.9: 2x4 grid of posterior distributions for each class size for parameter sigma\n\n\n\n\nOnce again, we notice that as the class size increases, the variability in sigma decreases.\n\nPosterior Predictive Distribution (Hierarchical Model)\nJust as we performed in the first model of this analysis, we will generate a posterior predictive distribution to make class specific predictions. Since we built a hierarchical model, we will generate 8 separate PPDs for each class size. Figure 4.10 shows the respective PPD for each class size.\n\n\n\n\n\nFigure 4.10: 2x4 grid of posterior predictive distributions for each class size\n\n\n\n\nFigure 4.10 shows just how variable the smaller classes can be. While the x-small through medium classes are all relatively centered around 0 (possibly due to scale of x axis), the variation in their predictions are fairly wide, especially in x-small classes. However, this should not be too problematic as an x-small class of size 10, even doubling in size, would only be 20. More than likely 1 professor and 1 TA would still be sufficient for this class.\nIn contrast, we see that our larger classes do not have nearly as much predicted variability (a repeated theme we have mentioned a lot). This is good to know since these classes require the most resources. Since they appear to be reliably consistent, we can “trust” more in the predictions and know they will not be too far off from the usual.\nFor example, take the class MATH 140 at TAMU. It is considered an intro level math course that many students must take in order to progress in their respective majors. It is considered a xxx-large class due to its popularity. Below is a point prediction for this class.\n\n\nBayes Prediction for % change in pct_enroll: -0.027\nTrue Value for % change in pct_enroll: 0.180\n\n\nThe model, due to its MAP, incorretly predicted the percent change in percent share of enrollment. However, point estimates in continuous predictions are rarely ever correct and thus require an interval prediction to ensure that our model at least captures the value within the interval. Below is a 95% PI for MATH 140.\n\n\n95% Prediction Interval for % change in pct_enroll: (-0.229, 0.179)\n\n\nAs we can see, our interval would include the actual value (even if it is on the farthest outside). This indicates that the true value that occured is in fact a possibel anomaly. That is, it is a plausible value but is just not very likely. Nonetheless, because we used a Bayesian approach, we can probabilistically say that it is a possible value in the range of 95% probability.\nIn order to properly predict the actual count of student enrollment by class, as mentioned previously, we use Equation 4.2. Below is the predicted number of students, both point estimate and 95% PI.\n\n\nPoint Estimate for MATH 140: 3527\nActual Value for MATH 140: 4392\n\n\n\n\n95% Prediction Interval for MATH 140: (2794, 4271)\n\n\nAs we can see, the 95% prediction interval is not far off from the actual value. The missing values would be trivial in such a large class forecast (at least for allocating university resources). While this test case has been informative, we should test another class to ensure our model is not always facing possible anomalies. To do this, we will look at MATH 308. Below is the point prediction for MATH 308.\n\n\nBayes Prediction for % change in pct_enroll: 0.105\nTrue Value for % change in pct_enroll: 0.051\n\n\nAs we can see, this time our model is over estimating the point estimate for percent change in percent share of enrollment. Our model predicts about a 10% increase in percent share of enrollment when the actual 2023 value was about 5%. While the point estimate is incorrect, it is not too far off in terms of practical prediction. Below is the corresponding 95% PI and resulting predictions from the point estimate and the PI.\n\n\n95% Prediction Interval for % change in pct_enroll: (-0.610, 0.832)\n\n\n\n\nPoint Estimate for MATH 308: 990\nActual Value for MATH 308: 967\n\n\n\n\n95% Prediction Interval for MATH 308: (349, 1640)\n\n\nAs mentioned above, our point estimate from a practical point of view is not too far from the actual value and would not have any major ramifications from a resource allocation point of view. Our 95% PI though, it quite wide. While it is slightly narrower than our 95% PI for MATH 140, it is still a very large range that would not be too useful for forecasting. Nonetheless, this model has proven to be very useful in predicting (especially from a practical point of view) the percent change in percent share of enrollment. Combined with the first model, we have created a system for forecasting class enrollment down to specific class levels."
  },
  {
    "objectID": "posts/stat_685_proj/final_project.html#model-3-bayesian-hierarchical-linear-model-college-and-class-size",
    "href": "posts/stat_685_proj/final_project.html#model-3-bayesian-hierarchical-linear-model-college-and-class-size",
    "title": "Forecasting Enrollment Size by Class at TAMU",
    "section": "Model 3: Bayesian Hierarchical Linear Model (college and class size)",
    "text": "Model 3: Bayesian Hierarchical Linear Model (college and class size)\nWhile our Bayesian hierarchical model proved to be a useful model in our system for forecasting class enrollment, we believe we can do better given the amount of information we have. Another useful piece of information that we can utilize is college. Specifying to which college each class size pertains can help our model distinguish more clearly the percent change in percent share of enrollment for a specific class. Our model will then be an additive model, where we have two covariates (college and class size) predicting our response (% change in % share of enrollment). Below is the mathematical form of our model.\n\\[\nY_{ijk} = X_{ijk}\\beta_{jk} + \\epsilon_{ijk}\n\\]\n\\[\n\\epsilon_{ijk} \\sim N(0, \\sigma^{2})\n\\]\n\\[\nY_{jk} \\sim MVN(X_{jk}\\beta_{jk}, \\sigma^{2}I)\n\\]\n\\[\n\\beta_{j} \\sim N(\\mu_{m},\\sigma_{m}^{2})  \n\\]\n\\[\n\\beta_{k} \\sim N(\\mu_{t}, \\sigma_{t}^{2})\n\\]\n\\[\n\\mu_{m} \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma_{m} \\sim HN(0.5)\n\\]\n\\[\n\\mu_{t} \\sim N(0, 0.5)\n\\]\n\\[\n\\sigma_{t} \\sim HN(0.5)\n\\]\nAfter fitting our model to the data and using a NUTS sampler w/ 500 warmup and 5000 samples, we achieve the below results.\n\n\n\n                     mean       std    median      5.0%     95.0%     n_eff     r_hat\n  class_beta[0]      0.03      0.04      0.03     -0.03      0.10   1825.25      1.00\n  class_beta[1]      0.05      0.04      0.04     -0.01      0.10    688.67      1.00\n  class_beta[2]      0.04      0.03      0.03     -0.01      0.07    752.82      1.00\n  class_beta[3]      0.03      0.02      0.03     -0.01      0.07   1153.19      1.00\n  class_beta[4]      0.03      0.03      0.03     -0.02      0.07   1733.09      1.00\n  class_beta[5]      0.02      0.03      0.02     -0.03      0.07   1545.40      1.00\n  class_beta[6]      0.02      0.03      0.02     -0.03      0.08   1703.07      1.00\n  class_beta[7]      0.02      0.03      0.02     -0.03      0.07   1596.52      1.00\n   class_int[0]      0.03      0.02      0.03     -0.00      0.07    517.34      1.00\n   class_int[1]      0.02      0.02      0.02     -0.01      0.05   1396.59      1.00\n   class_int[2]      0.01      0.02      0.01     -0.02      0.04   2576.21      1.00\n   class_int[3]      0.01      0.02      0.01     -0.02      0.04   2259.81      1.00\n   class_int[4]      0.01      0.02      0.01     -0.02      0.04   2348.81      1.00\n   class_int[5]      0.01      0.02      0.01     -0.02      0.04   2704.20      1.00\n   class_int[6]      0.01      0.02      0.01     -0.02      0.04   2207.97      1.00\n   class_int[7]      0.01      0.02      0.01     -0.02      0.04   1860.08      1.00\ncollege_beta[0]      0.02      0.01      0.02      0.01      0.03    711.33      1.00\ncollege_beta[1]      0.01      0.01      0.01     -0.01      0.03   1322.16      1.00\ncollege_beta[2]      0.00      0.01      0.00     -0.02      0.02    647.16      1.00\ncollege_beta[3]      0.01      0.02      0.01     -0.02      0.03   1262.88      1.00\ncollege_beta[4]      0.00      0.02      0.01     -0.03      0.04   1269.12      1.00\ncollege_beta[5]      0.00      0.03      0.01     -0.04      0.04   1430.99      1.00\ncollege_beta[6]      0.00      0.03      0.01     -0.04      0.05   1733.26      1.00\ncollege_beta[7]      0.00      0.03      0.01     -0.04      0.04   1146.67      1.00\ncollege_beta[8]      0.01      0.03      0.01     -0.03      0.05   2263.37      1.00\ncollege_beta[9]      0.01      0.03      0.01     -0.04      0.04   2195.16      1.00\n       mu_class      0.03      0.02      0.03     -0.01      0.07    844.81      1.00\n     mu_college      0.01      0.01      0.01     -0.02      0.03    915.74      1.00\n         mu_int      0.01      0.01      0.01     -0.00      0.03   1558.78      1.00\n          sigma      0.96      0.01      0.96      0.95      0.98   3696.71      1.00\n    sigma_class      0.03      0.02      0.02      0.00      0.06    456.18      1.00\n  sigma_college      0.02      0.02      0.01      0.00      0.04    407.22      1.00\n      sigma_int      0.01      0.01      0.01      0.00      0.03    348.40      1.00\n\nNumber of divergences: 131\n\n\nOur output for each beta appears to indicate that we have acheived stationarity and good mixing. To verify this visually, we can look at Figure 4.11.\n\n\n\n\n\nFigure 4.11: 2x4 grid of trace plots for each class size\n\n\n\n\nThe betas for each class size level appear to be well mixed and stationary. To verify the betas for each college, we can look at Figure 4.12.\n\n\n\n\n\nFigure 4.12: 2x4 grid of trace plots for each college\n\n\n\n\nJust as with Figure 4.11, we can see that each college appears to have reached appropriate stationarity and mixing. Now that we have verified that our sampler functioned well, we can begin investigating the posterior distributions. Figure 4.13 shows the posterior distributions of each class size and Figure 4.14 shows the posterior distributions for each college.\n\n\n\n\n\nFigure 4.13: Posterior distributions of each class size beta\n\n\n\n\n\n\n\n\n\nFigure 4.14: Posterior distributions of each college beta\n\n\n\n\nIt is interesting to note that in both Figure 4.13 and Figure 4.14, we see that most of the distributions are relatively centered around the point. The distinguishing feature for each distribution is the respective spread.\nJust as we performed for our first hierarchical model, we can perform predictions by generating a posterior predictive distribution (see Equation 4.1). We will first explore the predictions with this model for MATH 308 and compare it to our predictions from our first hierarchical model. The results are shown below.\n\n\nMean of pred_lm: 0.0403\n95% PI of pred_lm: (-0.0108, 0.0907)\nTrue value of pct_enroll_yoy: 0.0507\nPredicted Number of Students: 932\n95% PI of Number of Students: (886, 977)\nTrue Number of Students: 967\n\n\nThe very notable change in our prediction from our hierarchical linear model is that our PI range has shrunk significantly. Instead of the upper limit being close to .8, our upper limit of our 95% PI is approximately .09. That is a significant improvement! Our new hierarchical model also still contains the true value of percent change in percent share of enrollment. As we can see from above, our 95% PI for the predicted number of students is significantly better than before and would be of much better use practically for forecasting. We can perform this exercise again with predictions for MATH 140. Below are our results.\n\n\nMean of pred_lm: 0.0354\n95% PI of pred_lm: (-0.0483, 0.1073)\nTrue value of pct_enroll_yoy: 0.1796\nPredicted Number of Students: 3751\n95% PI of Number of Students: (3448, 4012)\nTrue Number of Students: 4392\n\n\nWe once again see a shrunken PI interval for this class. However, because the true percent change in percent enrollment was a large anomaly, our newly shrunken 95% PI does not capture our true percent change in percent share of enrollment. Nonetheless, the 95% PI can still be of practical use as it is not too far off from the true number of predicted students."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html",
    "href": "posts/stat_692_proj/hotel_analysis_final.html",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "",
    "text": "TA-MU Hotels is a small, privately owned hotel business. Currently, TA-MU Hotels owns two hotels: One in the heart of Manhattan, NYC and another in Cancun, Mexico. TA-MU Hotels is looking to grow its business and hired us (TheBayesianBandit LLC) to look into how their respective hotels are performing. Specifically, our goal is to generate more profit for TA-MU Hotels by identifying profitable opportunities from their hotel booking data."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#revenue-generation",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#revenue-generation",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Revenue Generation",
    "text": "Revenue Generation\nAs noted in the introduction, TA-MU Hotels currently has two operating hotels. Figure 3.1 shows the current revenue generation by TA-MU Hotels on a daily level. The top panel indicates the total revenue of both hotels while the bottom panel shows the amount of revenue generated by each hotel.\n\n\n\n\n\nFigure 3.1: Time series plot of daily revenue generation of TA-MU Hotels. Top panel shows combined revenue whereas bottom panel shows revenue split by hotel location.\n\n\n\n\nFigure 3.1 shows a general trend of increasing revenue for the company. There also appears to be indications of seasonality in the total company revenue as revenue slows during the winter months and rises again in the spring and summer. Viewing the revenue splits by hotel types, generally the city hotel generates more revenue throughout the year. The resort hotel appears to have large spikes in revenue during the holiday season (December-January). In general, revenue for TA-MU Hotels appears to be gradually increasing over time.\nWhile the city hotel generally brings in the most revenue per day, Figure 3.2 shows that the resort hotel during peak times has more money generated per guest. There appears to be a recurring trend of high spending guests between the months of July-September at the resort hotel.\n\n\n\n\n\nFigure 3.2: Time series plot of average revenue per guest on a daily scale.\n\n\n\n\nWhile Figure 3.1 and Figure 3.2 show the daily revenue generation, we were also curious to see if the amount of money spent by guests differed by their respective length of stay. Figure 3.3 breaks down the average amount of money spent per night by the length of stay for each booking.\n\n\n\n\n\nFigure 3.3: Box plot of average amount of revenue spent per night stay.\n\n\n\n\nFrom Figure 3.3, we see that there is no major difference between the average amount of money spent per night by the length of stay. Customers who only stay one night spend about the same on average per night as those who stay three nights. This result is again seen in Figure 3.4 when we take at each hotel’s average customer expenditure per night.\n\n\n\n\n\nFigure 3.4: Box plot of average amount of revenue spent per night stay by each hotel."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#client-demographics",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#client-demographics",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Client Demographics",
    "text": "Client Demographics\nIdentifying who are our clients generating revenue for our business is the first step in understanding how we strategize to generate more revenue. Figure 3.5 shows the daily average number of people per booking at each hotel.\n\n\n\n\n\nFigure 3.5: Time series plot of average number of people per booking, split by hotel type.\n\n\n\n\nIt appears that on average, the lines for both hotels hover mostly in the two people per booking area. There are a few highs and lows indicating that the average at times could be close to one guest per booking or three guests per booking. Table 3.1 goes into more detail by looking into specific demographic details and the percent of bookings each demographic represents.\n\n\nTable 3.1: Client Demographics\n\n\n\n\n\n\n\n\n\nHotel\nNumber of Adults\nKids in Booking\nCustomer Type\n% Bookings\n\n\n\n\nCity Hotel\n2\nNo\nTransient\n33%\n\n\nResort Hotel\n2\nNo\nTransient\n18%\n\n\nCity Hotel\n2\nNo\nTransient-Party\n10%\n\n\nCity Hotel\n1\nNo\nTransient\n9%\n\n\nResort Hotel\n2\nNo\nTransient-Party\n4%\n\n\n\n\nTable 3.1 shows the top five demographics based on percent of bookings. From the table, we see that our biggest client demographic is two adults of customer type transient (when the booking is not part of a group or contract and is not associated with another transient booking) and no kids in the booking. Both the city hotel and resort hotel have this demographic as their most popular client demographic, with over 50% of total company bookings from this demographic alone. Additionally, in 3rd and 5th place respectively are two adult parties as well but of transient party (booked with a connection to at least one other transient booking)."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#lost-revenue",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#lost-revenue",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Lost Revenue",
    "text": "Lost Revenue\nImportant Note: For the column “is_canceled”, it is a binary classification for canceled = 1 and not canceled = 0.\nNow that we have identified TA-MU Hotels’ current revenue generation trends and their primary clients, we will identify areas of improvement based on lost revenue. Lost revenue primarily comes from canceled bookings. We see in Figure 3.6 that the city hotel, while generating the most bookings overall, also has a larger proportion of canceled bookings than the resort hotel.\n\n\n\n\n\nFigure 3.6: Bar plot of number of not-canceled/canceled bookings broken up by hotel type.\n\n\n\n\nIn order to quantify the impact of these cancelations, Figure 3.7 shows the daily gain of non-canceled bookings vs the daily loss of canceled bookings for each hotel.\n\n\n\n\n\nFigure 3.7: Top panel shows time series plot of daily revenue gained vs daily revenue lost based on non-canceled/canceled bookings in city hotel. Bottom panel shows time series plot of daily revenue gained vs daily revenue lost based on non-canceled/canceled bookings in resort hotel.\n\n\n\n\nAs we can see in Figure 3.7, the city hotel has quite a few times where lost revenue overtakes revenue gained. Several times we see large spikes in canceled bookings and subsequently large spikes in lost revenue. In contrast, it appears the resort hotel rarely has any days where lost revenue overtakes revenue gained. To further dive into the details of all this lost revenue, we can view Table 3.2 to see how many cancelations occur from our top five client demographics.\n\n\nTable 3.2: Client Demographics w/ Cancelations\n\n\n\n\n\n\n\n\n\n\nHotel\nNumber of Adults\nKids in Booking\nCustomer Type\n% Bookings\n% Bookings Canceled\n\n\n\n\nCity Hotel\n2\nNo\nTransient\n33%\n48%\n\n\nResort Hotel\n2\nNo\nTransient\n18%\n33%\n\n\nCity Hotel\n2\nNo\nTransient-Party\n10%\n32%\n\n\nCity Hotel\n1\nNo\nTransient\n9%\n42%\n\n\nResort Hotel\n2\nNo\nTransient-Party\n4%\n22%\n\n\n\n\nTable 3.2 reveals a troubling finding for our top five client demographics. Our number one demographic currently has canceled about 48% of bookings! Our number two demographic, of the same type but at the resort hotel, also boasts a very high 33% canceled bookings. Identifying the reasons behind these cancelations are paramount in helping TA-MU Hotels increase their profitability."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#canceled-bookings-detail",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#canceled-bookings-detail",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Canceled Bookings Detail",
    "text": "Canceled Bookings Detail\nTo further investigate the reasons behind these cancelations for our top demographics, we first investigate the relationship between lead time (time from when booking first entered system to arrival date) and cancelations. Figure 3.8 shows the average lead time between non-canceled bookings and canceled bookings (split by hotel type).\n\n\n\n\n\nFigure 3.8: Bar plot of average lead time split by hotel type and booking status\n\n\n\n\nFrom Figure 3.8, we see that there is a large discrepancy between average lead time between non-canceled and canceled bookings. Particulary, in the city hotel, the canceled bookings nearly have double the lead time on average compared to non-canceled bookings. To further dive into the details of the relationship between canceled bookings and lead time, Figure 3.9 shows the average lead time broken down by how the booking was created (through what marketing channel) split by hotel type and booking status.\n\n\n\n\n\nFigure 3.9: Bar plot of average lead time split by hotel type, distribution channel, and booking status.\n\n\n\n\nThe barplots marked with 0s are bookings that were non-canceled and the barplots with 1s are bookings that were canceled (ie (City Hotel, 0) is average lead time for bookings that were not canceled). As we can see there are still large discrepancies between not canceled and canceled bookings, especially for the city hotel. The TA/TO distribution channel appears to generate the largest lead times for canceled bookings in both the city hotel and the resort hotel."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#target-market",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#target-market",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Target Market",
    "text": "Target Market\nAs noted in Table 3.2 and Table 3.1, over 50% of TA-MU bookings are generated by a single demographic: two adults, transient, no kids. To help TA-MU Hotels become more profitable, we propose that we priortize this demographic for the remainder of the report with the goal of minimizing the effects of canceled bookings and maximizing profits from this primary demographic.\nTo confirm our findings from the canceled bookings detail section, we run the same analyses performed in that section but just on our focused demographic. Figure 4.1 shows the average lead time split by hotel type for non-canceled and canceled bookings, focused on our primary demographic.\n\n\n\n\n\nFigure 4.1: Bar plot of average lead time split by hotel type and booking status.\n\n\n\n\nAs we can see in Figure 4.1, we confirm that the discrepancies between lead time of non-canceled and canceled bookings exist for our primary demographic. To get a further detailed view of where our primary demographic cancels their bookings, we plot in Figure 4.2 the percent of canceled bookings split by distribution channel and hotel type.\n\n\n\n\n\nFigure 4.2: Bar plot of % bookings canceled split by hotel type and marketin distribution channel.\n\n\n\n\nIt appears that for our primary demographic (two adults, transient, no kids in booking), the highest percentage of cancellations for the city hotel are bookings made through the TA/TO channel. For the resort hotel, the highest percentage of cancellations are bookings made through the corporate channel."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#data-setup",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#data-setup",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Data Setup",
    "text": "Data Setup\nWe begin the process of creating a good predictive model by ensuring that the data we feed the model is accurate and useful. To do this, we dropped irrelevant columns to our analysis and filter it to include only data pertaining to our primary demographic. Additionally, we create dummy variable columns in order to account for factors in our model (i.e. distribution channel). The result of our data cleaning and feature engineering is a change from raw data of 119390 rows of data and 37 columns to training data of 61024 rows and 24 columns of data.\nTo help our model more accurately predict cancelations, we use a standard scaler to standardize each column of data. This will help the model recognize each feature as a normally distributed variable. Furthermore, to help determine the predictive capability of our model, we use a train-test split of 75% training and 25% test data. This allocation is randomly done by the train_test_split function provided in scikit-learn."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#model-selection",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#model-selection",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Model Selection",
    "text": "Model Selection\nIn order to determine whether or not we should use the full dataset of 24 features for our model or if a subset of these features would be adequate, we perform forward stepwise selection using a logistic regression model. The results of our model selection procedure are found below.\n\n\nIndex(['lead_time', 'stays_in_weekend_nights', 'previous_cancellations',\n       'previous_bookings_not_canceled', 'booking_changes',\n       'required_car_parking_spaces', 'distribution_channel_TA/TO',\n       'reserved_room_type_E', 'deposit_type_Non Refund',\n       'deposit_type_Refundable', 'hotel_Resort Hotel'],\n      dtype='object')\n\n\nThese 11 features, according to forward stepwise selection, are the features that provide the greatest additional improvement to the model. To test this theory, we compared a logistic regression model with these 11 features against a logistic regression model with all 24 features. The results show that the models performed equally as well at predictive accuracy (both around 75%). We determine that using less features is more computationally efficient for similar predictive accuracy so we will use the 11 features that we found with the forward stepwise selection for the remainder of the iterative process of building a predictive model.\n\nAdditional Features\nAs noted previously in the report, there could be interactions between some of these features. For example, the effect that lead time has on the probability of a cancelation might change based on the kind of distribution channel the lead came from. Therefore, we will add two extra features to current subset of 11 features for our predictive model: interaction between distribution channel and lead time, and interaction between distribution channel and hotel type."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#model-comparisons",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#model-comparisons",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Model Comparisons",
    "text": "Model Comparisons\n\nLogistic Regression\nWe begin with logistic regression to see if this kind of model performs well at predicting cancelations. After fitting our training data to our logistic regression model, we obtainthe following results.\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nis_canceled\nNo. Observations:\n45768\n\n\nModel:\nGLM\nDf Residuals:\n45754\n\n\nModel Family:\nBinomial\nDf Model:\n13\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-21165.\n\n\nDate:\nThu, 25 Apr 2024\nDeviance:\n42330.\n\n\nTime:\n13:11:47\nPearson chi2:\n3.81e+07\n\n\nNo. Iterations:\n28\nPseudo R-squ. (CS):\n0.3573\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.5268\n3333.240\n0.001\n0.999\n-6530.503\n6535.557\n\n\nx1\n0.5459\n0.016\n33.679\n0.000\n0.514\n0.578\n\n\nx2\n-0.0389\n0.012\n-3.250\n0.001\n-0.062\n-0.015\n\n\nx3\n1.9948\n0.127\n15.751\n0.000\n1.747\n2.243\n\n\nx4\n-0.6542\n0.061\n-10.808\n0.000\n-0.773\n-0.536\n\n\nx5\n-0.1151\n0.013\n-8.943\n0.000\n-0.140\n-0.090\n\n\nx6\n-7.3338\n6598.810\n-0.001\n0.999\n-1.29e+04\n1.29e+04\n\n\nx7\n0.2570\n0.016\n16.150\n0.000\n0.226\n0.288\n\n\nx8\n0.0512\n0.012\n4.387\n0.000\n0.028\n0.074\n\n\nx9\n11.0061\n6281.574\n0.002\n0.999\n-1.23e+04\n1.23e+04\n\n\nx10\n0.4591\n6533.082\n7.03e-05\n1.000\n-1.28e+04\n1.28e+04\n\n\nx11\n-0.0209\n0.013\n-1.658\n0.097\n-0.046\n0.004\n\n\nx12\n0.0620\n0.019\n3.326\n0.001\n0.025\n0.099\n\n\nx13\n-0.0320\n0.013\n-2.467\n0.014\n-0.057\n-0.007\n\n\n\n\n\nFeatures x1, x3, x7, x8, and x12 (lead time, previous cancellations, distribution channel TA/TO, reserved room type E, interaction between lead time and distribution channel TA/TO) show an increase in the log-odds of a cancelation and appear to be statistically significant (alpha = 0.05). Conversely, x2, x4, x5, and x13 (stays in weekend nights, previous bookings not canceled, booking changes, and interaction between distribution channel TA/TO and hotel type) show a decrease in the log-odds of a cancelation.\nUsing this fitted logistic regression model, we can generate predictions and see how the model does at predicting cancelations using our test data. One metric to test the predictive capability of a classification type model is using a ROC curve and computing the AUC score from it. Figure 5.1 shows the corresponding ROC curve and AUC score.\n\n\n\n\n\nFigure 5.1: ROC curve for logistic regression model.\n\n\n\n\nThe ROC curve in Figure 5.1 shows that the model achieves an AUC score of about .82, indicating that the model does a pretty good job at distinguishing between a canceled booking and a non-canceled booking. Using this graph, we can extract the optimal threshold that minimizes our false positive rate while still achieveing a max true positive rate. Using this threshold, we can construct a confusion matrix to gain further inference on how the model performs on discovering cancelations. Figure 5.2 shows the resulting confusion matrix.\n\n\n\n\n\nFigure 5.2: Confusion matrix for logistic regression model.\n\n\n\n\nFrom the confusion matrix, we see that the logistic regression model does a very good job at identifying non-canceled bookings, but does not perform as well at identifying canceled bookings. However, while the performance does drop significantly between canceled and non-canceled booking classification, the majority of the time the model does predict the correct label for canceled bookings. To view more specific metrics from the confusion table, we can view Table 5.1.\n\n\nTable 5.1: Logistic Regression Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n55%\n\n\nSpecificity\n89%\n\n\nPPV\n80%\n\n\nNPV\n72%\n\n\n\n\nFrom Table 5.1, we see that our logistic regression model performed well in every category except sensitivity. Even then, the sensitivity score is an acceptable one since the majority of the time it correctly labels a cancelation.\n\n\nRandom Forest\nWe now compare a tree based method approach to logistic regression by using a random forest. As we performed above, we will fit a random forest with the same training data and gather predictive metrics (AUC score, confusion matrix metrics) by using the same test set as the logistic regression model. Figure 5.3 shows the ROC curve and corresponding AUC score for the random forest model.\n\n\n\n\n\nFigure 5.3: ROC curve and AUC score for random forest model.\n\n\n\n\nFrom Figure 5.3, we see that the random forest model yielded a worse AUC score than the logistic regression model. We will check the resulting confusion matrix metrics for further comparison. Figure 5.4 contains the confusion matrix and Table 5.2 shows the resulting metrics from the confusion matrix.\n\n\n\n\n\nFigure 5.4: Confusion matrix for random forest model.\n\n\n\n\n\n\nTable 5.2: Random Forest Model vs Logistic Regression Model\n\n\n\n\n(a) Random Forest Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n60%\n\n\nSpecificity\n86%\n\n\nPPV\n76%\n\n\nNPV\n74%\n\n\n\n\n\n\n(b) Logistic Regression Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n55%\n\n\nSpecificity\n89%\n\n\nPPV\n80%\n\n\nNPV\n72%\n\n\n\n\n\n\nFrom Table 5.2 (a) and comparing these values to Table 5.2 (b), the random forest model performed very similarily to the logistic regression model. The random forest model achieved a higher sensitivity score while dropping in the specificity score. Additionally, the random forest model dropped in PPV but gained in NPV.\n\n\nXGBoost\nSince the random forest model did not appear to beat out the performance of the logistic regression model, we will now perform a comparison between XGBoost classification and the logistic regression model. We will perform the same procedures as done with the logistic regression model and the random forest model. Figure 5.5 shows the resulting ROC curve and corresponding AUC score.\n\n\n\n\n\nFigure 5.5: ROC curve and AUC score for XGBoost model\n\n\n\n\nThe AUC score for the XGBoost model appears to be very similar to the random forest model, and consequently is much lower than the logistic regression model AUC. Figure 5.6 and Table 5.3 show the resulting confusion matrix and corresponding metrics.\n\n\n\n\n\nFigure 5.6: Confusion matrix for XGBoost model.\n\n\n\n\n\n\nTable 5.3: XGBoost vs Logistic Regression Model\n\n\n\n\n(a) XGBoost Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n50%\n\n\nSpecificity\n95%\n\n\nPPV\n88%\n\n\nNPV\n71%\n\n\n\n\n\n\n(b) Logistic Regression Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n55%\n\n\nSpecificity\n89%\n\n\nPPV\n80%\n\n\nNPV\n72%\n\n\n\n\n\n\nFrom Table 5.3, we see that XGBoost improves in specificity over the logistic regression model, but decreases at almost the same degree in sensitivity. There is also a large jump in PPV and a slight decline in NPV for XGBoost over logistic regression.\n\n\nBayesian Logistic Regression\nThus far we have attempted to compare the logistic regression model to more ensemble-esque methods. We now attempt to demonstrate a different approach to the logistic regression model by incorporating priors on each feature. That is, we will perform a bayesian logistic regression to compare to the normal logistic regression method.\nDue to our limited subject matter expertise in the hotel industry, we will use uninformative priors for each feature. We will assume that each feature is drawn from a normal distribution with mean = \\(\\mu\\) and standard deviation = \\(\\sigma\\). For each \\(\\mu\\), we will place a prior of a normal distribution with \\(\\mu\\) = 0 and \\(\\sigma\\) = 2. For each \\(\\sigma\\), we will place a prior of a inverse gamma distribution with \\(\\alpha\\) = 1 and \\(\\beta\\) = 1.\nFor our sampler, we will use a Hamiltonian Monte Carlo with Energy Conserving Sampling. This is chosen due to the size of our dataset and the lack of sufficient computational power to perform a more precise sampling with sampler such as the No-U Turn sampler. After fitting our bayesian logistic regression model, we obtain the following results in Figure 5.7.\n\n\n\n\n\nFigure 5.7: Posterior distributions for each feature.\n\n\n\n\nA benefit of performing a bayesian model is the viewable uncertainty around parameters of interest. In our case, we can see the distributions of each beta value from our model. For example, the change in log-odds on booking status by lead time is likely to be around .7, but can be anywhere between .65 and .75, according to our model. Additionally, our predictions can also have this kind of uncertainty around them. For example, below is information pertaining to a booking that ended up canceling.\n\n\nlead_time                         110.0\nstays_in_weekend_nights             0.0\nprevious_cancellations              0.0\nprevious_bookings_not_canceled      0.0\nbooking_changes                     0.0\nrequired_car_parking_spaces         0.0\ndistribution_channel_TA/TO          1.0\nreserved_room_type_E                0.0\ndeposit_type_Non Refund             0.0\ndeposit_type_Refundable             0.0\nhotel_Resort Hotel                  0.0\nName: 10000, dtype: float64\n\n\nWe see that there were 110 days between lead entered into system and arrival date, that they were staying in the city hotel (hotel_resort = 0) and that they booked throuh TA/TO. Using our bayesian model, we can view the uncertainty around this prediction, as shown in Figure 5.8.\n\n\n\n\n\nFigure 5.8: Posterior Predictive Distribution for the described canceled booking.\n\n\n\n\nFrom Figure 5.8, we see that the most likely value from the distribution is around .42 to .43. According to the distribution, the probability that somebody with this kind of booking information (from our primary demographic) cancels is around .41 to .44.\nWhile the world of bayesian modeling is fascinating and can be powerful, the goal remains the same of providing a model that could be of use to us in identifying where we need to double book rooms in order to maximize profit for TA-MU Hotels. Just like in the other sections, we will compare the AUC score and derived confusion matrix metrics. Figure 5.9 shows the ROC curve for our bayesian model.\n\n\n\n\n\nFigure 5.9: ROC curve and corresponding AUC score for bayesian logistic regression model.\n\n\n\n\nFrom Figure 5.9, we see that the AUC score is very similar to the original logistic regression model. Figure 5.10 shows the corresponding confusion matrix and Table 5.4 show the comparison between the bayesian logistic regression model and the regular logistic regression model.\n\n\n\n\n\nFigure 5.10: Confusion matrix for bayesian logistic regression model\n\n\n\n\n\n\nTable 5.4: Bayesian Logistic Regression Model vs Logistic Regression Model\n\n\n\n\n(a) Bayesian Logistic Regression Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n53%\n\n\nSpecificity\n92%\n\n\nPPV\n84%\n\n\nNPV\n72%\n\n\n\n\n\n\n(b) Logistic Regression Metrics\n\n\nMetric Name\nScore\n\n\n\n\nAccuracy\n75%\n\n\nSensitivity\n55%\n\n\nSpecificity\n89%\n\n\nPPV\n80%\n\n\nNPV\n72%\n\n\n\n\n\n\nFrom Table 5.4, we see that the two models are very similar. The bayesian model shows a slight increase in specificity and a small decrease in sensitivity. The bayesian model also shows an increase in PPV while keeping NPV the same as the regular logistic regression model."
  },
  {
    "objectID": "posts/stat_692_proj/hotel_analysis_final.html#model-decision",
    "href": "posts/stat_692_proj/hotel_analysis_final.html#model-decision",
    "title": "An Analysis of TA-MU Hotel Booking Data",
    "section": "Model Decision",
    "text": "Model Decision\nWhile all four models proved to be viable models worthy of use in our system, the two logistic regression models had higher AUC scores and were more balanced in the sensitivity-specificity trade-off. To decide which logistic regression model to use, we look at the confusion matrix metrics and AUC scores. Both models have similar confusion matrix metrics and AUC scores. However, the bayesian logistic regression model has a particular advantage over the regular logistic regression model due to its inherit ability at including uncertainty around parameter estimates and predictions. Therefore, we recommend that we continue with the bayesian logistic regression model as our initial cancelation predictive model.\nThe full model for our bayesian logistic regression model is shown in Equation 5.1\n\\[\n\\text{log}(\\frac{p(Y)}{1-p(Y)}) = \\beta_0 + \\beta_1x_1 ... + \\beta_{m}x_m\n\\]\n\\[\n\\text{Y}_1 ... \\text{Y}_n \\overset{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(p)\n\\]\n\\[\n\\beta \\sim N(\\mu, \\sigma)\n\\]\n\\[\n\\mu \\sim N(0, 2)\n\\]\n\\[\n\\sigma \\sim IG(1, 1)\n\\tag{5.1}\\]\nEquation 5.1 states that the log-odds of booking status (1 being cancelation and 0 being non-cancelation) is a linear function of \\(m\\) predictors (our 11 features and two interaction terms) weighted by \\(m+1\\) betas (13 for the features and one intercept beta). Our response variable \\(Y\\) is a collection of n number of i.i.d. Bernoulli random variables with probability \\(p\\). Our betas are assumed to be normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). All \\(\\mu\\) values have a normal prior distribution with mean = 0 and standard deviation = 2. All \\(\\sigma\\) values have an inverse gamma prior distribution with shape = 1 and scale = 1."
  },
  {
    "objectID": "posts/uplift_paper/uplift_paper.html",
    "href": "posts/uplift_paper/uplift_paper.html",
    "title": "Multiple Treatments in Marketing Messaging with Non-Compliance",
    "section": "",
    "text": "Abstract\nIn this post, we provide an implementation for the paper Uplift Modeling for Multiple Treatments with Cost Optimization [1]. We provide this implementation in the context of a marketing department testing which marketing campaign is most effective for a given loyalty tier. We continue with this example by extending the uplift framework to non-compliance situations. In this, we demonstrate the effectiveness of Causal Forests [2] in calculating conditional local average treatment effects (C-LATE). The full code for this implementation can be found at the github repo.\n\n\nIntroduction\nUplift modeling is a useful way of measuring the incremental impact of treatments compared to the control. Essentially, it measures how an individual will react to interventions. Equation 2.1 shows the math behind the calculation.\n\\[\n\\text{Uplift}_i = Y_i(1) - Y_i(0)\n\\tag{2.1}\\]\nEquation 2.1 utilizes the potential outcomes framework derived by Rubin in [3]. Since we can only ever observe the effects of treatment or control on an individual, the average treatment effect (ATE) Equation 2.2 is the usual metric calculated to understand treatment impact in an experiment. Furthermore, to understand how the treatment effect changes amongst subgroups/subpopulations, the conditional average treatment effect (CATE) Equation 2.3 is useful in identifying the heterogeneous treatment effect.\n\\[\n\\tau = E[Y_i(1) - Y_i(0)]\n\\tag{2.2}\\]\n\\[\n\\tau(x_i) = E[Y_i(1) - Y_i(0) | X]\n\\tag{2.3}\\]\nIn the context of marketing, the CATE is of great interest since it helps marketing organizations identify how treatments fair with different customer segments. For example, a marketing team might run a A/B test with some customers receiving a special promo (treatment) while others receive nothing (control) to see if there is a change in purchase behavior.\nUplift modeling follows the same basic assumptions of most causal inference models, namely: stable unit treatment value assumption (SUTVA), unconfoundness, and overlap. SUTVA means that there is no interference between treatment assignments and potential outcomes of individuals, as well as no unaccounted treatment versions in the study. Unconfoundness means the treatment assignment is independent of potential outcomes after conditioning on observed covariates (X) (see Equation 2.4). Overlap means that for every observed combination of covariates (X) in the data, there is a non-zero probability of being assigned to either treatment or control.\n\\[\n\\{Y(1), Y(0)\\} \\perp T \\mid X\n\\tag{2.4}\\]\n\n\nProblem Formulation\nWe pose the situation of a bakery business looking to increase ticket sizes via promotional campaigns. The bakery business is fairly simple with the following price-cost structure (see Table 3.1).\n\n\nTable 3.1: Bakery Price-Cost Structure\n\n\nItems\nPrice\nCost\n\n\n\n\nDonut\n$2.50\n$1.00\n\n\nPastry\n$3.00\n$1.50\n\n\nSandwich\n$4.50\n$2.00\n\n\nCoffee\n$1.00\n$0.20\n\n\nJuice\n$1.50\n$0.50\n\n\n\n\nCurrently, the business runs a loyalty program that is four-tiered with levels: none, bronze, silver, gold. Each tier repeats purchases at a different rate (from the synthetic data, this is defined as a Poisson r.v.). Furthermore, customers are not willing to go and purchase things from our bakery if they are too far away. We define the distance from the bakery as the time when promotions are sent out to customers (for those who receive no promotions, it is still marked at the same time as promotion receivers). Lastly, we assume that the company’s promotions have an effect on the ticket size of an individual transaction. Figure 3.1 illustrates our assumed flow of causality for this situation.\n\n\n\n\n\nFigure 3.1: Proposed DAG for our experiment on increasing individual transaction ticket size.\n\n\n\n\nWhile Figure 3.1 shows the flow of causality going through the number of items purchased to determine ticket size, we will focus our analysis and modeling on the ticket size and will ignore this step of the DAG. This is to more easily calculate Net Value of the transaction, where we will define this as \\(NV = R - MC\\), where \\(NV\\) is net value, \\(R\\) is revenue, and \\(MC\\) is marketing costs. From this, we propose that the optimal treatment program (i.e. optimal policy) can be defined in Equation 3.1.\n\\[\\pi^* \\in \\underset{\\pi}{\\operatorname{argmax}} \\{V(\\pi)\\} , \\quad \\pi^*(x) = \\mathbf{1} (\\{\\tau(x) &gt; 0\\}) \\tag{3.1}\\]\nEquation 3.1 states that the optimal policy \\(\\pi\\) is that which maximizes the some arbitrary value function \\(V(\\pi)\\). Although the above question is generally for single treatment experiments, it can be extended to multiple treatments.\nSpecifically, the business wants to experiment with 3 different treatments: free donut, free juice, and a 30% discount. Furthermore, they wish to know through which marketing channel is the treatment most effective, namely: social media, in-app notifications, and email. Therefore, we have 9 distinct treatments that we need to test against the control (i.e. receiving no promotion).\nAssummptions: We list here some crucial assumptions for this analysis demonstration. First, we operate under the 3 assumptions stated above (SUTVA, unconfoundness, and overlap). Second, we operate under the assumption that there is no spill over effect amongst participants. Though this is not an exhaustive list of assumptions from this experiment, these are sufficient for the educational purposes of this post.\n\n\nAnalysis\nWe begin our analysis by illustrating the average number of items purchased daily segmented by customer loyalty tier. Figure 4.1 shows such distributions.\n\n\n\n\n\nFigure 4.1: Average # of items purchased per day segmented by customer loyalty status.\n\n\n\n\nFigure 4.1 shows that on average, our none tier customers don’t purchase anything, bronze tier is around 1 item, silver tier around 2 items, and gold tier around 4. This is expected as we suppose that as loyalty increases so does amount purchased daily.\nWithin the experiment itself, we aimed to have 50% of customers be part of the control group with the other 50% in treatment split amongst the 9 treatmentes. The dataset below shows the splits in treatments.\n\n\nmarketing_channel  marketing_treatment\nemail              discount                222\n                   free_donut              212\n                   free_juice              225\nin-app             discount                211\n                   free_donut              208\n                   free_juice              222\nnone               none                   2047\nsocial             discount                213\n                   free_donut              204\n                   free_juice              236\nName: user_id, dtype: int64\n\n\nThe total observations in the dataset is 4000. Each treatment received around 200 customers and the control sat around 2000.\nTo get a good idea on whether the treatments had an impact or not, we can look at Figure 4.2.\n\n\n\n\n\nFigure 4.2: Average ticket between control and treated customers.\n\n\n\n\nFigure 4.2 clearly shows an increase in the average ticket size of treated customers. While we don’t know for sure if the difference is statistically significant, it is comforting to see that from the data, there is approximately a 50 cent difference. We can further view the effects of treatments by segmenting by tier and viewing the respective changes in ticket size based on the treatments received. This is shown in Figure 4.3.\n\n\n\n\n\nFigure 4.3: Average ticket size for each treatment, broken down by customer loyalty tier.\n\n\n\n\nFigure 4.3 tells an even better story than Figure 4.2. Here, we see that each loyalty tier had different responses to the treatments. The none tier customers had a huge spike in average ticket size when treated via social media with the 30% discount. Bronze tier customers seemed to favor the free donut the most via the app. Silver tier enjoyed any promotion via the app. Gold tier is a bit harder to identify due to the amount of variation in their data, but free donuts appear to be the most impactful for ticket size.\nWhile the increase in revenue is an important start in the uplift analysis, we need to remember we are optimizing for Net Value. Thus, we need to include the costs associated with sending these promotions. Figure 4.4 shows this change.\n\n\n\n\n\nFigure 4.4: Average net value for each treatment, broken down by customer loyalty tier.\n\n\n\n\nFigure 4.4 shows how the costs of marketing affected each treatment, but especially the discount ones. A 30% discount is a hefty price to pay for our business so if we can avoid such a heavy cost by finding treatments that cost less but are just as effective, that would be great. The results appear to hold from Figure 4.3.\n\n\nUplift Modeling\nAs mentioned in the introduction, uplift modeling estimates how a customer will react to interventions. There are many ways to estimate the uplift of a treatment. In this section, we implement a x-learner [4]. The x-learner belongs to a family of algorithms called meta-learners that specialize in utilizing machine learning approaches like random forests or boosting methods to estimate causal effects. While the x-learner was originally designed for binary treatments, [1] demonstrates a way to extend the framework to multiple treatments. The stages of implementing the x-learner are shown below.\n\\[\n\\mu_{t_{j}} = E[Y(t_{j})|X=x]\n\\tag{5.1}\\]\n\\[\nD^{t_{0}}_{i} = \\hat{\\mu}_{t_{j}}(x) - Y_{i}\n\\] \\[\nD^{t_{j}}_{i} = Y_{i} - \\hat{\\mu}_{t_{0}}(x)\n\\tag{5.2}\\]\n\\[\n\\hat{\\tau}^{t_{j}}(x) = \\frac{\\hat{e}_{t_{j}}(x)}{\\hat{e}_{t_{j}}(x) + \\hat{e}_{t_{0}}(x)}\\hat{\\tau}_{t_{0}}(x) + \\frac{\\hat{e}_{t_{0}}(x)}{\\hat{e}_{t_{j}}(x) + \\hat{e}_{t_{0}}(x)}\\hat{\\tau}_{t_{j}}(x)\n\\tag{5.3}\\]\nThe first stage of the x-learner is just normal regression. We fit any regression type model to model the outcomes of \\(Y\\) on covariates \\(X\\) for each treatment \\(t\\). The second stage of the x-learner is where we impute the treatment effect \\(D\\) by using the fitted regression models from Equation 5.1. This is essentially calculating counterfactual outcomes such that we can estimate individual treatment effects (ITEs). This is once again done for each treatment \\(t\\). We then “repeat” the process as before and create regression models that regress the treatment effects (or more properly, pseudo-effects) on the covariates \\(X\\) to obtain functions \\(\\hat{\\tau}_{t_{0}}\\) for the control and \\(\\hat{\\tau}_{t_{j}}\\) for the j-th treatment. The third stage utilizes the propensity scores for each treatment to adjust the “weights” of each treatment. These weights adjust the outputs from the models generated from Equation 5.2 to obtain our CATE for each treatment \\(j\\).\nTo train our x-learner, we use the causalML library. We use an 80-20 split on 4000 observations (~2000 control vs ~2000 treated, split among 9 treatments). The base learner for our x-learner is a gradient boosting regressor from sklearn.\nWhile understanding the CATE due to all features in the dataset, for this business case specifically, understanding the CATE for each tier is of great importance. An understanding of what promotion is most effective with each tier is crucical for marketing strategy optimization. Below we output the CATE values for those in the none tier.\n\n\narray([-0.50800717, -0.33919301, -0.48013309, -0.16522906, -0.2352354 ,\n       -0.09895951,  1.14556021, -0.28385185, -0.26179227])\n\n\n\n\n{'email-discount': 0,\n 'email-free_donut': 1,\n 'email-free_juice': 2,\n 'in-app-discount': 3,\n 'in-app-free_donut': 4,\n 'in-app-free_juice': 5,\n 'social-discount': 6,\n 'social-free_donut': 7,\n 'social-free_juice': 8}\n\n\nUtilizing the mapping dictionary above, we see that most of our promotions on average perform no better (or actually worse) than the control, with the exception to one. The social discount promotion shows that it adds an additional 1.15 to our net value, on average. Due to the synthetic nature of our data, we are pleased with this result as the x-learner correctly identified the one promotion that we set to have an influence on purchasing behavior for those in the none loyalty tier. In a real business case, we’d want to further validate these estimates by doing a variety of checks (e.g. confidence intervals, AUUC, sensitivity checks, etc.). For the purpose of this post, it suffices us to see that the x-learner identified the hidden causal mechanism in the data.\n\n\nUplift Modeling with Non Compliance\nFor the purposes of business strategy in understanding which promotions worked best, our above approach and results would suffice. We identified with our x-learner which promotions cause a higher ticket size for different tiered customers. We can present these suggestions to our marketing team and away they go at promoting the business more effectively. However, from the lens of causal inference data scientists, we know these estimates are biased. Not everyone we sent our promotions to went to the store to make a purchase. This means that not everyone who was treated acted upon the treatment we provided. This suggests that we have non compliance within our study.\nNon compliance gets its name from biostatistic experiments where researchers would assign treatments to individuals but those individuals would not comply with the directive. This is exactly what is happening in our study. Not everyone who was treated with a promotion went to the store to make a purchase. Figure 6.1 shows exactly this problem.\n\n\n\n\n\nFigure 6.1: Scatterplot of net value by distance to store colored by treatment.\n\n\n\n\nWe see that anyone who was more than 2.75 miles from the store did not make a purchase, regardless of treatment. However, we assumed in the beginning that those who were in the control could not receive promotional effects (i.e. control can’t non comply). Therefore, we are only dealing with non complianace via treated -&gt; control. From this perspective, while we did calculate a CATE, we did so estimating the intent to treat (ITT). This is the causal effect of treatment assignment, not of the treatment itself. To calculate the causal effect of the treatment, we need to accounnt for non compliance.\nIn order to account for non compliance within our study, we can utilize a popular approach of implementing an instrument variable (IV). IVs help reintroduce randomization into our study to help isolate the causal variation in our treatments. A IV needs to meet three assumptions. First, the IV must be correlated with the treatment (Relevance). Second, the IV must affect the outcome only through the treatment (Exclusion). Third, the IV must be independent of unmeasured confounders that affect the treatment and outcome (Exogeneity). In our case, we can simply add the instrument Promotion Offered to be our IV since it matches these assumptions. Figure 6.2 illustrates our new flow of causality.\n\n\n\n\n\nFigure 6.2: Proposed DAG for addressing non compliance in our study.\n\n\n\n\nOur updated DAG in Figure 6.2 shows how this new knowledge of non compliance affects how we model causality. Because we know that distance to store affects whether or not a customer goes to the store, it now has causal edges to promotion redeemed and ticket size. Additionally, from our EDA we noticed that different tiers purchase at different rates so tier still affects ticket size directly. As well, different tiers value different promotions so it has an effect on whether the promotion is redeemed or not. Lastly, we included our IV in promotion offered. Promotion offered is not affected by other confounders, only affects the outcome of net value through promotion redeemed, and is clearly correlated with promotion redeemed.\nTo model our new approach, we utilize causal IV forests from [2]. Causal forests build off the foundations of random forests. It utilizes n number of decision trees to calculate the average result. The causality comes from estimating the counterfactual outcomes, just as we saw with the x-learner above. To perform this, the splitting criterion focuses on the variation in treatment effect. A short summary of the algorithm is provided below.\n\nFor a given tree, randomly sample data with replacement and split into split and estimation set.\nFor a given node in a tree, iterate through features and cut points.\nIdentify the split that maximizes the variance-based criterion such that heterogeneity is maximized.\nFor a fully grown tree, for each terminal node, estimate CATE.\nAverage CATE across all trees.\n\nFor a more thorough explanation of the algorithm, see [2]. The algorithm is then adapted to our scenario where we utilize IVs. In this case, the algorithm becomes generalized to incorporate \\(Z\\), our indicator variable of whether or not a customer was assigned a promotion or not. The algorithm then adjusts for this by estimating the local conditional average treatment effect (L-CATE).\nOur causalIVforest is implemented using econML. Similar to above, we use a 80-20 split. We do not perform any hyperparameter tuning for the causalIVforest. We output the split of tiers in our test set below.\n\n\nloyalty_tier_none      309\nloyalty_tier_bronze    264\nloyalty_tier_silver    161\nloyalty_tier_gold       66\ndtype: int64\n\n\nAfter fitting our data, we test the model on the above sample and yield the following results shown in Figure 6.3. We also output the explicit effects for each treatment in the none loyalty tier below.\n\n\n\n\n\nFigure 6.3: C-LATE for each tier, treatment combination.\n\n\n\n\n\n\narray([-4.0571709 , -1.68161069, -4.44325053, -1.15262721, -2.26189429,\n       -0.27730894,  0.92089754, -0.57709819, -1.58279785])\n\n\nFigure 6.3 looks very similar to Figure 4.4, but with an important distinction. Figure 4.4 were the average net values in the raw data. Figure 6.3 shows the treatment effects for each treatment for each tier. Looking at the none tier graph and numeric output, we see that all treatments yield a negative treatment effect with the exception to social-discount. This makes sense since from our synthetic data, none of the other treatments should have yieled a change in consumer behavior. Thus, when accounting for the costs of marketing, these other treatments cause a negative effect in net value. Conversely, since social-discount yields a +1 increase to purchase behavior, it shows a positive increase in net value. This pattern continues throughout all the tiers.\nThe important distinction to make from this output and that of the x-learner is what the story behind each metric is. For the x-learner, we estimated the conditional intent to treat (CITT) effect, showing how assignment of the treatment causes changes in customer behavior. For the causalIVforest, we estimated the conditional local average treatment effect (C-LATE), showing how the redemption of the promotion causes changes in customer behavior. One estimates with everyone regardless of if they redeemed or not and the other estimates only for those who would redeem.\n\n\nConclusion\nIn this post, we posed a business problem of understanding the effects of our promotions on customer purchase behavior. We demonstrated two causal inference methods using machine learning: the x-learner and the causalIVforest. The x-learner was used to estimate CATE/CITT so that our business could understand the effects of assignment of promotion on our customers, specifically the different loyalty tiers. The causalIVforest was used to estimate C-LATE so that our business could more precisely understand the effects of redemption of promotion in customer behavior. Overall, we hope that this post was a good demonstration on the usefulness of causal inference in business situations and how one can utilize Python packages to carry out an informative analysis.\n\n\n\n\n\nReferences\n\n[1] Z. Zhao and T. Harinen, “Uplift modeling for multiple treatments with cost optimization,” arXiv preprint arXiv:1908.05372, 2019.\n\n\n[2] S. Wager and S. Athey, “Estimation and inference of heterogeneous treatment effects using random forests,” arXiv preprint arXiv:1510.04342, 2015.\n\n\n[3] D. B. Rubin, “Estimating causal effects of treatments in randomized and nonrandomized studies,” Journal of Educational Psychology, vol. 66, no. 5, pp. 688–701, 1974.\n\n\n[4] S. R. Kunzel, J. S. Sekhon, P. J. Bickel, and B. Yu, “Meta-learners for estimating heterogeneous treatment effects using machine learning,” Proceedings of the National Academy of Sciences, Jun. 2017."
  },
  {
    "objectID": "posts/welcome/welcome_to_blog.html",
    "href": "posts/welcome/welcome_to_blog.html",
    "title": "Welcome to my blog!",
    "section": "",
    "text": "TL;DR\nThis blog focuses on the use of statistics, particularly bayesian statistics, in the application of business problems. In this post, we review the necessary math background in order to understand future blog posts. As well, we establish expectations for future blog posts and the order in which they are shared.\n\n\nWelcome To My Blog!\nWelcome to my blog! If you are reading this, I hope that means you are as excited about learning statistics as I am about teaching it (and continuing to learn it!). This blog, honestly, is mostly a tool for me to improve my own knowledge in statistics by learning how I would convey it to others. I plan on doing this from a data analysis perspective. Given a certain situation/set of data points, how do we construct the problems we want to solve and implement proper solutions. Particularly, I hope to demonstrate that Bayesian methods can be viable methods to solve problems in business, healthcare, etc. Anyways, I hope you find this blog interesting and useful in your data career. Whether you are just entering data or are an experienced data professional, this blog intends to be a resource in demonstrating good data analysis with sound applied mathematical theory.\n\n\nPrerequisites\nWhile I did mention above that this blog is intended to be useful for beginners and experts alike, I will admit that I will be covering some higher level math, such as:\n\n\nMultivariate Calculus\n\n\nProbability Theory\n\n\nLinear Algebra\n\n\nFor example, suppose we are studying a dataset with a variable that is modeled as an exponential random variable, we would use the below notation to show the probability density function (PDF)\n\\[\\begin{equation}\n\\int_{0}^{\\infty} \\lambda e^{\\lambda x} \\,dx\n\\end{equation}\\]\n\\[\\begin{equation}\nX \\sim Exponential(\\lambda)\n\\end{equation}\\]\nOr, the important Bayes Theorem found below.\n\\[\\begin{equation}\nP(A|B) = \\frac{P(B|A) P(A)}{P(B)}\n\\end{equation}\\]\nDon’t be discourage if the above formulas don’t make sense right now. In each post that these appear, I will breakdown what they mean and their applicability in solving our problem.\n\n\nExpectations\nThe hope of this blog is to present statistics in the form of a data analysis case. For example, let’s say you are a host on AirBnB and want to maximize the number of nights you rent out. How do you properly price your rental given certain parameters (size, geographic setting, etc). We would then present the data points and walk through an analysis of the data by doing the following.\n\n\nExploring the data (EDA) to get to know our dataset\n\n\nConstructing a mathematical framework that can fit our data\n\n\nFit our data to said framework\n\n\nGather inference from our model\n\n\nReview answers explained by our model\n\n\nExplain possible enhancements to our model for future analyses\n\n\nThis will be our attempted framework to blog posts. I hope that by tackling problems in this way, our analytical toolbox will grow and our ability to construct measurable problems from our data will improve. This is the ultimate goal of this blog. I really hope that people will recognize this blog as an opportunity to learn how to think analytically.\n\n\nLet’s do this\nI just want to reiterate that I am excited to learn how to be a better analyst with you by getting to know the math that drives our analytics. I am very passionate about how data can be used to properly drive decision-making in organizations and I hope that this blog motivates you to do the same."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html",
    "href": "posts/wtp_air/wtp_air.html",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "",
    "text": "Dynamic pricing has been the dominant approach in airline revenue management systems for decades. This method has continually improved with advances in new algorithms and better computing power. However, even with these improvements, the general pricing system remained rooted in price discrimination principles. Recently, Large Language Models (LLMs) have opened new avenues for many technologies. Their underlying structures and capabilities provide an opportunity for a new pricing paradigm to enter the airline industry: fully personalized pricing. In this post, we review the history and current approaches to airline pricing systems, from economic theory to the algorithms generally utilized. We then present information on Large Market Models (LMMs), the new approach companies are leveraging to provide completely personalized prices to airline consumers. We conclude with a basic prototype of personalized pricing using a dynamic hierarchical Bayesian mixed logit model."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#first-methods-yield-management",
    "href": "posts/wtp_air/wtp_air.html#first-methods-yield-management",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "First Methods: Yield Management",
    "text": "First Methods: Yield Management\nThis market-based pricing approach became known as yield management (a term coined by Robert Cross at American Airlines). Yield management constituted a systematic approach focused on identifying distinct customer segments, setting different prices for these segments, and updating these prices regularly. Essentially, yield management aims to estimate a consumer’s WTP and set prices that capture as much of that value as possible.\nAirlines recognized that each customer had a unique WTP, but given the computational limitations of the time, calculating individual WTP was impractical. Insetad, general market approaches such as price discrimination were employed [4]. Rather than offering every seat at a uniform price, airlines could differentiate seats with varying perks and sell them at distinct price points, thereby segmenting consumers into different WTP ranges.\n\n\n\nDifferent seating choices on a Boeing 777\n\n\nBeyond differentiating prices based on seating sections, airlines also began to strategically set prices according to supply and demand dynamics. With the increasing prevalence of computer systems, airlines gained the capacity to collect data on passenger booking patterns. For instance, they observed that leisure travelers typically booked tickets well in advance, while business travelers often purchased tickets with short notice. This behavioral insight was leveraged to develop tactics such as fare classes and fences [5].\nThe fare class system operates as follows: While seat prices are initially differentiated by section, fare classes further segment inventory by stipulating the order in which seats are sold and at what price points. For example, within the economy section, different fare classes (e.g., A, B, C, D) are established. Seats assigned to class A are priced the lowest, with progressively higher prices for classes B, C, and D. This hierarchical pricing structure continues until a given class is fully booked. This approach is designed to mimic supply-demand principles: once a lower-priced class (e.g., A) is sold out, subsequent bookings are directed to higher-priced classes (e.g., B), reflecting a decrease in the remaining supply for that particular section. Thus, fare classes allowed airlines to “dynamically” adjust prices based on the quantity of seats sold within each designated class.\nFences extend the fare class system by further refining price discrimination. These rules aim to retain higher-WTP customers within their designated fare classes while encouraging lower-WTP customers to “jump” fences and access more price-sensitive fares. For instance, airlines typically offer lower prices for early bookings to incentivize leisure travelers to commit in advance, whereas business travelers often pay a premium for bookings made closer to the travel date. Another common example is the minimum stay rule. Airlines frequently implement a Saturday-night stay requirement for round-trip tickets to differentiate business travelers from leisure travelers."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#modern-methods-big-data",
    "href": "posts/wtp_air/wtp_air.html#modern-methods-big-data",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Modern Methods: Big Data",
    "text": "Modern Methods: Big Data\nWhile the overall approach to pricing airfare remains deeply rooted in economic theory and operations research, airline revenue management systems have become significantly more sophisticated due to advances in data collection and predictive analytics.\nA key development in this area, for example, is the application of multi-armed bandit problems to dynamic pricing. Papers such as A Modern Bayesian Look at the Multi-armed Bandit [6] leverage the power of multi-armed bandits to contextualize consumer behavior and optimize prices to encourage purchase behavior.\nModern systems have also embraced other machine learning techniques to process a broader array of data points. Instead of relying solely on booking history and fare class availability, algorithms now incorporate factors such as web traffic patterns, historical search data for specific routes, time of day, and even weather forecasts. These models are designed to uncover complex, non-linear relationships between these variables and consumer demand, allowing for more nuanced and responsive pricing adjustments. The goal is to move beyond simple segmentation based on booking time and toward a more data-driven, continuous assessment of market conditions."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#large-language-models",
    "href": "posts/wtp_air/wtp_air.html#large-language-models",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Large Language Models",
    "text": "Large Language Models\nIn 2017, Google released the paper “Attention Is All You Need” by Vaswani et al. [7]. This paper introduced the revolutionary architecture of transformers to process sequences of data, which was a significant breakthrough in natural language processing (NLP). The transformer’s ability to handle large amounts of data in parallel was a key enabler for the development of much larger models.\n\n\n\nTransformer Architecture\n\n\nFrom this foundational paper, research and development in the field accelerated. Examples include companies like OpenAI, who began developing their own Large Language Models (LLMs) to produce a personalized search engine. The transformer architecture fundamentally changed the landscape of AI, and today, almost all companies are looking to incorporate these powerful models into their services for personalization and advanced data processing."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#large-market-models",
    "href": "posts/wtp_air/wtp_air.html#large-market-models",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Large Market Models",
    "text": "Large Market Models\nFrom a simplified point of view, the core objective of an LLM is to predict the next token in a sequence of input tokens. This fundamental principle of next-token prediction is now being generalized across various industries to address different problems. A prominent challenge being tackled is next market dynamic prediction. That is, using sequential observations of a market’s behavior to forecast its subsequent state.\nThis new class of models aiming to predict market dynamics are called Large Market Models (LMMs) [8]. Just like LLMs, LMMs can process large amounts of data to ultimately predict optimal actions within a given market. In the context of airline revenue management, this optimal action would be the optimal price. However, this pricing approach fundamentally differs from current dynamic pricing strategies. Instead of the “hard jumps” between fare classes (as previously discussed), LMMs can generate a continuous function for pricing at the individual consumer level.\nOne company at the forefront of this research is Fetcherr. Founded in 2019 by Roy Cohen, Uri Yerushalmi, and Robby Nissan, the company believed that traditional business decision-making was not leveraging the full potential of modern AI and machine learning tools. Consequently, they developed a new revenue management system centered on the use of LMMs to revolutionize the airline industry. As their technology is proprietary, the specific implementation details cannot be discussed here. However, for those interested in a deeper understanding, a presentation on their approach is available in the video below."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#utility-theory",
    "href": "posts/wtp_air/wtp_air.html#utility-theory",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Utility Theory",
    "text": "Utility Theory\nThe theory for our modeling approach stems from microeconomics, namely utility theory [9]. As previously discussed, the primary objective for airlines is to capture as much consumer surplus as possible by accurately estimating a customer’s WTP. WTP itself is derived from the principle that individuals seek to maximize their utility from the consumption of a good or service. Therefore, if an airline’s service provides a consumer with utility, the goal is to set a price that is optimally aligned with the value of that utility, thereby maximizing the revenue captured.\nIn our simulation, given the discrete consumer choice between alternatives, we use the random utility model [10], as shown in Equation 4.1.\n\\[\nU_{ijt} = V_{ijt} + \\epsilon_{ijt}\n\\] \\[\n\\epsilon_{ijt} \\overset{i.i.d.}{\\sim} \\text{Gumbel}(0,1)\n\\tag{4.1}\\]\n\\(U_{ijt}\\) is the utility derived by the \\(i\\)th customer choosing the \\(j\\)th alternative in the \\(t\\)th scenario. This utility is composed of a deterministic portion \\(V_{ijt}\\) (our “observed” utility from purchased ticket data) and a stochastic term \\(\\epsilon_{ijt}\\) that we assume follows a Gumbel distribution.\nWhile we don’t observe an exact “utility” estimate in this data (or in any data for that matter), we postulate that utility is derived as a linear combination of observed features. This is illustrated in Equation 4.2.\n\\[\nV_{ijt} = \\text{ASC}_{flight1,i,t} I(\\text{Flight 1}) + \\text{ASC}_{flight2,i,t} I(\\text{Flight 2}) + \\beta_{i,price,t} Price_{jt} + \\beta_{i,stops,t} Stops_{jt} + \\beta_{i,seat,t} Seat_{jt}\n\\tag{4.2}\\]\nThe equation Equation 4.2 illustrates that the observed utility, \\(V_{ijt}\\), is a function of several key attributes of both the alternatives and the customer. Specifically, the utility is dependent on the flight’s price, the number of stops, and the type of seat purchased.\nThe utility we derive in Equation 4.2 is then used to calculate the probability of choosing a given alternative in a given scenario. This is illustrated in Equation 4.3.\n\\[\nP(Y_{ijt} = 1) = \\frac{exp(V_{ijt})}{\\sum_{z=1}^{J}exp(V_{izt})}\n\\tag{4.3}\\]\nEssentially, we assume that the probability of a consumer’s observed choice is a function of the utility of a given alternative ($V_{ijt}) relative to the utilities of all other available alternatives."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#hierarchical-bayesian",
    "href": "posts/wtp_air/wtp_air.html#hierarchical-bayesian",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Hierarchical Bayesian",
    "text": "Hierarchical Bayesian\nAs mentioned at the outset of this section, we are modeling this scenario hierarchically. This approach is based on the belief that individuals derive their preferences from a “global” distribution, which is then adjusted based on personal differences. To implement this, we model each \\(\\beta\\) value as a draw from a global population distribution, as shown in Equation 4.4.\n\\[\n\\beta_{it} = \\begin{pmatrix} \\text{ASC}_{flight1,i,t} \\\\ \\text{ASC}_{flight2,i,t} \\\\ \\beta_{i,price,t} \\\\ \\beta_{i,stops,t} \\\\ \\beta_{i,seat,t} \\end{pmatrix} \\sim N(\\mu_{t}, \\Sigma)\n\\tag{4.4}\\]\nEach customer \\(i\\) in scenario \\(t\\) has a vector \\(\\beta\\) that is drawn from a multivariate normal distribution with mean \\(\\mu_{t}\\) and covariance \\(\\Sigma\\). \\(\\mu_{t}\\) is our vector of population parameter estimates for the average preference for each attribute of a flight at time \\(t\\). \\(\\Sigma\\) represents the unobserved heterogeneity around \\(\\mu_{t}\\) (Note: we hold this value constant throughout to simplify modeling).\nWe model \\(\\mu_{t}\\) as an AR(1) process, as shown in Equation 4.5.\n\\[\n\\mu_{kt} = \\mu_{k,base} + \\phi_{k}(\\mu_{kt-1} - \\mu_{k,base}) + \\eta_{kt}\n\\tag{4.5}\\]\n\\(\\mu_{kt}\\) is the average preference for attribute \\(k\\) at time \\(t\\). This is a linear combination of three key components: The base preference (\\(\\mu_{k,base}\\)), the difference between the past average preference and the base preference scaled by autoregressive component \\(\\phi_{k}\\) (how much the past mean influences the current mean), and the random shock \\(\\eta_{kt}\\) which accounts for unobserved fluctuations in the average preference. (Note: We assume \\(\\eta_{kt}\\) is drawn from \\(N(0,q_{k})\\). Interested readers can see the code for \\(q_{k}\\) prior).\nThe \\(\\mu_{k,base}\\) parameter is estimated from the baseline population hyperparameters, as shown in Equation 4.6.\n\\[\n\\mu_{k,base} = \\alpha_{k} + \\delta_{kc}\n\\tag{4.6}\\]\n\\(\\alpha_{k}\\) is the intercept for the baseline population mean for each attribute \\(k\\). This is then adjusted based on different customer demographics \\(c\\) for each attribute \\(k\\). Each \\(\\alpha_{k}\\) and \\(\\delta_{kc}\\) has prior \\(N(\\mu, \\sigma)\\) (interested readers can view the numpyro code for specific numbers).\nThe likelihood function based on all these parameters is modeled in Equation 4.7.\n\\[\nL(Y|\\beta, \\mu, \\Sigma, \\phi, Q) = \\prod_{i=1}^{N} \\prod_{j=1}^{J} \\prod_{t=1}^{T_{i}} P(Y_{ijt} = 1)\n\\tag{4.7}\\]\nOnce these parameters are estimated, we can calculate individual WTP using Equation 4.8.\n\\[\n\\text{WTP}_{ikt} = -\\frac{\\beta_{ikt}}{\\beta_{i,price,t}}\n\\tag{4.8}\\]\nWTP is formally defined as the marginal rate of substitution between a non-monetary attribute and price. Within our logit framework, this is calculated as the ratio of an attribute’s coefficient to the absolute value of the price coefficient. A positive WTP indicates the amount a customer is willing to pay to gain an attribute, while a negative WTP represents the cost a customer is willing to incur to avoid an undesirable attribute.\nTo model this data, we used numpyro [11] using a NUTS kernel [12] and collected 1000 samples."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#results",
    "href": "posts/wtp_air/wtp_air.html#results",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Results",
    "text": "Results\nIn the context of airline pricing, we’d like to understand the WTP for customers avoiding layovers (stops) on their flights and the WTP for customers seeking a premium class seat. If we can identify different WTP for each customer, we can better personalize pricing based on these attributes to capture more revenue per customer.\nTo illustrate the results of our model, we present the average WTP estimate for stops along with 95% credible interval for customer 5. These results are shown below for each scenario \\(t\\).\n\n\nAverage WTP stop for customer 5: -11.361211776733398\n95% CI for customer 5: [-60.          -0.18702286]\n\n\n\n\nAverage WTP stop for customer 5: -10.813233375549316\n95% CI for customer 5: [-60.         -0.1928775]\n\n\n\n\nAverage WTP stop for customer 5: -10.883264541625977\n95% CI for customer 5: [-60.          -0.17119325]\n\n\n\n\n\n\n\nFigure 4.5: Posterior distributions for WTP on stops across different scenarios\n\n\n\n\n\n\n\n\n\n\n\n\n\ncust_id\nscen_id\nage\nincome\nis_business\nseat_browsed\nprice_f1\nstops_f1\nprice_f2\nstops_f2\nchoice\n\n\n\n\n15\n5\n0\n18-24\nmed\n0\necon\n207.0\n1\n237.0\n1\n1\n\n\n16\n5\n1\n18-24\nmed\n0\nprem\n323.0\n0\n328.0\n0\n0\n\n\n17\n5\n2\n18-24\nmed\n0\necon\n180.0\n2\n228.0\n0\n1\n\n\n\n\n\n\n\nCustomer 5, an individual aged 18-24 with a middle income who is not a business traveler, shows a WTP of approximately $11 to avoid one additional stop. This is the marginal value that customer 5 would be willing to add to the ticket, on average, to avoid an additional stop on their flight. If we were to set prices at the customer level, we could use this information to set a price below this ceiling amount to better capture the additional consumer surplus.\nWe also can view customer 5’s WTP for seat upgradge (going up from economy to premium). These results are shown below.\n\n\nAverage WTP seat for customer 5: 26.012453079223633\n95% CI for customer 5: [ 0.8786044 60.       ]\n\n\n\n\nAverage WTP seat for customer 5: 26.450408935546875\n95% CI for customer 5: [ 0.83505126 60.        ]\n\n\n\n\nAverage WTP seat for customer 5: 26.11977767944336\n95% CI for customer 5: [ 1.03312797 60.        ]\n\n\n\n\n\n\n\nFigure 4.6: Posterior distribution for WTP on seat upgrade across different scenarios\n\n\n\n\nThe posterior distributions for seat upgrades are more spread out, indicating a higher degree of uncertainty in our estimates compared to those for stops. Similar to the distributions in Figure 4.5, these posterior distributions do not vary significantly across each scenario. It is worth noting that the mean WTP for seat upgrades is higher for Customer 5, reflecting a greater potential value, though this estimate comes with higher uncertainty due to the wider distribution. This uncertainty can be incorporated directly into our pricing strategy to create a more robust offering system."
  },
  {
    "objectID": "posts/wtp_air/wtp_air.html#price-offer-strategy",
    "href": "posts/wtp_air/wtp_air.html#price-offer-strategy",
    "title": "(AI)rline Pricing: Estimating Willingness To Pay",
    "section": "Price Offer Strategy",
    "text": "Price Offer Strategy\nOur goal is to maximize the expected profit from a given price point for a particular customer. While we could set the price as the base fare plus the mean of the posterior distribution, a more optimal approach would be finding a price point \\(p\\) that would maximize Equation 4.9.\n\\[\nE[p-c] = (p-c)P(WTP_{ikt} &gt; p)\n\\tag{4.9}\\]\n\\(p\\) is the price and \\(c\\) is the cost associated with the flight. The probabilities from Equation 4.9 can be pulled from our posterior distributions.\nAssuming a cost \\(c\\) of $200 and a base fare \\(p\\) of $200, we determine the optimal price for a flight based on the WTP for avoiding a stop. To do this, we test integer price points ranging from $5 to $30 above the base fare. Using the posterior distribution derived from scenario 2 for customer 5, the resulting optimal price is shown below.\n\n\nOptimal price to maximize profit: $218"
  },
  {
    "objectID": "posts/net_pass/net_pass.html",
    "href": "posts/net_pass/net_pass.html",
    "title": "Can I get your password? An empirical approach to account sharing policies",
    "section": "",
    "text": "Password sharing was a staple of modern video streaming services, particularly when the market was dominated by few players. Now, with many new streaming entrants, account sharing has become a wound in corporate revenue. In this post, we take the perspective of a video streaming company tackling this problem. We analyze account sharing data along with the impact on the company bottom line. We then walk through different ways of amending this problem through strategic frameworks and empirical models. We conclude by discussing how our findings should guide company policy on account usage."
  },
  {
    "objectID": "posts/net_pass/net_pass.html#the-founding",
    "href": "posts/net_pass/net_pass.html#the-founding",
    "title": "Can I get your password? An empirical approach to account sharing policies",
    "section": "The Founding",
    "text": "The Founding\n\nIn 1997, Netflix was founded by Reed Hastings and Marc Randolph. The two set out to provide a new way of movie watching, specifically for movie lovers. From a strategic point of view, Reed and Marc wanted to serve a niche market by providing a more accommodating service.\nFrom their perspective, companies like Blockbuster experienced high fixed costs (e.g., physical store locations), with their main stream of revenue coming from newly released movies and late fees. Thus, the shelf life of movies (with regard to lifetime value) was short-lived, as many would rent the new releases, watch them once or twice, forget to return the movie, and Blockbuster would rack up the late fees. What was once an incentive to get valuable inventory back on the shelves turned into the most important line of business for Blockbuster.\nReed and Marc decided that there could be a better way to serve the entertainment-seeking public. Instead of designing a business around late fees, physical locations, and the short shelf life of movies, they decided to essentially flip the model. The business would be a centralized warehouse serving subscribers. Instead of customers being tied to a deadline for returning movies, they could keep them for as long as they liked by paying a monthly subscription. This would allow movie lovers to watch multiple movies for a flat monthly fee.\nAnother key component of their business would be providing movie recommendations. Reed and Marc wanted to help customers discover their entire inventory of movies, not just the popular ones. Their theory was that if they could help customers find other movies they might enjoy, customers would be more willing to try different titles and become more attached to the company.\nWith these insights and ideas, Reed and Marc iterated and grew Netflix. Netflix became very popular, outgrowing Blockbuster and becoming the king of home entertainment. Because their business model was setup well for iterating, Netflix eventually migrated from the physical DVD world to the digital world of streaming. With this move, Netflix became the streaming king, with approximately 200 million subscribers in 2020."
  },
  {
    "objectID": "posts/net_pass/net_pass.html#the-achilles-heel",
    "href": "posts/net_pass/net_pass.html#the-achilles-heel",
    "title": "Can I get your password? An empirical approach to account sharing policies",
    "section": "The Achilles Heel",
    "text": "The Achilles Heel\nNetflix, as an established entertainment service, launched their streaming service in 2007. With first-mover advantage, Netflix established themselves as the king of streaming. Others would soon follow with their own streaming services: Hulu launched in 2010, HBO in 2015, and Disney+ and Apple TV+ in 2019, just to name a few. Even with all of these other services taking a piece of the pie, Netflix continued to show substantial growth year-over-year (see ?@tbl-sub-1 for details).\n\nNetflix Subscribers YoY {#tbl-sub-1} (source: Netflix Investor Relations Letter)\n\n\nYear\nNumber of Subscribers (in million)\n\n\n\n\n2020\n192.9\n\n\n2019\n151.5\n\n\n2018\n124.3\n\n\n2017\n99\n\n\n2016\n79.9\n\n\n2015\n62.7\n\n\n2014\n47.9\n\n\n2013\n35.6\n\n\n2012\n25.7\n\n\n2011\n21.5\n\n\n\nHowever, growth began to slow. In 2021, Netflix grew 14% in subscribers, but that figure dropped to only 5% growth in 2022. What could have been slowing Netflix’s subscriber growth? One of the greatest causes was password sharing."
  }
]