{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f71d18a1-d578-4061-92e6-fd1b6ee03945",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"An implementation of the ITCC algorithm in Python\"\n",
    "draft: false\n",
    "toc: true\n",
    "author: \"Brandon Scott\"\n",
    "date: \"2025-02-10\"\n",
    "image: clust_pic.webp\n",
    "number-sections: false\n",
    "categories: [python, clustering]\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "crossref:\n",
    "    chapters: true\n",
    "format:\n",
    "    html:\n",
    "        code-fold: false\n",
    "jupyter: python3\n",
    "bibliography: ../../references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26ff8e-2ad6-4e93-9250-e04e9b177564",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a614c9c-e6f7-4dac-b52f-b624b021b956",
   "metadata": {},
   "source": [
    "Clustering is a useful tool in data mining whereby data points are collected together based on some predefined similarity metric. This results in a more digestible data model that can provide solid inference into how related certain attributes of data are with each other. In this post, we discuss the information theoretic co-clustering algorithm [@dhillon2003information] and provide a python implementation of said algorithm. We discuss the usefulness of co-clustering and propose potential future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f07962-2dfd-4f8d-b8a6-9d50270f0873",
   "metadata": {},
   "source": [
    "See this <a href=\"https://www.cs.utexas.edu/~inderjit/public_papers/kdd_cocluster.pdf\">link</a> for the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e1492-38f6-4f87-89d4-d6e9ad1fd7e9",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40708f9-6193-4786-af76-8df88024f91a",
   "metadata": {},
   "source": [
    "Co-clustering can be defined as a family of algorithms that simultaneously clusters rows and columns. Suppose we have a matrix A that contains n rows and m columns, as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54822c47-9480-4345-8c8f-f91c3f2f4c65",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    "    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\cdots & a_{nm}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ecee5-f50b-4655-8d21-81e53a4a4c3d",
   "metadata": {},
   "source": [
    "Our objective with $A$ would be to identify distinct groups of data that exhibit great similarities to one another. One way to expose these groupings would be through a clustering algorithm like K-means. However, the down side to K-means is the single modality nature of the algorithm. That is, rows and columns would be clustered independetly of each other. In real-world scenarios, there are generally relationships between the rows and columns that we'd want to capture as part of our analysis. For example, suppose the rows of $A$ were customers and the columns of $A$ were products available. $a_{ij}$ would then represent the quantities purchased by the $i$th customer for the $j$th product. We'd want to capture in our analysis the relationships between customers and the products. This is where co-clustering comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81569d-5e24-4ad3-b9f0-4dd4bffddb48",
   "metadata": {},
   "source": [
    "Co-clustering are families of algorithms that simultaneously cluster rows and columns. Contuining with the example above of rows representing customers and columns representing products, suppose we have the following data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd7e45-223e-4b23-846a-440e09e14d4b",
   "metadata": {},
   "source": [
    "\\begin{pmatrix}\n",
    "    3 & 3 & 0 \\\\\n",
    "    4 & 4 & 0 \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367c711-8438-4886-8e70-9be8209b9498",
   "metadata": {},
   "source": [
    "$a_{ij}$ once again represents the purchases by the $i$th customer for the $j$th product. In normal clustering approaches (like in K means clustering), we would ignore the column structure of this matrix and instead focus on the row structure only. Thus, we would probably end up with two clusters, with the the first two rows pertaining to one cluster and the last row pertaining to another. This approach, while helpful in identifying data points that closely resemble each other, does not take into account the complete picture offered by the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80825257-533a-4b35-91f3-d85aa48ed589",
   "metadata": {},
   "source": [
    "In co-clustering, we would analyze both the row structure <b>and</b> the column structure. We would continue to use the row structure, as illustrated in the previous example, but we would also include column clusters, more than likely clustering the first two columns together and the last one by itself. Thinking back to what this data set represents, we can now show at a more granular level what customers have similar purchase histories, as well as what products are purchased together. Thus, we have a better understanding of a customer journey and their interactions with our products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6d6a1-c662-45a3-b6dc-f748ce3c5265",
   "metadata": {},
   "source": [
    "# Information Theoretic Co-Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f4864-be4f-4f26-978c-9dc6c4187522",
   "metadata": {},
   "source": [
    "Now that we have a solid foundation on what co-clustering is and its potential uses, we explore the co-clustering algorithm \"Information Theoretic Co-Clustering\" [@dhillon2003information]. For a more detailed explanation, please refer to the paper link above or the referenced paper in the works cited. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8a453f-a120-449c-801d-84cf4daa650e",
   "metadata": {},
   "source": [
    "The ITCC algorithm poses the challenge of clustering as an optimization problem using relative entropy, as shown below in @eq-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddeb621-ec57-4f15-8512-addd3c64c26d",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "D_{KL}(P||Q) = \\sum_{x} \\sum_{y} P(x, y) \\log \\left( \\frac{P(x, y)}{Q(x, y)} \\right) \\label{eq:kl_joint_discrete}\n",
    "\\end{equation}$$ {#eq-entropy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b1fb2-6970-4323-9e1e-bc8d38e7dd99",
   "metadata": {},
   "source": [
    "Essentially, we are trying to find a prototype or approximate joint distribution $Q(x,y)$ to minimizes the distance from $P(x,y)$. To do this, the ITCC algorithm attempts to calculate this minimized approximated joint disribution $Q(x,y)$ by monotonically decreasing the objective function (the objective function being mutual information loss, thus minimizing the information lost between the true joint distribution and the approximate joint distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db596e4-edf1-4759-9cf9-2046da8bed25",
   "metadata": {},
   "source": [
    "To calculate this approximate joint distribution, a set number of row clusters and column clusters are assigned. Then, rows and columns are assigned to a specific cluster number, up to n row clusters and m column clusters. A joint cluster distribution is then calculated, which we will denote as $P(\\hat{x},\\hat{y})$. The approximate joint distribution $Q(x,y)$ is the calculated using $P(\\hat{x},\\hat{y}$ and other conditional distributions. $Q(x,y)$ is calculated by finding rows and columns that minimize @eq-entropy, relative to the conditional $P(X|\\hat{Y})$ and $P(Y|\\hat{X})$. A more structured run through of the algorithm is found below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a243e2f-93be-4c07-969e-303a501051ac",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> Initialize co-cluster for rows ($C_X$) and co-cluster for columns ($C_Y$)</li>\n",
    "    <li> Calculate $q^{(0)}$ $(\\hat{X}, \\hat{Y})$, $q^{(0)}$ $(X|\\hat{X})$, $q^{(0)}$ $(Y|\\hat{Y})$, and $q^{(0)}$ $(Y|\\hat{x})$</li>\n",
    "    <li> Compute new column clusters for each row x where $C^{t+1}_{X}(x)=$ $argmin_{\\hat{x}}$ $D(p(Y|x) || q^{(t)}(Y|\\hat{x}))$</li>\n",
    "    <li> Compute distributions $q^{(t+1)}$ $(\\hat{X}, \\hat{Y})$, $q^{(t+1)}$ $(X|\\hat{X})$, $q^{(t+1)}$ $(Y|\\hat{Y})$, and $q^{(t+1)}$ $(X|\\hat{y})$</li>\n",
    "    <li> Compute new column clusters for each column y where $C^{t+2}_{Y}(y)=$ $argmin_{\\hat{y}}$ $D(p(X|y) || q^{(t+1)}(X|\\hat{y}))$</li>\n",
    "    <li> Compute distributions $q^{(t+2)}$ $(\\hat{X}, \\hat{Y})$, $q^{(t+2)}$ $(X|\\hat{X})$, $q^{(t+2)}$ $(Y|\\hat{Y})$, and $q^{(t+2)}$ $(Y|\\hat{x})$</li>\n",
    "    <li> Stop and return current row and column clusters ($C_X,C_Y$) if the change in the objective function value is small. Else, repeat starting at step 2.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4315825-da20-4e3c-90cb-cb67c69eb0a5",
   "metadata": {},
   "source": [
    "Essentially, the algorithm forms row and column cluster prototypes, calculates the appropriate distributions to get to the approximate joint distribution $q(X,Y)$, then measures the distance between $p(X,Y)$ and $q(X,Y$ using @eq-entropy. The algorithm is able to be monotonically decreasing due to the fact that we always select the cluster that minimizes @eq-entropy (see steps 3 and 5 in the algorithm steps above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ece34-bf4d-4ced-9b72-276d82b433cf",
   "metadata": {},
   "source": [
    "# Code Walk-Through"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df80a7-ea8a-4395-87e1-5e7003bba455",
   "metadata": {},
   "source": [
    "In this section, we'll walk through the various code chunks that make up our ITCC algorithm. Full disclosure, this is a very crude approach to implementing this algorithm. There are far better implementations of this algorithm along with other co-clustering algorithms (e.g. scikit learn). This is merely an educational exercise by me to practice implementing clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418fb027-8deb-456e-98d7-78a67ce8311b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import kl_div, rel_entr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f906e2d-27b3-4d94-8277-e3db38a26d49",
   "metadata": {},
   "source": [
    "To begin the code walk through, we will initialize our joint distribution $P(X,Y)$, which is found below. This is the same joint distribution used in the ITCC paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebf46f7-d5b3-42ab-892d-7b09d0549207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#| output: false\n",
    "#Initialize test array\n",
    "test_arr = np.array([.05, .05, .05, 0, 0, 0, \n",
    "          .05, .05, .05, 0, 0, 0, \n",
    "          0, 0, 0, .05, .05, .05, \n",
    "          0, 0, 0, .05, .05, .05, \n",
    "          .04, .04, 0, .04, .04, .04,\n",
    "          .04, .04, .04, 0, .04, .04]).reshape(6,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45dfd2e-18d3-480f-8af7-0db6801502e3",
   "metadata": {},
   "source": [
    "Our $P(X,Y)$ is a 6x6 matrix. We will attempt to find the optimal 3 row clusters and 2 columns clusters using our ITCC implementation, just as performed in the ITCC paper. To do this, we will implement various functions that calculate the necessary distributions for eventually comparing $P(X,Y)$ and $Q(X,Y)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0cbc2c-0518-4b12-9607-1d719456858e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04, 0.  , 0.04],\n",
       "       [0.05, 0.05, 0.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "temp_1 = np.ix_([4,1], [0,2,4])\n",
    "test_arr[temp_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d90d9f-f99c-4663-9ac9-2c97e4140a73",
   "metadata": {},
   "source": [
    "Our first function in the code walk through is calculating the joint distribution $Q(\\hat{X},\\hat{Y})$, which is the joint distribution of the row and column clusters. The code to do this is found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fa6fdfc-df9d-4833-a237-36db55da6692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating joint distribution\n",
    "def calc_joint(x: dict, y: dict, a: np.array) -> np.array:\n",
    "    joint_arr = np.zeros((len(x), len(y)))\n",
    "    #joint_arr = {}\n",
    "    for i, j in x.items():\n",
    "        for z, p in y.items():\n",
    "            ind = np.ix_(j,p)\n",
    "            joint_arr[i,z] = a[ind].sum()\n",
    "    \n",
    "    return joint_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c2600-53db-4712-a602-bb90f6ebe9fb",
   "metadata": {},
   "source": [
    "The function accepts the row clusters, column clusters, and joint distribution $P(X,Y)$. We chose to use dictionaries to illustrate the structure of co cluster prototypes with the co cluster number as the key and the value being a list of indices where the respective row(s) or column(s) lie. The function returns the mxn array, where m is the number of row clusters and n is the number of columns clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df71dd13-546b-4bb4-9c7f-c54964311ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 0. ],\n",
       "       [0. , 0.3],\n",
       "       [0.2, 0.2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| output: false\n",
    "calc_joint({0: np.array([0, 1]), 1: np.array([2, 3]), 2: np.array([4, 5])}, \n",
    "          {0: np.array([0, 1, 2]), 1: np.array([3, 4, 5])}, test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3284d4c-2f16-4dec-b67d-b903ca4a748a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for randomly generating x_hat\n",
    "def get_x_hat(jd: np.array, k: int, new=False):\n",
    "    x_hat = {}\n",
    "    \n",
    "    if new:\n",
    "        x_ind = np.array(range(jd.shape[0]))\n",
    "        np.random.shuffle(x_ind)\n",
    "        \n",
    "        for i in range(k):\n",
    "            x_hat[i] = x_ind[i::k]\n",
    "        \n",
    "        return x_hat\n",
    "    else:\n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4acbe4b-01be-4624-8d22-4f70780f6de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for randomly generating y_hat\n",
    "def get_y_hat(jd: np.array, l: int, new=False):\n",
    "    y_hat = {}\n",
    "    \n",
    "    if new:\n",
    "        y_ind = np.array(range(jd.shape[0]))\n",
    "        np.random.shuffle(y_ind)\n",
    "        \n",
    "        for i in range(l):\n",
    "            y_hat[i] = y_ind[i::l]\n",
    "        \n",
    "        return y_hat\n",
    "    else:\n",
    "        print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141dd644-b248-424a-8996-f688580c642a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for calculating marginal of x\n",
    "def calc_x_mar(jd: np.array, x_ind: dict) -> np.array:\n",
    "    mar_dist = np.zeros((len(x_ind)))\n",
    "    \n",
    "    for i, ind in x_ind.items():\n",
    "        mar_dist[i] = jd[ind,:].sum()\n",
    "     \n",
    "    return mar_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91acb1f-7889-42b6-bbb1-12c97a784331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for calculating marginal of y\n",
    "def calc_y_mar(jd: np.array, y_ind: dict) -> np.array:\n",
    "    mar_dist = np.zeros((len(y_ind)))\n",
    "\n",
    "    for i, ind in y_ind.items():\n",
    "        mar_dist[i] = jd[:,ind].sum()\n",
    "        \n",
    "    return mar_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a15c2-56f7-4dde-ae62-651933e5b889",
   "metadata": {},
   "source": [
    "The next function we define is $Q(X|\\hat{X})$. This accepts the joint distribution $P(X,Y)$ and the dictionary for the row cluster prototypes. The function then returns a mxn matrix where m is the number of rows in the original matrix (or the total number of rows from all the row clusters) and n is the number of row clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aa5c93b-d033-4c06-83ca-d05ca68942ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating conditional of x given x_hat\n",
    "def calc_x_cond(jd: np.array, x_ind: dict) -> np.array:\n",
    "    x_mar = jd.sum(axis=1)\n",
    "    xhat_mar = calc_x_mar(jd, x_ind)\n",
    "    cond_dist = np.zeros((len(x_mar), len(x_ind)))\n",
    "    \n",
    "    for key, val in x_ind.items():\n",
    "        for idx, prb in enumerate(x_mar):\n",
    "            if idx in val:\n",
    "                cond_dist[idx,key] = prb / xhat_mar[key]\n",
    "            else:\n",
    "                cond_dist[idx,key] = 0\n",
    "    \n",
    "    return cond_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5c6e0-f87e-4e5a-82e1-36eaa7027f03",
   "metadata": {},
   "source": [
    "$Q(\\hat{X}|\\hat{Y})$ is the conditional distribution of the $\\hat{X}$ given $\\hat{Y}$. This just becomes the joint distribution $Q(\\hat{X},\\hat{Y})$ divided by the marginal of $Q(\\hat{Y})$. The function returns a mxn matrix where m is the number of row clusters and n is the number of column clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397dfe77-df10-4827-b952-c60776b702bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculation conditional of x_hat given y_hat\n",
    "def calc_xhat_cond(jd: np.array) -> np.array:\n",
    "    cond_dist = np.zeros((jd.shape[0], jd.shape[1]))\n",
    "    \n",
    "    for j in range(jd.shape[1]):\n",
    "        y_mar = jd[:,j].sum()\n",
    "        for i in range(jd.shape[0]):\n",
    "            if y_mar == 0:\n",
    "                cond_dist[i,j] = 0\n",
    "            else:\n",
    "                cond_dist[i,j] = jd[i,j] / y_mar\n",
    "    \n",
    "    return cond_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5bea6-07b3-4786-bfc5-8a2f196ca863",
   "metadata": {},
   "source": [
    "$Q(X|\\hat{Y})$ is calculated as the product of $Q(X|\\hat{X})Q(\\hat{X}|\\hat{Y})$. This function accepts then the previously calculated $Q(X|\\hat{X})$ distribution and the $Q(\\hat{X}|\\hat{Y})$ distribution. This function returns a mxn matrix where m is the number of rows from the original joint distribution $P(X,Y)$ and n is the the number of columns from the same said distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2d3b7a-ed49-4803-ae61-017b1632d160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating x given y_hat\n",
    "def calc_x_cond_yhat(x_xhat: np.array, xhat_yhat: np.array, x_ind: dict, y_ind: dict) -> np.array:\n",
    "    full_arr = []\n",
    "    \n",
    "    for y_key, y_val in y_ind.items():\n",
    "        for _ in range(len(y_val)):\n",
    "            row = []\n",
    "            for x_key, x_val in x_ind.items():\n",
    "                row.extend(xhat_yhat[x_key,y_key] * x_xhat[x_val, x_key])\n",
    "            full_arr.append(row)\n",
    "            \n",
    "    return np.array(full_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472357a-1752-413a-b5cd-c42c15bb5233",
   "metadata": {},
   "source": [
    "We now arrive at defining the column distributions, first with $Q(Y|\\hat{Y})$. This function accepts the primary joint distribution and the dictionary of $\\hat{Y}$. The function returns a matrix of dimension mxn where m is the number of columns in the original joint distribution and n is the number of column clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21eb290-6252-4a9f-9019-f2c234a80537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating conditional of y given y_hat\n",
    "def calc_y_cond(jd: np.array, y_ind: dict) -> np.array:\n",
    "    y_mar = jd.sum(axis=0)\n",
    "    yhat_mar = calc_y_mar(jd, y_ind)\n",
    "    cond_dist = np.zeros((len(y_mar), len(y_ind)))\n",
    "    \n",
    "    for key, val in y_ind.items():\n",
    "        for idx, prb in enumerate(y_mar):\n",
    "            if idx in val:\n",
    "                cond_dist[idx,key] = prb / yhat_mar[key]\n",
    "            else:\n",
    "                cond_dist[idx,key] = 0\n",
    "    \n",
    "    return cond_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3ea0e-b1cb-4997-8a00-2cfd4965b8de",
   "metadata": {},
   "source": [
    "$Q(\\hat{Y}|\\hat{X})$ is calculated very similariy to $Q(\\hat{X}|\\hat{Y})$ and returns, like the previous function, the same dimension of the of the joint co cluster distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512ffcea-acee-43f3-aa6d-3fcdc7aba34d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating conditional of y_hat given x_hat\n",
    "def calc_yhat_cond(jd: np.array) -> np.array:\n",
    "    cond_dist = np.zeros((jd.shape[0], jd.shape[1]))\n",
    "    \n",
    "    for i in range(jd.shape[0]):\n",
    "        x_mar = jd[i,:].sum()\n",
    "        for j in range(jd.shape[1]):\n",
    "            if x_mar == 0:\n",
    "                cond_dist[i,j] = 0\n",
    "            else:\n",
    "                cond_dist[i,j] = jd[i,j] / x_mar\n",
    "    \n",
    "    return cond_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd02d7-8b74-4abc-bcd7-f2c45523872b",
   "metadata": {},
   "source": [
    "$Q(Y|\\hat{X})$ is equal to the product of $Q(Y|\\hat{Y}) Q(\\hat{Y}|\\hat{X})$. This function accepts the $Q(Y|\\hat{Y})$ distribution, $Q(\\hat{Y}|\\hat{X})$ distribution, the column cluster dictionary, and the row cluster dictionary. The function returns a mxn matrix of the same dimension of the original joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d41b7388-0ae0-476c-ab9a-584ba351ab89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculating y given x_hat\n",
    "def calc_y_cond_xhat(y_yhat: np.array, yhat_xhat: np.array, y_ind: dict, x_ind: dict) -> np.array:\n",
    "    full_arr = []\n",
    "    \n",
    "    for x_key, x_val in x_ind.items():\n",
    "        for _ in range(len(x_val)):\n",
    "            row = []\n",
    "            for y_key, y_val in y_ind.items():\n",
    "                row.extend(yhat_xhat[x_key,y_key] * y_yhat[y_val, y_key])\n",
    "            full_arr.append(row)\n",
    "            \n",
    "    return np.array(full_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9891c-535f-4bbe-9c2f-f6dfac82ceb9",
   "metadata": {},
   "source": [
    "$C_X$ represents our current row clusters. The function below takes in the distributions $Q(Y|\\hat{X}$ and $P(Y|X)$, along with the mappings of each row to row to cluster using the x_ind dictionary. This function utilizes @eq-entropy to find calculate the distance between these distributions. It then selects the cluster row prototype that minimizes the distance between a row of the true distribution and that of the approximate distribution. The function returns a dictionary with the new row cluster prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d045de-f235-4a7a-8f4c-3ba6bdf49f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculation next c_x\n",
    "def next_cx(y_xhat: np.array, y_x: np.array, x_ind: dict) -> dict:\n",
    "    new_x_hat = {key: [] for key in x_ind}\n",
    "\n",
    "    for i in range(y_x.shape[0]):\n",
    "        temp_idx = []\n",
    "    \n",
    "        for key, val in x_ind.items():\n",
    "            proto = np.mean(y_xhat[val,:], axis=0)\n",
    "            kl_res = np.nan_to_num(kl_div(y_x[i,:], proto), posinf=10, neginf=-10)\n",
    "            #kl_res = rel_entr(y_x[i,:], proto)\n",
    "            temp_idx.append(np.sum(kl_res))\n",
    "       \n",
    "        temp_val = np.argmin(np.array(temp_idx))\n",
    "        #temp_val = tie_breaker(temp_idx)\n",
    "        new_x_hat[temp_val].append(i)\n",
    "    \n",
    "    return new_x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff77de-0624-4158-8812-ad962cf01afe",
   "metadata": {},
   "source": [
    "Similarily to the function above $C_Y$ represents our current column clusters. The function utilizes $Q(X|\\hat{Y})$ and $P(X|Y)$ along with the mappings found in y_ind, which contain the mappings of columns to column clusters. It then uses @eq-entropy to perform the same calculations used in $C_X$. This function also returns a dictionary with the new mappings for column cluster prototypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d0b5d98-fd93-4600-a1f7-483cf4876a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define function for calculation next c_y\n",
    "def next_cy(x_yhat: np.array, x_y: np.array, y_ind: dict) -> dict:\n",
    "    new_y_hat = {key: [] for key in y_ind}\n",
    "\n",
    "    for i in range(x_y.shape[1]):\n",
    "        temp_idx = []\n",
    "\n",
    "        for key, val in y_ind.items():\n",
    "            proto = np.mean(x_yhat[:,val], axis=1)\n",
    "            kl_res = np.nan_to_num(kl_div(x_y[:,i], proto), posinf=10, neginf=-10)\n",
    "            #kl_res = rel_entr(x_y[:,i], proto)\n",
    "            temp_idx.append(np.sum(kl_res))\n",
    "            \n",
    "        temp_val = np.argmin(np.array(temp_idx))\n",
    "        #temp_val = tie_breaker(temp_idx)\n",
    "        new_y_hat[temp_val].append(i)\n",
    "       \n",
    "    return new_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44721a53-45da-4466-b46e-84d0e70f5095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for valid clusters\n",
    "def valid_cluster(clust_dict: dict) -> dict:\n",
    "    max_val = len(clust_dict[0])\n",
    "    max_key = 0\n",
    "    \n",
    "    for key, val in clust_dict.items():\n",
    "        if len(val) > max_val:\n",
    "            max_val = len(val)\n",
    "            max_key = key\n",
    "    \n",
    "    for key, val in clust_dict.items():\n",
    "        if len(val) == 0:\n",
    "            clust_dict[key].append(clust_dict[max_key].pop(-1))\n",
    "            \n",
    "            \n",
    "    return clust_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3ff7b70-af37-45e7-b814-deb9e1637f3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define function for tie breaking\n",
    "def tie_breaker(idx: list):\n",
    "    min_val = min(idx)\n",
    "    min_idx = [i for i, val in enumerate(idx) if val == min_val]\n",
    "    \n",
    "    return np.random.choice(min_idx,1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582696c7-888f-4554-b780-0d0f84c348b9",
   "metadata": {},
   "source": [
    "This function is where we formally define the co-clustering algorithm utilizing the functions defined above as well as the steps outline for the ITCC algorithm above. We refer the reader to the above section \"Information Theoretic Co-Clustering\". The function accepts the target distribution we wish to perform co-clustering on, as well as the number of row clusters $k$ and column clusters $l$. The user can also specify the number of iterations. The function returns a tuple of form (dict, dict), where the first dictionary is the row cluster and the second is the column cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68612c8e-dbd2-4291-84ca-9589b8406c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| echo: true\n",
    "#Define co-clustering algorithm\n",
    "def co_cluster(joint_dist: np.array, k: int, l: int, num_iter: int) -> (dict, dict):\n",
    "    #Initialize x_hat\n",
    "    x_hat = get_x_hat(joint_dist, k, new=True)\n",
    "    #Initialize y_hat \n",
    "    y_hat = get_y_hat(joint_dist, l, new=True)\n",
    "    #Initialize min kl val\n",
    "    max_kl = .0001\n",
    "    print(f\"init_x_hat: {x_hat}, init_y_hat: {y_hat}\")\n",
    "    #Enter loop\n",
    "    for _ in range(num_iter):\n",
    "        \n",
    "        #Calculate q(x_hat, y_hat)\n",
    "        q_joint_hat = calc_joint(x_hat, y_hat, joint_dist)\n",
    "    \n",
    "        #Calculate q(x|x_hat)\n",
    "        q_x_cond_xhat = calc_x_cond(joint_dist, x_hat)\n",
    "        #Calculate q(y|y_hat)\n",
    "        q_y_cond_yhat = calc_y_cond(joint_dist, y_hat)\n",
    "        #Calculate p(y_hat|x_hat)\n",
    "        q_yhat_cond_xhat = calc_yhat_cond(q_joint_hat)\n",
    "        \n",
    "        #Calculate q(y|x_hat)\n",
    "        q_y_cond_xhat = calc_y_cond_xhat(q_y_cond_yhat, q_yhat_cond_xhat, y_hat, x_hat)\n",
    "        #Calculate p(y|x)\n",
    "        p_y_x = joint_dist / joint_dist.sum(axis=1).reshape(-1,1)\n",
    "        \n",
    "        #Get next cx\n",
    "        x_hat_2 = next_cx(q_y_cond_xhat, p_y_x, x_hat)\n",
    "        #Check if x_hat_2 is valid\n",
    "        x_hat_2 = valid_cluster(x_hat_2)\n",
    "        \n",
    "        \n",
    "        #Calculate qt+1(x_hat, y_hat)\n",
    "        q_joint_hat_2 = calc_joint(x_hat_2, y_hat, joint_dist)\n",
    "        \n",
    "        #Calculate qt+1(x|x_hat)\n",
    "        q_x_cond_xhat_2 = calc_x_cond(joint_dist, x_hat_2)\n",
    "        \n",
    "        #Calculate qt+1(x_hat|y_hat)\n",
    "        q_xhat_cond_yhat_2 = calc_xhat_cond(q_joint_hat_2)\n",
    "        \n",
    "        #Calculate qt+1(x|y_hat)\n",
    "        q_x_cond_yhat = calc_x_cond_yhat(q_x_cond_xhat_2, q_xhat_cond_yhat_2, x_hat_2, y_hat)\n",
    "        #Calculate p(x|y)\n",
    "        p_x_y = joint_dist / joint_dist.sum(axis=0).reshape(-1,1)\n",
    "        \n",
    "        #Get next cy\n",
    "        y_hat_2 = next_cy(q_x_cond_yhat, p_x_y, y_hat)\n",
    "        #Check if y_hat_2 is valid\n",
    "        y_hat_2 = valid_cluster(y_hat_2)\n",
    "        \n",
    "        \n",
    "        #Calculate qt+2(x_hat, y_hat)\n",
    "        q_joint_hat_3 = calc_joint(x_hat_2, y_hat_2, joint_dist)\n",
    "         \n",
    "        #Calculate qt+2(y|y_hat)\n",
    "        q_y_cond_yhat_2 = calc_y_cond(joint_dist, y_hat_2)\n",
    "        \n",
    "        #Calculate qt+2(y_hat|x_hat)\n",
    "        q_yhat_cond_xhat_2 = calc_yhat_cond(q_joint_hat_3)\n",
    "        \n",
    "        #Calculate qt+2(y|x_hat)\n",
    "        q_y_cond_xhat_2 = calc_y_cond_xhat(q_y_cond_yhat_2, q_yhat_cond_xhat_2, y_hat_2, x_hat_2)\n",
    "        \n",
    "        #Calculate q(x,y)\n",
    "        joint_first = joint_dist.sum(axis=1).reshape(-1,1) * q_y_cond_xhat\n",
    "        #Calculate qt+2(x,y)\n",
    "        joint_second = joint_dist.sum(axis=1).reshape(-1,1) * q_y_cond_xhat_2\n",
    "        \n",
    "        \n",
    "        kl_res_1 = np.sum(np.nan_to_num(kl_div(joint_dist, joint_first), posinf=10, neginf=-10))\n",
    "        kl_res_2 = np.sum(np.nan_to_num(kl_div(joint_dist, joint_second), posinf=10, neginf=-10))\n",
    "        \n",
    "        kl = kl_res_1 - kl_res_2\n",
    "        \n",
    "        if kl > max_kl:\n",
    "            x_hat = x_hat_2\n",
    "            y_hat = y_hat_2\n",
    "        else:\n",
    "            print(f\"kl={kl} on iteration {_}\")\n",
    "            return x_hat, y_hat\n",
    "    \n",
    "    return x_hat, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59b50afc-ee81-4249-824c-17d1dd5bfc8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_x_hat: {0: array([0, 3]), 1: array([1, 5]), 2: array([2, 4])}, init_y_hat: {0: array([2, 5, 1]), 1: array([0, 4, 3])}\n",
      "kl=0.0 on iteration 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: [0, 1], 1: [2, 3], 2: [4, 5]}, {0: [0, 1, 2], 1: [3, 4, 5]})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_cluster(test_arr, 3, 2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01103812-0290-448b-bd42-078f988efe51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_x_hat: {0: array([2, 0]), 1: array([3, 1]), 2: array([5, 4])}, init_y_hat: {0: array([5, 2, 4]), 1: array([0, 1, 3])}\n",
      "kl=-0.024462773127011506 on iteration 0\n",
      "({0: array([2, 0]), 1: array([3, 1]), 2: array([5, 4])}, {0: array([5, 2, 4]), 1: array([0, 1, 3])})\n",
      "init_x_hat: {0: array([4, 3]), 1: array([5, 1]), 2: array([0, 2])}, init_y_hat: {0: array([0, 5, 1]), 1: array([3, 2, 4])}\n",
      "kl=0.0 on iteration 2\n",
      "({0: [0, 1], 1: [2, 3], 2: [4, 5]}, {0: [0, 1, 2], 1: [3, 4, 5]})\n",
      "init_x_hat: {0: array([3, 2]), 1: array([0, 5]), 2: array([1, 4])}, init_y_hat: {0: array([3, 5, 1]), 1: array([4, 2, 0])}\n",
      "kl=-60.67395171964527 on iteration 0\n",
      "({0: array([3, 2]), 1: array([0, 5]), 2: array([1, 4])}, {0: array([3, 5, 1]), 1: array([4, 2, 0])})\n",
      "init_x_hat: {0: array([1, 5]), 1: array([3, 4]), 2: array([0, 2])}, init_y_hat: {0: array([3, 4, 1]), 1: array([5, 0, 2])}\n",
      "kl=0.0 on iteration 1\n",
      "({0: [2, 3], 1: [0, 1, 5], 2: [4]}, {0: [3, 4, 5], 1: [0, 1, 2]})\n",
      "init_x_hat: {0: array([1, 5]), 1: array([3, 2]), 2: array([4, 0])}, init_y_hat: {0: array([5, 4, 3]), 1: array([2, 1, 0])}\n",
      "kl=0.0 on iteration 2\n",
      "({0: [2, 3], 1: [0, 1], 2: [4, 5]}, {0: [3, 4, 5], 1: [0, 1, 2]})\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(co_cluster(test_arr, 3, 2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f3f7e4d-ba79-40d0-9900-a4f5b87b5f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_test_arr = np.array([\n",
    "    [5, 0, 4, 0, 1],\n",
    "    [0, 4, 0, 3, 0],\n",
    "    [1, 0, 5, 0, 4],\n",
    "    [0, 2, 0, 4, 0],\n",
    "    [4, 0, 1, 0, 5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a10b5620-955c-4329-9aa4-645bfe86d704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_x_hat: {0: array([4, 2]), 1: array([1, 0]), 2: array([3])}, init_y_hat: {0: array([0, 4, 2]), 1: array([1, 3])}\n",
      "kl=-4.239118604475969 on iteration 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: [3], 1: [4], 2: [0, 1, 2]}, {0: [0, 3, 4], 1: [1, 2]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_cluster(new_test_arr, 3, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f556582d-8d2d-4b94-a53a-52563cc8e0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_x_hat = {0: [0,1], 1: [2,3], 2: [4,5]}\n",
    "temp_y_hat = {0: [0,1,2], 1: [3,4,5]}\n",
    "temp_joint = calc_joint(temp_x_hat, temp_y_hat, test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "719081f4-a715-4dd4-91f6-29e96a6fa261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 0. ],\n",
       "       [0. , 0.3],\n",
       "       [0.2, 0.2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73565a23-ac70-472e-a238-ea756c596571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(y_hat|x_hat)\n",
    "t = calc_yhat_cond(temp_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b902d93b-ed87-4ee6-9921-056cc458f93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(y|y_hat)\n",
    "z = calc_y_cond(test_arr, temp_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8530e8e4-d507-45c6-aebd-c7f006ee96fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(y|x_hat)\n",
    "a = calc_y_cond_xhat(z, t, temp_y_hat, temp_x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b84abdb6-22d8-4f2f-b4f8-cc2f0b387693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(y|x)\n",
    "b = (test_arr / test_arr.sum(axis=1).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c89b6712-beb0-44a2-abfe-3e1e68fba0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 1], 1: [2, 3], 2: [4, 5]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get next cx\n",
    "next_cx(a, b, temp_x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04f0cc03-4371-43fa-acb6-c6dd21359a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(x_hat|y_hat)\n",
    "d = calc_xhat_cond(temp_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "939b3a18-a366-417e-9dae-8b4a038b6fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(x|x_hat)\n",
    "e = calc_x_cond(test_arr, temp_x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bac9c870-2761-4a50-83c8-c071d7d50cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculate p(x|y_hat)\n",
    "a = calc_x_cond_yhat(e, d, temp_x_hat, temp_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1370fede-52bf-4250-be84-e70c947c6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate p(x|y)\n",
    "b = (test_arr / test_arr.sum(axis=0).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0041d5b3-11cf-4abd-b397-4fe4d936b2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 1, 2], 1: [3, 4, 5]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get next cy\n",
    "next_cy(a, b, temp_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cf41a6-6f6a-46b1-b61b-a411ac2d8301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
